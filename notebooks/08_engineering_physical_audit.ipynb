{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 08 - Engineering & Physical Audit\n",
        "\n",
        "**Philosophy:** Raw Data Only - Zero Scoring, Zero Decisions\n",
        "\n",
        "**Purpose:** High-fidelity engineering documentation for each motion capture session. This notebook provides pure physical measurements, mathematical methodology, and biomechanical profiles WITHOUT synthetic quality scores or decision labels.\n",
        "\n",
        "**Target Audience:** Researchers, biomechanists, and data scientists who need transparent access to:\n",
        "- Raw capture quality metrics (SNR, missing data, jitter)\n",
        "- Processing methodology (interpolation, filtering, differentiation formulas)\n",
        "- Structural integrity (skeleton stability, calibration offsets)\n",
        "- Kinematic extremes (peak velocities, accelerations)\n",
        "- Per-joint noise profiles\n",
        "\n",
        "**What This Report Does NOT Include:**\n",
        "- Quality scores (0-100)\n",
        "- Decision labels (ACCEPT/REVIEW/REJECT)\n",
        "- Synthetic grades (GOLD/SILVER/BRONZE)\n",
        "- Pass/Fail judgments\n",
        "\n",
        "**References:**\n",
        "- Cereatti et al. (2024) - Data lineage & provenance\n",
        "- Winter (2009) - Residual analysis\n",
        "- R√°cz et al. (2025) - Calibration layer\n",
        "- Shoemake (1985) - Quaternion interpolation\n",
        "- Savitzky & Golay (1964) - Smoothing differentiation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Setup & Data Loading](#setup)\n",
        "2. [Methodology Passport](#methodology) - Mathematical documentation\n",
        "3. [Data Lineage](#lineage) - Recording provenance\n",
        "4. [Capture Baseline](#baseline) - Raw state before processing\n",
        "5. [Structural Integrity](#structure) - Skeleton & calibration\n",
        "6. [Signal Quality Profile](#signal) - Pre-processing SNR\n",
        "7. [Processing Transparency](#processing) - What was done\n",
        "8. [Kinematic Extremes](#kinematics) - Processed output\n",
        "9. [Per-Joint Noise Profile](#noise) - Root cause analysis\n",
        "10. [Outlier Distribution](#outliers) - Frame-level patterns\n",
        "11. [Excel Export](#export) - Engineering audit log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"setup\"></a>\n",
        "## 1. Setup & Data Loading\n",
        "\n",
        "Load all JSON files **once** and reuse throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project Root: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\n",
            "Git Hash: f83538a\n",
            "Timestamp: 2026-02-15 13:09:04\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# IMPORTS & PATH SETUP\n",
        "# ============================================================\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from IPython.display import display, HTML, Markdown\n",
        "\n",
        "# Setup paths\n",
        "if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "else:\n",
        "    PROJECT_ROOT = os.path.abspath(os.getcwd())\n",
        "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
        "if SRC_PATH not in sys.path:\n",
        "    sys.path.insert(0, SRC_PATH)\n",
        "\n",
        "# Import utility module (force reload for updates)\n",
        "import importlib\n",
        "if 'utils_nb07' in sys.modules:\n",
        "    import utils_nb07\n",
        "    importlib.reload(utils_nb07)\n",
        "\n",
        "from utils_nb07 import (\n",
        "    load_all_runs, \n",
        "    filter_complete_runs, \n",
        "    build_engineering_profile_row,\n",
        "    build_subject_profile,\n",
        "    extract_per_joint_noise_profile,\n",
        "    extract_bone_stability_profile,\n",
        "    extract_selected_segments,\n",
        "    compute_noise_locality_index,\n",
        "    get_git_hash,\n",
        "    print_section_header,\n",
        "    METHODOLOGY_PASSPORT\n",
        ")\n",
        "\n",
        "print(f\"Project Root: {PROJECT_ROOT}\")\n",
        "print(f\"Git Hash: {get_git_hash(PROJECT_ROOT)}\")\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading JSON files...\n",
            "Found 3 total runs\n",
            "Complete runs: 3\n",
            "\n",
            "Steps available per run:\n",
            "\n",
            "  671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001\n",
            "    ‚úÖ Found: ['step_01', 'step_02', 'step_03', 'step_04', 'step_05', 'step_06']\n",
            "    ‚Üí This run is COMPLETE and will be processed\n",
            "\n",
            "  671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005\n",
            "    ‚úÖ Found: ['step_01', 'step_02', 'step_03', 'step_04', 'step_05', 'step_06']\n",
            "    ‚Üí This run is COMPLETE and will be processed\n",
            "\n",
            "  671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000\n",
            "    ‚úÖ Found: ['step_01', 'step_02', 'step_03', 'step_04', 'step_05', 'step_06']\n",
            "    ‚Üí This run is COMPLETE and will be processed\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD ALL DATA (ONCE)\n",
        "# ============================================================\n",
        "DERIV_ROOT = os.path.join(PROJECT_ROOT, \"derivatives\")\n",
        "\n",
        "# Load all JSON files\n",
        "print(\"Loading JSON files...\")\n",
        "all_runs = load_all_runs(DERIV_ROOT)\n",
        "print(f\"Found {len(all_runs)} total runs\")\n",
        "\n",
        "# Filter to complete runs (require step_01 and step_06)\n",
        "runs_data = filter_complete_runs(all_runs, required_steps=[\"step_01\", \"step_06\"])\n",
        "print(f\"Complete runs: {len(runs_data)}\")\n",
        "\n",
        "# Show available steps per run\n",
        "print(\"\\nSteps available per run:\")\n",
        "expected_steps = ['step_01', 'step_02', 'step_03', 'step_04', 'step_05', 'step_06']\n",
        "\n",
        "if len(all_runs) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: No runs found at all!\")\n",
        "    print(f\"\\nSearched in: {DERIV_ROOT}\")\n",
        "    print(\"\\nExpected structure:\")\n",
        "    print(\"  derivatives/\")\n",
        "    print(\"    ‚îú‚îÄ‚îÄ step_01_parse/\")\n",
        "    print(\"    ‚îÇ   ‚îî‚îÄ‚îÄ {run_id}__step01_loader_report.json\")\n",
        "    print(\"    ‚îú‚îÄ‚îÄ step_02_preprocess/\")\n",
        "    print(\"    ‚îÇ   ‚îî‚îÄ‚îÄ {run_id}__preprocess_summary.json\")\n",
        "    print(\"    ‚îú‚îÄ‚îÄ step_03_resample/\")\n",
        "    print(\"    ‚îú‚îÄ‚îÄ step_04_filtering/\")\n",
        "    print(\"    ‚îú‚îÄ‚îÄ step_05_reference/\")\n",
        "    print(\"    ‚îî‚îÄ‚îÄ step_06_kinematics/\")\n",
        "    print(\"        ‚îî‚îÄ‚îÄ ultimate/\")\n",
        "    print(\"            ‚îî‚îÄ‚îÄ {run_id}__outlier_validation.json\")\n",
        "    print(\"\\nPlease run the full pipeline (notebooks 01-06) first.\")\n",
        "else:\n",
        "    for run_id, steps in all_runs.items():\n",
        "        steps_list = sorted(steps.keys())\n",
        "        missing = [s for s in expected_steps if s not in steps_list]\n",
        "        \n",
        "        # Truncate run_id for display\n",
        "        display_id = run_id[:60] + \"...\" if len(run_id) > 60 else run_id\n",
        "        print(f\"\\n  {display_id}\")\n",
        "        print(f\"    ‚úÖ Found: {steps_list}\")\n",
        "        \n",
        "        if missing:\n",
        "            print(f\"    ‚ö†Ô∏è Missing: {missing}\")\n",
        "            if run_id not in runs_data:\n",
        "                print(f\"    ‚Üí This run is INCOMPLETE and will be skipped\")\n",
        "        else:\n",
        "            print(f\"    ‚Üí This run is COMPLETE and will be processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Building engineering profiles for 3 complete runs...\n",
            "[DEBUG] extract_phase2_metrics: Found 'path_length' in s06\n",
            "[DEBUG] Found by_region structure: {'Neck': 50.527266676734286, 'Shoulders': 47.292705996517235, 'Elbows': 60.685467516215276, 'Wrists': 117.42213285178761, 'Spine': 28.43047571703742, 'Hips': 12.197133030420384, 'Knees': 46.326368809313486, 'Ankles': 52.78297513463008}\n",
            "[DEBUG] Found new intensity structure with by_region: {'Neck': 0.19812671964213033, 'Shoulders': 0.18544341141659537, 'Elbows': 0.23795889625023145, 'Wrists': 0.46043381179016807, 'Spine': 0.11148113211268472, 'Hips': 0.04782720529524707, 'Knees': 0.18165422530855205, 'Ankles': 0.2069717680016864}\n",
            "[DEBUG] extract_phase2_metrics: Found 'path_length' in s06\n",
            "[DEBUG] Found by_region structure: {'Neck': 49.90148006766413, 'Shoulders': 45.95326641925252, 'Elbows': 56.45029809817945, 'Wrists': 120.76143018035427, 'Spine': 26.3290052573553, 'Hips': 14.646004291906534, 'Knees': 48.881444417541104, 'Ankles': 57.12237953196643}\n",
            "[DEBUG] Found new intensity structure with by_region: {'Neck': 0.19727154037620476, 'Shoulders': 0.18166338231956192, 'Elbows': 0.22316046027941144, 'Wrists': 0.47739652846788044, 'Spine': 0.10408435614833259, 'Hips': 0.05789888041603637, 'Knees': 0.19323911481156095, 'Ankles': 0.22581734619785773}\n",
            "[DEBUG] extract_phase2_metrics: Found 'path_length' in s06\n",
            "[DEBUG] Found by_region structure: {'Neck': 52.69303590151728, 'Shoulders': 47.97703343405953, 'Elbows': 59.5996459124679, 'Wrists': 114.48789025966448, 'Spine': 40.39580890810076, 'Hips': 12.610182282302242, 'Knees': 44.4555372349478, 'Ankles': 54.45508435462273}\n",
            "[DEBUG] Found new intensity structure with by_region: {'Neck': 0.19963264217282545, 'Shoulders': 0.18176561255563375, 'Elbows': 0.2257989994789464, 'Wrists': 0.4337484003018166, 'Spine': 0.15304341317711978, 'Hips': 0.04777489025308673, 'Knees': 0.1684240849969608, 'Ankles': 0.2063083324668412}\n",
            "\n",
            "‚úÖ Engineering DataFrame: 3 runs √ó 94 measurements\n",
            "\n",
            "Column groups:\n",
            "  - Lineage: Run_ID, Subject_ID, Session_ID, Pipeline_Version\n",
            "  - Baseline: Duration, Sampling_Rate, Raw_Missing_%, SNR\n",
            "  - Structure: Bone_CV%, Skeleton_Segments, Height, Mass\n",
            "  - Processing: Interpolation, Filtering, Resampling\n",
            "  - Kinematics: Max_Velocity, Max_Acceleration, Path_Length\n",
            "  - Outliers: Total_Frames, Classification, Data_Retained\n",
            "\n",
            "[DEBUG] Found 12 Path columns: ['Path_Length_Hips_mm', 'Path_Length_Max_m', 'Path_Length_Mean_m', 'Path_Length_Total_m', 'Path_Neck_m']...\n",
            "[DEBUG] Found 11 Intensity columns: ['Intensity_Index', 'Intensity_Max_m_per_s', 'Intensity_Mean_m_per_s', 'Intensity_Neck_m_per_s', 'Intensity_Shoulders_m_per_s']...\n",
            "[DEBUG] Sample path values: [0. 0. 0.]\n",
            "[DEBUG] Sample intensity values: [0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BUILD ENGINEERING PROFILE DATAFRAME\n",
        "# ============================================================\n",
        "\n",
        "# Check if we have any complete runs\n",
        "if len(runs_data) == 0:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚ö†Ô∏è ERROR: NO COMPLETE RUNS FOUND\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nDiagnostics:\")\n",
        "    print(f\"  Total runs discovered: {len(all_runs)}\")\n",
        "    print(f\"  Runs with step_01 AND step_06: 0\")\n",
        "    print(\"\\nShowing what's available for each run:\\n\")\n",
        "    \n",
        "    for run_id, steps in all_runs.items():\n",
        "        print(f\"  Run: {run_id[:70]}\")\n",
        "        print(f\"    Steps found: {sorted(steps.keys())}\")\n",
        "        missing = [s for s in ['step_01', 'step_02', 'step_03', 'step_04', 'step_05', 'step_06'] if s not in steps]\n",
        "        if missing:\n",
        "            print(f\"    ‚ö†Ô∏è Missing: {missing}\")\n",
        "        print()\n",
        "    \n",
        "    print(\"\\nTo fix this issue:\")\n",
        "    print(\"  1. Ensure all pipeline steps (01-06) have been run for your sessions\")\n",
        "    print(\"  2. Check that JSON files exist in derivatives/step_01_parse/ and derivatives/step_06_kinematics/ultimate/\")\n",
        "    print(\"  3. Verify the run_id prefixes match across all steps\")\n",
        "    print(\"\\nCannot proceed without complete runs. Stopping execution.\")\n",
        "    raise ValueError(\"No complete runs found. Please run the full pipeline (steps 01-06) first.\")\n",
        "\n",
        "# Extract pure physical measurements (NO SCORING)\n",
        "print(f\"\\nBuilding engineering profiles for {len(runs_data)} complete runs...\")\n",
        "engineering_rows = [build_engineering_profile_row(run_id, steps) for run_id, steps in runs_data.items()]\n",
        "df_engineering = pd.DataFrame(engineering_rows)\n",
        "\n",
        "# Sort by subject and session for easy review\n",
        "if len(df_engineering) > 0:\n",
        "    df_engineering = df_engineering.sort_values([\"Subject_ID\", \"Session_ID\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n‚úÖ Engineering DataFrame: {len(df_engineering)} runs √ó {len(df_engineering.columns)} measurements\")\n",
        "print(f\"\\nColumn groups:\")\n",
        "print(f\"  - Lineage: Run_ID, Subject_ID, Session_ID, Pipeline_Version\")\n",
        "print(f\"  - Baseline: Duration, Sampling_Rate, Raw_Missing_%, SNR\")\n",
        "print(f\"  - Structure: Bone_CV%, Skeleton_Segments, Height, Mass\")\n",
        "print(f\"  - Processing: Interpolation, Filtering, Resampling\")\n",
        "print(f\"  - Kinematics: Max_Velocity, Max_Acceleration, Path_Length\")\n",
        "print(f\"  - Outliers: Total_Frames, Classification, Data_Retained\")\n",
        "\n",
        "# DEBUG: Check for anatomical region columns\n",
        "path_cols = [c for c in df_engineering.columns if 'Path_' in c and '_m' in c]\n",
        "intensity_cols = [c for c in df_engineering.columns if 'Intensity_' in c]\n",
        "print(f\"\\n[DEBUG] Found {len(path_cols)} Path columns: {path_cols[:5]}...\")\n",
        "print(f\"[DEBUG] Found {len(intensity_cols)} Intensity columns: {intensity_cols[:5]}...\")\n",
        "if len(path_cols) > 0:\n",
        "    print(f\"[DEBUG] Sample path values: {df_engineering[path_cols[0]].values}\")\n",
        "if len(intensity_cols) > 0:\n",
        "    print(f\"[DEBUG] Sample intensity values: {df_engineering[intensity_cols[0]].values}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"methodology\"></a>\n",
        "## 2. Methodology Passport\n",
        "\n",
        "**Purpose:** Document the mathematical methods used to derive all reported values.\n",
        "\n",
        "This section provides explicit formulas, implementation details, and references for:\n",
        "- Quaternion interpolation (SLERP)\n",
        "- Angular velocity extraction\n",
        "- Angular acceleration differentiation\n",
        "- 3-stage signal cleaning pipeline\n",
        "- Resampling strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "METHODOLOGY PASSPORT - MATHEMATICAL DOCUMENTATION\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "INTERPOLATION METHODS\n",
            "================================================================================\n",
            "\n",
            "üìê Rotation Interpolation: SLERP (Spherical Linear Interpolation)\n",
            "   Formula: q(t) = sin((1-t)Œ∏)/sin(Œ∏) ¬∑ q‚ÇÄ + sin(tŒ∏)/sin(Œ∏) ¬∑ q‚ÇÅ\n",
            "   Constraint: Maintains unit quaternion: ||q|| = 1\n",
            "   Geodesic: Shortest path on SO(3) manifold\n",
            "   Reference: Shoemake (1985)\n",
            "   Implementation: scipy.spatial.transform.Slerp\n",
            "\n",
            "üìê Position Interpolation: CubicSpline\n",
            "   Formula: p(t) = a‚ÇÄ + a‚ÇÅt + a‚ÇÇt¬≤ + a‚ÇÉt¬≥ (piecewise)\n",
            "   Continuity: C¬≤ (smooth velocity and acceleration)\n",
            "   Constraint: Natural boundary conditions\n",
            "   Implementation: scipy.interpolate.CubicSpline\n",
            "\n",
            "================================================================================\n",
            "DIFFERENTIATION METHODS\n",
            "================================================================================\n",
            "\n",
            "üîÑ Angular Velocity: Quaternion Derivative\n",
            "   Formula: œâ = 2 ¬∑ (dq/dt) ¬∑ q*\n",
            "   Derivation: qÃá via finite differences, then œâ = 2qÃáq* (quaternion conjugate)\n",
            "   Units: deg/s\n",
            "   Note: Extracts instantaneous axis-angle velocity from quaternion time series\n",
            "\n",
            "üîÑ Angular Acceleration: Savitzky-Golay Filter\n",
            "   Formula: Œ± = d/dt(œâ) via least-squares polynomial fitting\n",
            "   Window: 0.175s (21 frames @ 120Hz)\n",
            "   Polynomial Order: 3\n",
            "   Units: deg/s¬≤\n",
            "   Reference: Savitzky & Golay (1964)\n",
            "   Implementation: scipy.signal.savgol_filter with mode='interp'\n",
            "\n",
            "üìè Linear Velocity: CubicSpline Analytical Derivative\n",
            "   Formula: v = dp/dt (analytical derivative of cubic spline)\n",
            "   Note: Exact derivative, not finite-difference approximation\n",
            "\n",
            "üìè Linear Acceleration: CubicSpline Second Derivative\n",
            "   Formula: a = d¬≤p/dt¬≤ (analytical second derivative)\n",
            "   Note: Exact second derivative from spline coefficients\n",
            "\n",
            "================================================================================\n",
            "3-STAGE SIGNAL CLEANING PIPELINE (v3.0)\n",
            "================================================================================\n",
            "\n",
            "Philosophy: Artifact removal with movement preservation\n",
            "Version: v3.0_3stage_signal_cleaning\n",
            "\n",
            "üîç Stage 1: Z-Score + Velocity Threshold\n",
            "   Velocity Limit: 5000.0 mm/s\n",
            "   Z-Score Threshold: 5.0œÉ\n",
            "   Interpolation: PCHIP (Piecewise Cubic Hermite Interpolating Polynomial)\n",
            "   Purpose: Remove physically impossible spikes (tracking glitches)\n",
            "\n",
            "üîç Stage 2: Sliding Window Median Filter\n",
            "   Window Size: 5 frames\n",
            "   Sigma Threshold: 3.0œÉ\n",
            "   Purpose: Remove isolated outliers while preserving edges\n",
            "   Note: Median-based, robust to non-Gaussian noise\n",
            "\n",
            "üîç Stage 3: Winter's Residual Analysis (1990)\n",
            "   Strategy: Per-Region Adaptive Cutoff Selection\n",
            "   Frequency Range: 1.0-20.0 Hz\n",
            "   Filter Type: Butterworth Low-Pass\n",
            "   Filter Order: 2\n",
            "   Implementation: Zero-lag (filtfilt) - forward-backward pass\n",
            "   Rationale: Different body regions have different frequency content (distal faster than proximal)\n",
            "\n",
            "   Body Regions:\n",
            "     - head: Head markers\n",
            "     - upper_proximal: Shoulders, upper arms\n",
            "     - trunk: Spine segments\n",
            "     - lower_distal: Feet, ankles\n",
            "     - lower_proximal: Thighs, hips\n",
            "     - upper_distal: Hands, forearms\n",
            "\n",
            "================================================================================\n",
            "RESAMPLING STRATEGY\n",
            "================================================================================\n",
            "\n",
            "Target Frequency: 120.0 Hz\n",
            "Purpose: Uniform temporal grid for frequency-domain analysis\n",
            "Method: Interpolate to exact 1/120s intervals (8.333ms)\n",
            "Positions: CubicSpline\n",
            "Rotations: SLERP\n",
            "Validation: Time grid standard deviation < 0.001ms\n",
            "\n",
            "================================================================================\n",
            "REFERENCE ALIGNMENT\n",
            "================================================================================\n",
            "\n",
            "Method: ISB/CAST Static Pose Detection\n",
            "Reference: R√°cz et al. (2025) - Anatomical Calibration Layer\n",
            "Detection: Sliding window variance minimization (1-2 second stable period)\n",
            "Offset Correction: V-pose to T-pose quaternion transformation\n",
            "Bilateral Correction: Left/Right arm abduction offset removal\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"METHODOLOGY PASSPORT - MATHEMATICAL DOCUMENTATION\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INTERPOLATION METHODS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Rotation Interpolation\n",
        "rot_method = METHODOLOGY_PASSPORT[\"interpolation\"][\"rotations\"]\n",
        "print(f\"\\nüìê Rotation Interpolation: {rot_method['method']}\")\n",
        "print(f\"   Formula: {rot_method['formula']}\")\n",
        "print(f\"   Constraint: {rot_method['constraint']}\")\n",
        "print(f\"   Geodesic: {rot_method['geodesic']}\")\n",
        "print(f\"   Reference: {rot_method['reference']}\")\n",
        "print(f\"   Implementation: {rot_method['implementation']}\")\n",
        "\n",
        "# Position Interpolation\n",
        "pos_method = METHODOLOGY_PASSPORT[\"interpolation\"][\"positions\"]\n",
        "print(f\"\\nüìê Position Interpolation: {pos_method['method']}\")\n",
        "print(f\"   Formula: {pos_method['formula']}\")\n",
        "print(f\"   Continuity: {pos_method['continuity']}\")\n",
        "print(f\"   Constraint: {pos_method['constraint']}\")\n",
        "print(f\"   Implementation: {pos_method['implementation']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DIFFERENTIATION METHODS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Angular Velocity\n",
        "ang_vel = METHODOLOGY_PASSPORT[\"differentiation\"][\"angular_velocity\"]\n",
        "print(f\"\\nüîÑ Angular Velocity: {ang_vel['method']}\")\n",
        "print(f\"   Formula: {ang_vel['formula']}\")\n",
        "print(f\"   Derivation: {ang_vel['derivation']}\")\n",
        "print(f\"   Units: {ang_vel['units']}\")\n",
        "print(f\"   Note: {ang_vel['note']}\")\n",
        "\n",
        "# Angular Acceleration\n",
        "ang_accel = METHODOLOGY_PASSPORT[\"differentiation\"][\"angular_acceleration\"]\n",
        "print(f\"\\nüîÑ Angular Acceleration: {ang_accel['method']}\")\n",
        "print(f\"   Formula: {ang_accel['formula']}\")\n",
        "print(f\"   Window: {ang_accel['window_sec']}s ({ang_accel['window_frames']} frames @ 120Hz)\")\n",
        "print(f\"   Polynomial Order: {ang_accel['polynomial_order']}\")\n",
        "print(f\"   Units: {ang_accel['units']}\")\n",
        "print(f\"   Reference: {ang_accel['reference']}\")\n",
        "print(f\"   Implementation: {ang_accel['implementation']}\")\n",
        "\n",
        "# Linear Derivatives\n",
        "lin_vel = METHODOLOGY_PASSPORT[\"differentiation\"][\"linear_velocity\"]\n",
        "lin_accel = METHODOLOGY_PASSPORT[\"differentiation\"][\"linear_acceleration\"]\n",
        "print(f\"\\nüìè Linear Velocity: {lin_vel['method']}\")\n",
        "print(f\"   Formula: {lin_vel['formula']}\")\n",
        "print(f\"   Note: {lin_vel['note']}\")\n",
        "print(f\"\\nüìè Linear Acceleration: {lin_accel['method']}\")\n",
        "print(f\"   Formula: {lin_accel['formula']}\")\n",
        "print(f\"   Note: {lin_accel['note']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3-STAGE SIGNAL CLEANING PIPELINE (v3.0)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "filtering = METHODOLOGY_PASSPORT[\"filtering\"]\n",
        "print(f\"\\nPhilosophy: {filtering['philosophy']}\")\n",
        "print(f\"Version: {filtering['pipeline_version']}\")\n",
        "\n",
        "# Stage 1\n",
        "stage1 = filtering[\"stage1_artifact_detection\"]\n",
        "print(f\"\\nüîç Stage 1: {stage1['method']}\")\n",
        "print(f\"   Velocity Limit: {stage1['velocity_limit_mm_s']} mm/s\")\n",
        "print(f\"   Z-Score Threshold: {stage1['zscore_threshold']}œÉ\")\n",
        "print(f\"   Interpolation: {stage1['interpolation']}\")\n",
        "print(f\"   Purpose: {stage1['purpose']}\")\n",
        "\n",
        "# Stage 2\n",
        "stage2 = filtering[\"stage2_hampel\"]\n",
        "print(f\"\\nüîç Stage 2: {stage2['method']}\")\n",
        "print(f\"   Window Size: {stage2['window_size']} frames\")\n",
        "print(f\"   Sigma Threshold: {stage2['n_sigma']}œÉ\")\n",
        "print(f\"   Purpose: {stage2['purpose']}\")\n",
        "print(f\"   Note: {stage2['note']}\")\n",
        "\n",
        "# Stage 3\n",
        "stage3 = filtering[\"stage3_adaptive_winter\"]\n",
        "print(f\"\\nüîç Stage 3: {stage3['method']}\")\n",
        "print(f\"   Strategy: {stage3['strategy']}\")\n",
        "print(f\"   Frequency Range: {stage3['fmin_hz']}-{stage3['fmax_hz']} Hz\")\n",
        "print(f\"   Filter Type: {stage3['filter_type']}\")\n",
        "print(f\"   Filter Order: {stage3['filter_order']}\")\n",
        "print(f\"   Implementation: {stage3['implementation']}\")\n",
        "print(f\"   Rationale: {stage3['rationale']}\")\n",
        "print(f\"\\n   Body Regions:\")\n",
        "for region, description in stage3[\"regions\"].items():\n",
        "    print(f\"     - {region}: {description}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESAMPLING STRATEGY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "resampling = METHODOLOGY_PASSPORT[\"resampling\"]\n",
        "print(f\"\\nTarget Frequency: {resampling['target_fs_hz']} Hz\")\n",
        "print(f\"Purpose: {resampling['purpose']}\")\n",
        "print(f\"Method: {resampling['method']}\")\n",
        "print(f\"Positions: {resampling['positions_method']}\")\n",
        "print(f\"Rotations: {resampling['rotations_method']}\")\n",
        "print(f\"Validation: {resampling['validation']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"REFERENCE ALIGNMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ref_align = METHODOLOGY_PASSPORT[\"reference_alignment\"]\n",
        "print(f\"\\nMethod: {ref_align['method']}\")\n",
        "print(f\"Reference: {ref_align['reference']}\")\n",
        "print(f\"Detection: {ref_align['detection']}\")\n",
        "print(f\"Offset Correction: {ref_align['offset_correction']}\")\n",
        "print(f\"Bilateral Correction: {ref_align['bilateral_correction']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"lineage\"></a>\n",
        "## 3. Data Lineage & Provenance\n",
        "\n",
        "**Purpose:** Ensure recording traceability from raw file to final result (Cereatti et al., 2024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "DATA LINEAGE & PROVENANCE\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run_ID</th>\n",
              "      <th>Subject_ID</th>\n",
              "      <th>Session_ID</th>\n",
              "      <th>Processing_Timestamp</th>\n",
              "      <th>Pipeline_Version</th>\n",
              "      <th>CSV_Source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001</td>\n",
              "      <td>671</td>\n",
              "      <td>T1</td>\n",
              "      <td>2026-02-15 12:01</td>\n",
              "      <td>v2.6_calibration_enhanced</td>\n",
              "      <td>C:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005</td>\n",
              "      <td>671</td>\n",
              "      <td>T2</td>\n",
              "      <td>2026-02-15 12:54</td>\n",
              "      <td>v2.6_calibration_enhanced</td>\n",
              "      <td>C:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000</td>\n",
              "      <td>671</td>\n",
              "      <td>T3</td>\n",
              "      <td>2026-02-15 12:58</td>\n",
              "      <td>v2.6_calibration_enhanced</td>\n",
              "      <td>C:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gag...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Run_ID Subject_ID Session_ID  \\\n",
              "0  671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001        671         T1   \n",
              "1  671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005        671         T2   \n",
              "2  671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000        671         T3   \n",
              "\n",
              "  Processing_Timestamp           Pipeline_Version  \\\n",
              "0     2026-02-15 12:01  v2.6_calibration_enhanced   \n",
              "1     2026-02-15 12:54  v2.6_calibration_enhanced   \n",
              "2     2026-02-15 12:58  v2.6_calibration_enhanced   \n",
              "\n",
              "                                          CSV_Source  \n",
              "0  C:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gag...  \n",
              "1  C:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gag...  \n",
              "2  C:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gag...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset Summary:\n",
            "  Total Recordings: 3\n",
            "  Unique Subjects: 1\n",
            "  Unique Sessions: 3\n",
            "  Pipeline Version: v2.6_calibration_enhanced\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"DATA LINEAGE & PROVENANCE\")\n",
        "\n",
        "# Display provenance info\n",
        "cols_lineage = ['Run_ID', 'Subject_ID', 'Session_ID', 'Processing_Timestamp', 'Pipeline_Version', 'CSV_Source']\n",
        "display(df_engineering[cols_lineage])\n",
        "\n",
        "print(f\"\\nDataset Summary:\")\n",
        "print(f\"  Total Recordings: {len(df_engineering)}\")\n",
        "print(f\"  Unique Subjects: {df_engineering['Subject_ID'].nunique()}\")\n",
        "print(f\"  Unique Sessions: {df_engineering['Session_ID'].nunique()}\")\n",
        "print(f\"  Pipeline Version: {df_engineering['Pipeline_Version'].iloc[0] if len(df_engineering) > 0 else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"baseline\"></a>\n",
        "## 4. Capture Baseline Profile\n",
        "\n",
        "**Purpose:** Document the raw state of the data BEFORE any processing.\n",
        "\n",
        "This section represents the \"Ground Truth\" capture quality:\n",
        "- How much data was missing in the raw OptiTrack export?\n",
        "- What was the inherent Signal-to-Noise Ratio (SNR) of the raw tracking?\n",
        "- What was the native sampling rate and jitter?\n",
        "- What was the OptiTrack system calibration error?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 11.5: Anatomical Region View (Human-Readable)\n",
        "\n",
        "Group path lengths by anatomical regions for easier interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ANATOMICAL REGION PATH LENGTHS (meters)\n",
            "================================================================================\n",
            "\n",
            "Human-readable view of movement by body region:\n",
            "\n",
            "Mapping:\n",
            "  ‚Ä¢ Neck       ‚Üí Neck joint\n",
            "  ‚Ä¢ Shoulders  ‚Üí Left/Right shoulder joints (max)\n",
            "  ‚Ä¢ Elbows     ‚Üí Left/Right forearm segments (max)\n",
            "  ‚Ä¢ Wrists     ‚Üí Left/Right hand/wrist joints (max)\n",
            "  ‚Ä¢ Spine      ‚Üí Mid-back / thoracic region\n",
            "  ‚Ä¢ Hips       ‚Üí Pelvis + hip joints\n",
            "  ‚Ä¢ Knees      ‚Üí Left/Right shin segments (max)\n",
            "  ‚Ä¢ Ankles     ‚Üí Left/Right foot/ankle joints (max)\n",
            "\n",
            "================================================================================\n",
            "                                      Run_ID  Neck  Shoulders  Elbows  Wrists  Spine  Hips  Knees  Ankles\n",
            "671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001 50.53      47.29   60.69  117.42  28.43 12.20  46.33   52.78\n",
            "671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005 49.90      45.95   56.45  120.76  26.33 14.65  48.88   57.12\n",
            "671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000 52.69      47.98   59.60  114.49  40.40 12.61  44.46   54.46\n",
            "\n",
            "================================================================================\n",
            "REGION SUMMARY (across all runs)\n",
            "================================================================================\n",
            "       Neck  Shoulders     Elbows      Wrists  Spine       Hips      Knees     Ankles\n",
            "mean  51.04  47.073333  58.913333  117.556667  31.72  13.153333  46.556667  54.786667\n",
            "min   49.90  45.950000  56.450000  114.490000  26.33  12.200000  44.460000  52.780000\n",
            "max   52.69  47.980000  60.690000  120.760000  40.40  14.650000  48.880000  57.120000\n",
            "\n",
            "================================================================================\n",
            "MOST ACTIVE REGIONS (ranked by average path length)\n",
            "================================================================================\n",
            "  1. Wrists      : 117.56m\n",
            "  2. Elbows      : 58.91m\n",
            "  3. Ankles      : 54.79m\n",
            "  4. Neck        : 51.04m\n",
            "  5. Shoulders   : 47.07m\n",
            "  6. Knees       : 46.56m\n",
            "  7. Spine       : 31.72m\n",
            "  8. Hips        : 13.15m\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ANATOMICAL REGION BREAKDOWN\n",
        "# ============================================================\n",
        "\n",
        "# Extract anatomical region columns\n",
        "anatomical_cols = [\n",
        "    \"Path_Neck_m\", \"Path_Shoulders_m\", \"Path_Elbows_m\", \"Path_Wrists_m\",\n",
        "    \"Path_Spine_m\", \"Path_Hips_m\", \"Path_Knees_m\", \"Path_Ankles_m\"\n",
        "]\n",
        "\n",
        "if all(col in df_engineering.columns for col in anatomical_cols):\n",
        "    # Create anatomical summary\n",
        "    region_data = []\n",
        "    \n",
        "    for _, row in df_engineering.iterrows():\n",
        "        regions = {\n",
        "            \"Run_ID\": row[\"Run_ID\"],\n",
        "            \"Neck\": row[\"Path_Neck_m\"],\n",
        "            \"Shoulders\": row[\"Path_Shoulders_m\"],\n",
        "            \"Elbows\": row[\"Path_Elbows_m\"],\n",
        "            \"Wrists\": row[\"Path_Wrists_m\"],\n",
        "            \"Spine\": row[\"Path_Spine_m\"],\n",
        "            \"Hips\": row[\"Path_Hips_m\"],\n",
        "            \"Knees\": row[\"Path_Knees_m\"],\n",
        "            \"Ankles\": row[\"Path_Ankles_m\"],\n",
        "        }\n",
        "        region_data.append(regions)\n",
        "    \n",
        "    df_anatomical = pd.DataFrame(region_data)\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"ANATOMICAL REGION PATH LENGTHS (meters)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nHuman-readable view of movement by body region:\")\n",
        "    print(\"\\nMapping:\")\n",
        "    print(\"  ‚Ä¢ Neck       ‚Üí Neck joint\")\n",
        "    print(\"  ‚Ä¢ Shoulders  ‚Üí Left/Right shoulder joints (max)\")\n",
        "    print(\"  ‚Ä¢ Elbows     ‚Üí Left/Right forearm segments (max)\")\n",
        "    print(\"  ‚Ä¢ Wrists     ‚Üí Left/Right hand/wrist joints (max)\")\n",
        "    print(\"  ‚Ä¢ Spine      ‚Üí Mid-back / thoracic region\")\n",
        "    print(\"  ‚Ä¢ Hips       ‚Üí Pelvis + hip joints\")\n",
        "    print(\"  ‚Ä¢ Knees      ‚Üí Left/Right shin segments (max)\")\n",
        "    print(\"  ‚Ä¢ Ankles     ‚Üí Left/Right foot/ankle joints (max)\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    \n",
        "    # Display table\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):\n",
        "        print(df_anatomical.to_string(index=False))\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"REGION SUMMARY (across all runs)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    region_summary = df_anatomical.drop(columns=[\"Run_ID\"]).describe().loc[[\"mean\", \"min\", \"max\"]]\n",
        "    print(region_summary.to_string())\n",
        "    \n",
        "    # Most active regions\n",
        "    mean_by_region = df_anatomical.drop(columns=[\"Run_ID\"]).mean().sort_values(ascending=False)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MOST ACTIVE REGIONS (ranked by average path length)\")\n",
        "    print(\"=\"*80)\n",
        "    for i, (region, value) in enumerate(mean_by_region.items(), 1):\n",
        "        print(f\"  {i}. {region:12s}: {value:.2f}m\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Anatomical region columns not found. Re-run notebook 06 with Phase 2 updates.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 11.6: Intensity Index (Phase 3)\n",
        "\n",
        "**Movement intensity normalized by duration** - allows fair comparison between sessions of different lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "INTENSITY INDEX (meters per second)\n",
            "================================================================================\n",
            "\n",
            "Intensity = Path Length / Duration\n",
            "  ‚Üí Normalized measure of movement activity\n",
            "  ‚Üí Allows comparison between sessions of different durations\n",
            "  ‚Üí Units: m/s (average velocity-like measure)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Intensity by Anatomical Region (m/s):\n",
            "                                      Run_ID  Duration_sec   Neck  Shoulders  Elbows  Wrists  Spine   Hips  Knees  Ankles\n",
            "671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001        255.03 0.1981     0.1854  0.2380  0.4604 0.1115 0.0478 0.1817  0.2070\n",
            "671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005        252.96 0.1973     0.1817  0.2232  0.4774 0.1041 0.0579 0.1932  0.2258\n",
            "671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000        263.94 0.1996     0.1818  0.2258  0.4337 0.1530 0.0478 0.1684  0.2063\n",
            "\n",
            "================================================================================\n",
            "INTENSITY SUMMARY (across all runs)\n",
            "================================================================================\n",
            "          Neck  Shoulders  Elbows    Wrists     Spine      Hips   Knees    Ankles\n",
            "mean  0.198333   0.182967  0.2290  0.457167  0.122867  0.051167  0.1811  0.213033\n",
            "min   0.197300   0.181700  0.2232  0.433700  0.104100  0.047800  0.1684  0.206300\n",
            "max   0.199600   0.185400  0.2380  0.477400  0.153000  0.057900  0.1932  0.225800\n",
            "\n",
            "================================================================================\n",
            "MOST INTENSE REGIONS (ranked by average intensity)\n",
            "================================================================================\n",
            "  1. Wrists      : 0.4572 m/s\n",
            "  2. Elbows      : 0.2290 m/s\n",
            "  3. Ankles      : 0.2130 m/s\n",
            "  4. Neck        : 0.1983 m/s\n",
            "  5. Shoulders   : 0.1830 m/s\n",
            "  6. Knees       : 0.1811 m/s\n",
            "  7. Spine       : 0.1229 m/s\n",
            "  8. Hips        : 0.0512 m/s\n",
            "\n",
            "================================================================================\n",
            "INTERPRETATION GUIDE\n",
            "================================================================================\n",
            "\n",
            "Intensity Index represents average speed of movement:\n",
            "  ‚Ä¢ 0.10 - 0.30 m/s : Slow, controlled movements\n",
            "  ‚Ä¢ 0.30 - 0.60 m/s : Moderate movement speed\n",
            "  ‚Ä¢ 0.60 - 1.00 m/s : Fast, dynamic movements\n",
            "  ‚Ä¢ >1.00 m/s       : Very fast movements (e.g., sports, rapid reaching)\n",
            "\n",
            "Higher intensity = more active movement per unit time\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# INTENSITY INDEX (Phase 3)\n",
        "# ============================================================\n",
        "\n",
        "intensity_cols = [\n",
        "    \"Intensity_Neck_m_per_s\", \"Intensity_Shoulders_m_per_s\", \"Intensity_Elbows_m_per_s\", \"Intensity_Wrists_m_per_s\",\n",
        "    \"Intensity_Spine_m_per_s\", \"Intensity_Hips_m_per_s\", \"Intensity_Knees_m_per_s\", \"Intensity_Ankles_m_per_s\"\n",
        "]\n",
        "\n",
        "if all(col in df_engineering.columns for col in intensity_cols):\n",
        "    print(\"=\"*80)\n",
        "    print(\"INTENSITY INDEX (meters per second)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nIntensity = Path Length / Duration\")\n",
        "    print(\"  ‚Üí Normalized measure of movement activity\")\n",
        "    print(\"  ‚Üí Allows comparison between sessions of different durations\")\n",
        "    print(\"  ‚Üí Units: m/s (average velocity-like measure)\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    \n",
        "    # Display intensity by anatomical region\n",
        "    intensity_data = []\n",
        "    for _, row in df_engineering.iterrows():\n",
        "        regions = {\n",
        "            \"Run_ID\": row[\"Run_ID\"],\n",
        "            \"Duration_sec\": row.get(\"Duration_sec\", 0),\n",
        "            \"Neck\": row[\"Intensity_Neck_m_per_s\"],\n",
        "            \"Shoulders\": row[\"Intensity_Shoulders_m_per_s\"],\n",
        "            \"Elbows\": row[\"Intensity_Elbows_m_per_s\"],\n",
        "            \"Wrists\": row[\"Intensity_Wrists_m_per_s\"],\n",
        "            \"Spine\": row[\"Intensity_Spine_m_per_s\"],\n",
        "            \"Hips\": row[\"Intensity_Hips_m_per_s\"],\n",
        "            \"Knees\": row[\"Intensity_Knees_m_per_s\"],\n",
        "            \"Ankles\": row[\"Intensity_Ankles_m_per_s\"],\n",
        "        }\n",
        "        intensity_data.append(regions)\n",
        "    \n",
        "    df_intensity = pd.DataFrame(intensity_data)\n",
        "    \n",
        "    print(\"\\nIntensity by Anatomical Region (m/s):\")\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None, 'display.precision', 4):\n",
        "        print(df_intensity.to_string(index=False))\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INTENSITY SUMMARY (across all runs)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    intensity_summary = df_intensity.drop(columns=[\"Run_ID\", \"Duration_sec\"]).describe().loc[[\"mean\", \"min\", \"max\"]]\n",
        "    print(intensity_summary.to_string())\n",
        "    \n",
        "    # Most intense regions\n",
        "    mean_intensity = df_intensity.drop(columns=[\"Run_ID\", \"Duration_sec\"]).mean().sort_values(ascending=False)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MOST INTENSE REGIONS (ranked by average intensity)\")\n",
        "    print(\"=\"*80)\n",
        "    for i, (region, value) in enumerate(mean_intensity.items(), 1):\n",
        "        print(f\"  {i}. {region:12s}: {value:.4f} m/s\")\n",
        "    \n",
        "    # Interpretation guide\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INTERPRETATION GUIDE\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nIntensity Index represents average speed of movement:\")\n",
        "    print(\"  ‚Ä¢ 0.10 - 0.30 m/s : Slow, controlled movements\")\n",
        "    print(\"  ‚Ä¢ 0.30 - 0.60 m/s : Moderate movement speed\")\n",
        "    print(\"  ‚Ä¢ 0.60 - 1.00 m/s : Fast, dynamic movements\")\n",
        "    print(\"  ‚Ä¢ >1.00 m/s       : Very fast movements (e.g., sports, rapid reaching)\")\n",
        "    print(\"\\nHigher intensity = more active movement per unit time\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Intensity Index columns not found. Re-run notebook 06 with Phase 3 updates.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 12: Cross-Session Analysis (Phase 4)\n",
        "\n",
        "**Multi-session comparison and subject-level insights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CROSS-SESSION ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Analyzing 3 sessions across 1 subject(s)\n",
            "\n",
            "================================================================================\n",
            "SUBJECT-LEVEL SUMMARY\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Subject: 671 (3 sessions)\n",
            "================================================================================\n",
            "\n",
            "Sessions:\n",
            "  ‚Ä¢ T1: 255.0s, 30604 frames\n",
            "  ‚Ä¢ T2: 253.0s, 30356 frames\n",
            "  ‚Ä¢ T3: 263.9s, 31674 frames\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Key Metrics Across Sessions:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Path_Length_Total_m:\n",
            "  Mean ¬± Std: 1092.3367 ¬± 9.5772\n",
            "  Range: [1082.4100, 1105.2800]\n",
            "  CV%: 0.88%\n",
            "\n",
            "Intensity_Mean_m_per_s:\n",
            "  Mean ¬± Std: 0.2235 ¬± 0.0059\n",
            "  Range: [0.2158, 0.2300]\n",
            "  CV%: 2.62%\n",
            "\n",
            "Bilateral_Symmetry_Mean:\n",
            "  Mean ¬± Std: 0.9083 ¬± 0.0236\n",
            "  Range: [0.8860, 0.9410]\n",
            "  CV%: 2.60%\n",
            "\n",
            "Raw_Missing_Data_Percent:\n",
            "  Mean ¬± Std: 0.0000 ¬± 0.0000\n",
            "  Range: [0.0000, 0.0000]\n",
            "  CV%: 0.00%\n",
            "\n",
            "Bone_Length_CV_Percent:\n",
            "  Mean ¬± Std: 0.8603 ¬± 0.4822\n",
            "  Range: [0.4630, 1.5390]\n",
            "  CV%: 56.05%\n",
            "  ‚ö†Ô∏è HIGH VARIABILITY (CV > 25%)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Movement Patterns (Anatomical Regions):\n",
            "--------------------------------------------------------------------------------\n",
            "  Wrists      : 0.4572 m/s (œÉ=0.0180)\n",
            "  Elbows      : 0.2290 m/s (œÉ=0.0065)\n",
            "  Knees       : 0.1811 m/s (œÉ=0.0101)\n",
            "  Ankles      : 0.2130 m/s (œÉ=0.0090)\n",
            "\n",
            "================================================================================\n",
            "ANOMALY DETECTION\n",
            "================================================================================\n",
            "\n",
            "Identifying sessions that deviate from subject baseline:\n",
            "\n",
            "671:\n",
            "  ‚úÖ No significant anomalies detected\n",
            "\n",
            "================================================================================\n",
            "TREND ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "671:\n",
            "  Total Movement      : ‚û°Ô∏è STABLE (r=-0.295)\n",
            "  Movement Intensity  : üìâ DECREASING (r=-0.626)\n",
            "  Symmetry            : ‚û°Ô∏è STABLE (r=0.207)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# PHASE 4: CROSS-SESSION ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "if len(df_engineering) > 1:\n",
        "    print(\"=\"*80)\n",
        "    print(\"CROSS-SESSION ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nAnalyzing {len(df_engineering)} sessions across {df_engineering['Subject_ID'].nunique()} subject(s)\")\n",
        "    \n",
        "    # Group by subject\n",
        "    subjects = df_engineering.groupby('Subject_ID')\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUBJECT-LEVEL SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for subject_id, subject_data in subjects:\n",
        "        n_sessions = len(subject_data)\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Subject: {subject_id} ({n_sessions} sessions)\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Session list\n",
        "        print(\"\\nSessions:\")\n",
        "        for idx, row in subject_data.iterrows():\n",
        "            print(f\"  ‚Ä¢ {row['Session_ID']}: {row['Duration_sec']:.1f}s, {row['Total_Frames']} frames\")\n",
        "        \n",
        "        # Key metrics comparison\n",
        "        key_metrics = [\n",
        "            \"Path_Length_Total_m\",\n",
        "            \"Intensity_Mean_m_per_s\",\n",
        "            \"Bilateral_Symmetry_Mean\",\n",
        "            \"Raw_Missing_Data_Percent\",\n",
        "            \"Bone_Length_CV_Percent\",\n",
        "        ]\n",
        "        \n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(\"Key Metrics Across Sessions:\")\n",
        "        print(\"-\"*80)\n",
        "        \n",
        "        for metric in key_metrics:\n",
        "            if metric in subject_data.columns:\n",
        "                values = subject_data[metric].values\n",
        "                mean_val = values.mean()\n",
        "                std_val = values.std()\n",
        "                min_val = values.min()\n",
        "                max_val = values.max()\n",
        "                \n",
        "                # Coefficient of variation (CV%)\n",
        "                cv_pct = (std_val / mean_val * 100) if mean_val > 0 else 0\n",
        "                \n",
        "                print(f\"\\n{metric}:\")\n",
        "                print(f\"  Mean ¬± Std: {mean_val:.4f} ¬± {std_val:.4f}\")\n",
        "                print(f\"  Range: [{min_val:.4f}, {max_val:.4f}]\")\n",
        "                print(f\"  CV%: {cv_pct:.2f}%\")\n",
        "                \n",
        "                # Flag high variability\n",
        "                if cv_pct > 25:\n",
        "                    print(f\"  ‚ö†Ô∏è HIGH VARIABILITY (CV > 25%)\")\n",
        "        \n",
        "        # Movement patterns\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(\"Movement Patterns (Anatomical Regions):\")\n",
        "        print(\"-\"*80)\n",
        "        \n",
        "        region_cols = [\n",
        "            \"Intensity_Wrists_m_per_s\",\n",
        "            \"Intensity_Elbows_m_per_s\",\n",
        "            \"Intensity_Knees_m_per_s\",\n",
        "            \"Intensity_Ankles_m_per_s\",\n",
        "        ]\n",
        "        \n",
        "        for col in region_cols:\n",
        "            if col in subject_data.columns:\n",
        "                region_name = col.replace(\"Intensity_\", \"\").replace(\"_m_per_s\", \"\")\n",
        "                values = subject_data[col].values\n",
        "                print(f\"  {region_name:12s}: {values.mean():.4f} m/s (œÉ={values.std():.4f})\")\n",
        "    \n",
        "    # Anomaly Detection\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ANOMALY DETECTION\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nIdentifying sessions that deviate from subject baseline:\")\n",
        "    \n",
        "    for subject_id, subject_data in subjects:\n",
        "        if len(subject_data) < 2:\n",
        "            print(f\"\\n{subject_id}: Need ‚â•2 sessions for anomaly detection\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\n{subject_id}:\")\n",
        "        \n",
        "        # Check key metrics for outliers (Z-score method)\n",
        "        anomaly_metrics = [\n",
        "            \"Path_Length_Total_m\",\n",
        "            \"Intensity_Mean_m_per_s\",\n",
        "            \"Raw_Missing_Data_Percent\",\n",
        "        ]\n",
        "        \n",
        "        anomalies_found = False\n",
        "        \n",
        "        for metric in anomaly_metrics:\n",
        "            if metric in subject_data.columns:\n",
        "                values = subject_data[metric].values\n",
        "                mean = values.mean()\n",
        "                std = values.std()\n",
        "                \n",
        "                if std > 0:\n",
        "                    z_scores = (values - mean) / std\n",
        "                    \n",
        "                    # Flag outliers (|Z| > 2)\n",
        "                    outlier_mask = np.abs(z_scores) > 2\n",
        "                    \n",
        "                    if outlier_mask.any():\n",
        "                        anomalies_found = True\n",
        "                        outlier_sessions = subject_data[outlier_mask]['Session_ID'].values\n",
        "                        outlier_values = values[outlier_mask]\n",
        "                        outlier_z = z_scores[outlier_mask]\n",
        "                        \n",
        "                        print(f\"\\n  ‚ö†Ô∏è {metric}:\")\n",
        "                        for session, value, z in zip(outlier_sessions, outlier_values, outlier_z):\n",
        "                            direction = \"above\" if z > 0 else \"below\"\n",
        "                            print(f\"      {session}: {value:.4f} (Z={z:.2f}, {direction} baseline)\")\n",
        "        \n",
        "        if not anomalies_found:\n",
        "            print(\"  ‚úÖ No significant anomalies detected\")\n",
        "    \n",
        "    # Trend Analysis (if sessions are ordered by time)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TREND ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for subject_id, subject_data in subjects:\n",
        "        if len(subject_data) < 3:\n",
        "            print(f\"\\n{subject_id}: Need ‚â•3 sessions for trend analysis\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\n{subject_id}:\")\n",
        "        \n",
        "        # Assume sessions are ordered by Session_ID or Run_ID\n",
        "        subject_data_sorted = subject_data.sort_values('Session_ID')\n",
        "        \n",
        "        trend_metrics = [\n",
        "            (\"Path_Length_Total_m\", \"Total Movement\"),\n",
        "            (\"Intensity_Mean_m_per_s\", \"Movement Intensity\"),\n",
        "            (\"Bilateral_Symmetry_Mean\", \"Symmetry\"),\n",
        "        ]\n",
        "        \n",
        "        for metric, label in trend_metrics:\n",
        "            if metric in subject_data_sorted.columns:\n",
        "                values = subject_data_sorted[metric].values\n",
        "                \n",
        "                # Simple linear trend (correlation with session order)\n",
        "                session_order = np.arange(len(values))\n",
        "                correlation = np.corrcoef(session_order, values)[0, 1]\n",
        "                \n",
        "                # Trend direction\n",
        "                if correlation > 0.5:\n",
        "                    trend = \"üìà INCREASING\"\n",
        "                elif correlation < -0.5:\n",
        "                    trend = \"üìâ DECREASING\"\n",
        "                else:\n",
        "                    trend = \"‚û°Ô∏è STABLE\"\n",
        "                \n",
        "                print(f\"  {label:20s}: {trend} (r={correlation:.3f})\")\n",
        "\n",
        "else:\n",
        "    print(\"=\"*80)\n",
        "    print(\"CROSS-SESSION ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\n‚ö†Ô∏è Only {len(df_engineering)} session(s) available.\")\n",
        "    print(\"Cross-session analysis requires multiple sessions.\")\n",
        "    print(\"\\nTo enable:\")\n",
        "    print(\"  1. Process multiple sessions through the pipeline (notebooks 01-06)\")\n",
        "    print(\"  2. Re-run this notebook\")\n",
        "    print(f\"\\nCurrent session: {df_engineering['Run_ID'].iloc[0] if len(df_engineering) > 0 else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 13: Subject Profiles Export (Phase 4)\n",
        "\n",
        "Export aggregated subject-level profiles for longitudinal analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GENERATING SUBJECT PROFILES\n",
            "================================================================================\n",
            "\n",
            "Generated 1 subject profile(s)\n",
            "‚úÖ Saved: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\reports\\Subject_Profiles_20260215_130904.json\n",
            "\n",
            "================================================================================\n",
            "SUBJECT PROFILES SUMMARY\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Subject: 671 (3 sessions)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Movement:\n",
            "  Path Length:  1092.34m (œÉ=9.58)\n",
            "  Intensity:    0.2235 m/s (œÉ=0.0059)\n",
            "\n",
            "Symmetry:\n",
            "  Mean Index:   0.908 (œÉ=0.024)\n",
            "\n",
            "Data Quality:\n",
            "  Missing Data: 0.00% (œÉ=0.00%)\n",
            "  Bone CV%:     0.860% (œÉ=0.482%)\n",
            "\n",
            "Consistency:\n",
            "  Duration sec                  : VERY_CONSISTENT\n",
            "  Path Length Total             : VERY_CONSISTENT\n",
            "  Intensity Mean per s          : VERY_CONSISTENT\n",
            "  Bilateral Symmetry Mean       : VERY_CONSISTENT\n",
            "  Raw Missing Data              : VERY_CONSISTENT\n",
            "  Bone Length CV                : HIGHLY_VARIABLE\n",
            "\n",
            "Movement Pattern (Intensity by Region):\n",
            "  Wrists      : 0.4572 m/s (œÉ=0.0220)\n",
            "  Elbows      : 0.2290 m/s (œÉ=0.0079)\n",
            "  Ankles      : 0.2130 m/s (œÉ=0.0111)\n",
            "  Neck        : 0.1983 m/s (œÉ=0.0012)\n",
            "  Shoulders   : 0.1830 m/s (œÉ=0.0021)\n",
            "  Knees       : 0.1811 m/s (œÉ=0.0124)\n",
            "\n",
            "================================================================================\n",
            "Subject profiles saved to: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\reports\\Subject_Profiles_20260215_130904.json\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SUBJECT PROFILES EXPORT (Phase 4)\n",
        "# ============================================================\n",
        "\n",
        "if len(df_engineering) > 0:\n",
        "    print(\"=\"*80)\n",
        "    print(\"GENERATING SUBJECT PROFILES\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Build subject profiles\n",
        "    subject_profiles = []\n",
        "    \n",
        "    for subject_id in df_engineering['Subject_ID'].unique():\n",
        "        profile = build_subject_profile(df_engineering, subject_id)\n",
        "        subject_profiles.append(profile)\n",
        "    \n",
        "    print(f\"\\nGenerated {len(subject_profiles)} subject profile(s)\")\n",
        "    \n",
        "    # Create output directory and timestamp\n",
        "    REPORTS_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "    os.makedirs(REPORTS_DIR, exist_ok=True)\n",
        "    timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    \n",
        "    # Save to JSON\n",
        "    subject_profiles_path = os.path.join(REPORTS_DIR, f\"Subject_Profiles_{timestamp_str}.json\")\n",
        "    \n",
        "    with open(subject_profiles_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(subject_profiles, f, indent=2)\n",
        "    \n",
        "    print(f\"‚úÖ Saved: {subject_profiles_path}\")\n",
        "    \n",
        "    # Display summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUBJECT PROFILES SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for profile in subject_profiles:\n",
        "        subject_id = profile.get('subject_id', 'Unknown')\n",
        "        n_sessions = profile.get('n_sessions', 0)\n",
        "        \n",
        "        print(f\"\\n{'-'*80}\")\n",
        "        print(f\"Subject: {subject_id} ({n_sessions} sessions)\")\n",
        "        print(f\"{'-'*80}\")\n",
        "        \n",
        "        # Key metrics\n",
        "        if 'Path_Length_Total_m_mean' in profile:\n",
        "            print(f\"\\nMovement:\")\n",
        "            print(f\"  Path Length:  {profile['Path_Length_Total_m_mean']:.2f}m (œÉ={profile['Path_Length_Total_m_std']:.2f})\")\n",
        "            print(f\"  Intensity:    {profile['Intensity_Mean_m_per_s_mean']:.4f} m/s (œÉ={profile['Intensity_Mean_m_per_s_std']:.4f})\")\n",
        "        \n",
        "        if 'Bilateral_Symmetry_Mean_mean' in profile:\n",
        "            print(f\"\\nSymmetry:\")\n",
        "            print(f\"  Mean Index:   {profile['Bilateral_Symmetry_Mean_mean']:.3f} (œÉ={profile['Bilateral_Symmetry_Mean_std']:.3f})\")\n",
        "        \n",
        "        if 'Raw_Missing_Data_Percent_mean' in profile:\n",
        "            print(f\"\\nData Quality:\")\n",
        "            print(f\"  Missing Data: {profile['Raw_Missing_Data_Percent_mean']:.2f}% (œÉ={profile['Raw_Missing_Data_Percent_std']:.2f}%)\")\n",
        "            print(f\"  Bone CV%:     {profile['Bone_Length_CV_Percent_mean']:.3f}% (œÉ={profile['Bone_Length_CV_Percent_std']:.3f}%)\")\n",
        "        \n",
        "        # Consistency assessment\n",
        "        if 'consistency_assessment' in profile:\n",
        "            print(f\"\\nConsistency:\")\n",
        "            for metric, assessment in profile['consistency_assessment'].items():\n",
        "                metric_short = metric.replace(\"_m\", \"\").replace(\"_Percent\", \"\").replace(\"_\", \" \")\n",
        "                print(f\"  {metric_short:30s}: {assessment}\")\n",
        "        \n",
        "        # Movement signature\n",
        "        if 'movement_signature' in profile:\n",
        "            print(f\"\\nMovement Pattern (Intensity by Region):\")\n",
        "            for region, stats in sorted(profile['movement_signature'].items(), key=lambda x: x[1]['mean'], reverse=True):\n",
        "                print(f\"  {region:12s}: {stats['mean']:.4f} m/s (œÉ={stats['std']:.4f})\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Subject profiles saved to: {subject_profiles_path}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No sessions available for subject profile generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CAPTURE BASELINE PROFILE (RAW STATE)\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run_ID</th>\n",
              "      <th>Total_Frames</th>\n",
              "      <th>Duration_sec</th>\n",
              "      <th>Native_Sampling_Rate_Hz</th>\n",
              "      <th>Raw_Missing_Data_Percent</th>\n",
              "      <th>OptiTrack_System_Error_mm</th>\n",
              "      <th>True_Raw_SNR_Mean_dB</th>\n",
              "      <th>True_Raw_SNR_Min_dB</th>\n",
              "      <th>True_Raw_SNR_Max_dB</th>\n",
              "      <th>SNR_Joints_Excellent_Count</th>\n",
              "      <th>SNR_Joints_Failed_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001</td>\n",
              "      <td>30604</td>\n",
              "      <td>255.03</td>\n",
              "      <td>120.005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.1</td>\n",
              "      <td>38.7</td>\n",
              "      <td>52.6</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005</td>\n",
              "      <td>30356</td>\n",
              "      <td>252.96</td>\n",
              "      <td>120.005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47.8</td>\n",
              "      <td>45.7</td>\n",
              "      <td>51.0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000</td>\n",
              "      <td>31674</td>\n",
              "      <td>263.94</td>\n",
              "      <td>120.005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47.3</td>\n",
              "      <td>43.7</td>\n",
              "      <td>51.8</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Run_ID  Total_Frames  Duration_sec  \\\n",
              "0  671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001         30604        255.03   \n",
              "1  671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005         30356        252.96   \n",
              "2  671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000         31674        263.94   \n",
              "\n",
              "   Native_Sampling_Rate_Hz  Raw_Missing_Data_Percent  \\\n",
              "0                  120.005                       0.0   \n",
              "1                  120.005                       0.0   \n",
              "2                  120.005                       0.0   \n",
              "\n",
              "   OptiTrack_System_Error_mm  True_Raw_SNR_Mean_dB  True_Raw_SNR_Min_dB  \\\n",
              "0                        0.0                  44.1                 38.7   \n",
              "1                        0.0                  47.8                 45.7   \n",
              "2                        0.0                  47.3                 43.7   \n",
              "\n",
              "   True_Raw_SNR_Max_dB  SNR_Joints_Excellent_Count  SNR_Joints_Failed_Count  \n",
              "0                 52.6                          19                        0  \n",
              "1                 51.0                          19                        0  \n",
              "2                 51.8                          19                        0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "BASELINE SUMMARY STATISTICS\n",
            "================================================================================\n",
            "\n",
            "Recording Duration:\n",
            "  Total: 771.9 seconds (12.9 minutes)\n",
            "  Mean: 257.3 seconds\n",
            "  Range: 253.0 - 263.9 seconds\n",
            "\n",
            "Raw Data Completeness:\n",
            "  Pristine (0% missing): 3/3 recordings\n",
            "  Mean Missing: 0.000%\n",
            "  Max Missing: 0.000%\n",
            "\n",
            "Inherent Signal Quality (Pre-Processing SNR):\n",
            "  Mean SNR: 46.4 dB\n",
            "  Best Recording: 52.6 dB\n",
            "  Worst Recording: 38.7 dB\n",
            "  Interpretation: EXCELLENT - Publication-quality capture\n",
            "\n",
            "OptiTrack System Calibration:\n",
            "  Mean Error: 0.000 mm\n",
            "  Max Error: 0.000 mm\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"CAPTURE BASELINE PROFILE (RAW STATE)\")\n",
        "\n",
        "cols_baseline = [\n",
        "    'Run_ID',\n",
        "    'Total_Frames',\n",
        "    'Duration_sec',\n",
        "    'Native_Sampling_Rate_Hz',\n",
        "    'Raw_Missing_Data_Percent',\n",
        "    'OptiTrack_System_Error_mm',\n",
        "    'True_Raw_SNR_Mean_dB',\n",
        "    'True_Raw_SNR_Min_dB',\n",
        "    'True_Raw_SNR_Max_dB',\n",
        "    'SNR_Joints_Excellent_Count',\n",
        "    'SNR_Joints_Failed_Count'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_baseline])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BASELINE SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nRecording Duration:\")\n",
        "print(f\"  Total: {df_engineering['Duration_sec'].sum():.1f} seconds ({df_engineering['Duration_sec'].sum()/60:.1f} minutes)\")\n",
        "print(f\"  Mean: {df_engineering['Duration_sec'].mean():.1f} seconds\")\n",
        "print(f\"  Range: {df_engineering['Duration_sec'].min():.1f} - {df_engineering['Duration_sec'].max():.1f} seconds\")\n",
        "\n",
        "print(f\"\\nRaw Data Completeness:\")\n",
        "pristine_count = (df_engineering['Raw_Missing_Data_Percent'] == 0).sum()\n",
        "print(f\"  Pristine (0% missing): {pristine_count}/{len(df_engineering)} recordings\")\n",
        "print(f\"  Mean Missing: {df_engineering['Raw_Missing_Data_Percent'].mean():.3f}%\")\n",
        "print(f\"  Max Missing: {df_engineering['Raw_Missing_Data_Percent'].max():.3f}%\")\n",
        "\n",
        "print(f\"\\nInherent Signal Quality (Pre-Processing SNR):\")\n",
        "print(f\"  Mean SNR: {df_engineering['True_Raw_SNR_Mean_dB'].mean():.1f} dB\")\n",
        "print(f\"  Best Recording: {df_engineering['True_Raw_SNR_Max_dB'].max():.1f} dB\")\n",
        "print(f\"  Worst Recording: {df_engineering['True_Raw_SNR_Min_dB'].min():.1f} dB\")\n",
        "\n",
        "# Interpret SNR levels\n",
        "mean_snr = df_engineering['True_Raw_SNR_Mean_dB'].mean()\n",
        "if mean_snr >= 30:\n",
        "    snr_interpretation = \"EXCELLENT - Publication-quality capture\"\n",
        "elif mean_snr >= 20:\n",
        "    snr_interpretation = \"GOOD - Acceptable for research\"\n",
        "elif mean_snr >= 15:\n",
        "    snr_interpretation = \"ACCEPTABLE - Review recommended\"\n",
        "else:\n",
        "    snr_interpretation = \"POOR - Check capture environment\"\n",
        "\n",
        "print(f\"  Interpretation: {snr_interpretation}\")\n",
        "\n",
        "print(f\"\\nOptiTrack System Calibration:\")\n",
        "print(f\"  Mean Error: {df_engineering['OptiTrack_System_Error_mm'].mean():.3f} mm\")\n",
        "print(f\"  Max Error: {df_engineering['OptiTrack_System_Error_mm'].max():.3f} mm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"structure\"></a>\n",
        "## 5. Structural Integrity\n",
        "\n",
        "**Purpose:** Verify skeleton hierarchy and biomechanical stability.\n",
        "\n",
        "This section documents:\n",
        "- Skeleton completeness (all expected segments present?)\n",
        "- Bone length stability (CV% - rigid body assumption validity)\n",
        "- Subject anthropometry (height, mass)\n",
        "- Static pose calibration offsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "STRUCTURAL INTEGRITY PROFILE\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run_ID</th>\n",
              "      <th>Skeleton_Segments_Found</th>\n",
              "      <th>Skeleton_Segments_Missing</th>\n",
              "      <th>Bone_Length_CV_Percent</th>\n",
              "      <th>Worst_Bone_Segment</th>\n",
              "      <th>Subject_Height_cm</th>\n",
              "      <th>Subject_Mass_kg</th>\n",
              "      <th>Left_Arm_Offset_deg</th>\n",
              "      <th>Right_Arm_Offset_deg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001</td>\n",
              "      <td>51</td>\n",
              "      <td>0</td>\n",
              "      <td>0.463</td>\n",
              "      <td>Hips-&gt;Spine</td>\n",
              "      <td>152.66</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.65</td>\n",
              "      <td>11.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005</td>\n",
              "      <td>51</td>\n",
              "      <td>0</td>\n",
              "      <td>0.579</td>\n",
              "      <td>Hips-&gt;Spine</td>\n",
              "      <td>149.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.79</td>\n",
              "      <td>9.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000</td>\n",
              "      <td>51</td>\n",
              "      <td>0</td>\n",
              "      <td>1.539</td>\n",
              "      <td>Hips-&gt;Spine</td>\n",
              "      <td>153.68</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.01</td>\n",
              "      <td>4.82</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Run_ID  Skeleton_Segments_Found  \\\n",
              "0  671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001                       51   \n",
              "1  671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005                       51   \n",
              "2  671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000                       51   \n",
              "\n",
              "   Skeleton_Segments_Missing  Bone_Length_CV_Percent Worst_Bone_Segment  \\\n",
              "0                          0                   0.463        Hips->Spine   \n",
              "1                          0                   0.579        Hips->Spine   \n",
              "2                          0                   1.539        Hips->Spine   \n",
              "\n",
              "   Subject_Height_cm  Subject_Mass_kg  Left_Arm_Offset_deg  \\\n",
              "0             152.66              0.0                12.65   \n",
              "1             149.98              0.0                10.79   \n",
              "2             153.68              0.0                 8.01   \n",
              "\n",
              "   Right_Arm_Offset_deg  \n",
              "0                 11.64  \n",
              "1                  9.69  \n",
              "2                  4.82  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "SKELETON STABILITY ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Bone Length Coefficient of Variation:\n",
            "  Mean CV: 0.8603%\n",
            "  Range: 0.4630% - 1.5390%\n",
            "\n",
            "  Interpretation (R√°cz et al., 2025):\n",
            "    CV < 0.5%:  Excellent rigidity (research-grade)\n",
            "    CV 0.5-1%:  Good (acceptable soft tissue artifact)\n",
            "    CV 1-2%:    Marginal (review recommended)\n",
            "    CV > 2%:    Poor (tracking or marker placement issue)\n",
            "\n",
            "Worst Bone Segments (Most Variable):\n",
            "  Hips->Spine: 3 recordings, Mean CV = 0.8603%\n",
            "\n",
            "Anthropometry:\n",
            "  Mean Height: 152.1 cm\n",
            "  Height Range: 150.0 - 153.7 cm\n",
            "\n",
            "Static Pose Calibration Offsets:\n",
            "  Left Arm Mean: 10.48¬∞\n",
            "  Right Arm Mean: 8.72¬∞\n",
            "  Max Bilateral Asymmetry: 3.19¬∞\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"STRUCTURAL INTEGRITY PROFILE\")\n",
        "\n",
        "cols_structure = [\n",
        "    'Run_ID',\n",
        "    'Skeleton_Segments_Found',\n",
        "    'Skeleton_Segments_Missing',\n",
        "    'Bone_Length_CV_Percent',\n",
        "    'Worst_Bone_Segment',\n",
        "    'Subject_Height_cm',\n",
        "    'Subject_Mass_kg',\n",
        "    'Left_Arm_Offset_deg',\n",
        "    'Right_Arm_Offset_deg'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_structure])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SKELETON STABILITY ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nBone Length Coefficient of Variation:\")\n",
        "print(f\"  Mean CV: {df_engineering['Bone_Length_CV_Percent'].mean():.4f}%\")\n",
        "print(f\"  Range: {df_engineering['Bone_Length_CV_Percent'].min():.4f}% - {df_engineering['Bone_Length_CV_Percent'].max():.4f}%\")\n",
        "print(f\"\\n  Interpretation (R√°cz et al., 2025):\")\n",
        "print(f\"    CV < 0.5%:  Excellent rigidity (research-grade)\")\n",
        "print(f\"    CV 0.5-1%:  Good (acceptable soft tissue artifact)\")\n",
        "print(f\"    CV 1-2%:    Marginal (review recommended)\")\n",
        "print(f\"    CV > 2%:    Poor (tracking or marker placement issue)\")\n",
        "\n",
        "print(f\"\\nWorst Bone Segments (Most Variable):\")\n",
        "worst_bones = df_engineering.groupby('Worst_Bone_Segment')['Bone_Length_CV_Percent'].agg(['count', 'mean']).sort_values('count', ascending=False)\n",
        "for bone, stats in worst_bones.head(5).iterrows():\n",
        "    print(f\"  {bone}: {int(stats['count'])} recordings, Mean CV = {stats['mean']:.4f}%\")\n",
        "\n",
        "print(f\"\\nAnthropometry:\")\n",
        "print(f\"  Mean Height: {df_engineering['Subject_Height_cm'].mean():.1f} cm\")\n",
        "print(f\"  Height Range: {df_engineering['Subject_Height_cm'].min():.1f} - {df_engineering['Subject_Height_cm'].max():.1f} cm\")\n",
        "valid_mass = df_engineering['Subject_Mass_kg'][df_engineering['Subject_Mass_kg'] > 0]\n",
        "if len(valid_mass) > 0:\n",
        "    print(f\"  Mean Mass: {valid_mass.mean():.1f} kg\")\n",
        "\n",
        "print(f\"\\nStatic Pose Calibration Offsets:\")\n",
        "print(f\"  Left Arm Mean: {df_engineering['Left_Arm_Offset_deg'].mean():.2f}¬∞\")\n",
        "print(f\"  Right Arm Mean: {df_engineering['Right_Arm_Offset_deg'].mean():.2f}¬∞\")\n",
        "print(f\"  Max Bilateral Asymmetry: {abs(df_engineering['Left_Arm_Offset_deg'] - df_engineering['Right_Arm_Offset_deg']).max():.2f}¬∞\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "SELECTED KINEMATIC SEGMENTS (19 Joints)\n",
            "================================================================================\n",
            "\n",
            "These segments are used for rotation analysis and kinematic computation:\n",
            "\n",
            "Representative Run: 671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001...\n",
            "\n",
            "  Trunk:\n",
            "    - Head\n",
            "    - Hips\n",
            "    - Neck\n",
            "    - Spine\n",
            "    - Spine1\n",
            "  Left Upper Limb:\n",
            "    - LeftArm\n",
            "    - LeftForeArm\n",
            "    - LeftHand\n",
            "    - LeftShoulder\n",
            "  Right Upper Limb:\n",
            "    - RightArm\n",
            "    - RightForeArm\n",
            "    - RightHand\n",
            "    - RightShoulder\n",
            "  Left Lower Limb:\n",
            "    - LeftFoot\n",
            "    - LeftLeg\n",
            "    - LeftUpLeg\n",
            "  Right Lower Limb:\n",
            "    - RightFoot\n",
            "    - RightLeg\n",
            "    - RightUpLeg\n",
            "\n",
            "Total: 19 segments\n"
          ]
        }
      ],
      "source": [
        "# Display selected segments for one representative run\n",
        "if len(runs_data) > 0:\n",
        "    representative_run = list(runs_data.keys())[0]\n",
        "    selected_segments = extract_selected_segments(runs_data[representative_run])\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"SELECTED KINEMATIC SEGMENTS (19 Joints)\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nThese segments are used for rotation analysis and kinematic computation:\")\n",
        "    print(f\"\\nRepresentative Run: {representative_run[:60]}...\\n\")\n",
        "    \n",
        "    # Group by body region\n",
        "    regions = {\n",
        "        \"Trunk\": [s for s in selected_segments if s in ['Hips', 'Spine', 'Spine1', 'Neck', 'Head']],\n",
        "        \"Left Upper Limb\": [s for s in selected_segments if s.startswith('Left') and s in ['LeftShoulder', 'LeftArm', 'LeftForeArm', 'LeftHand']],\n",
        "        \"Right Upper Limb\": [s for s in selected_segments if s.startswith('Right') and s in ['RightShoulder', 'RightArm', 'RightForeArm', 'RightHand']],\n",
        "        \"Left Lower Limb\": [s for s in selected_segments if s.startswith('Left') and s in ['LeftUpLeg', 'LeftLeg', 'LeftFoot']],\n",
        "        \"Right Lower Limb\": [s for s in selected_segments if s.startswith('Right') and s in ['RightUpLeg', 'RightLeg', 'RightFoot']]\n",
        "    }\n",
        "    \n",
        "    for region, segments in regions.items():\n",
        "        if segments:\n",
        "            print(f\"  {region}:\")\n",
        "            for seg in segments:\n",
        "                print(f\"    - {seg}\")\n",
        "    \n",
        "    print(f\"\\nTotal: {len(selected_segments)} segments\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"signal\"></a>\n",
        "## 6. Signal Quality Profile\n",
        "\n",
        "**Purpose:** TRUE RAW SNR - Capture quality assessment BEFORE any filtering.\n",
        "\n",
        "Method: Raw data frequency analysis (signal: 0.5-10Hz, noise: 15-50Hz)\n",
        "\n",
        "This measures inherent capture quality, NOT filtering effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SIGNAL QUALITY PROFILE (TRUE RAW SNR)\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run_ID</th>\n",
              "      <th>True_Raw_SNR_Mean_dB</th>\n",
              "      <th>True_Raw_SNR_Min_dB</th>\n",
              "      <th>True_Raw_SNR_Max_dB</th>\n",
              "      <th>SNR_Joints_Excellent_Count</th>\n",
              "      <th>SNR_Joints_Failed_Count</th>\n",
              "      <th>SNR_Failed_Joint_List</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001</td>\n",
              "      <td>44.1</td>\n",
              "      <td>38.7</td>\n",
              "      <td>52.6</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005</td>\n",
              "      <td>47.8</td>\n",
              "      <td>45.7</td>\n",
              "      <td>51.0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000</td>\n",
              "      <td>47.3</td>\n",
              "      <td>43.7</td>\n",
              "      <td>51.8</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Run_ID  True_Raw_SNR_Mean_dB  \\\n",
              "0  671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001                  44.1   \n",
              "1  671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005                  47.8   \n",
              "2  671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000                  47.3   \n",
              "\n",
              "   True_Raw_SNR_Min_dB  True_Raw_SNR_Max_dB  SNR_Joints_Excellent_Count  \\\n",
              "0                 38.7                 52.6                          19   \n",
              "1                 45.7                 51.0                          19   \n",
              "2                 43.7                 51.8                          19   \n",
              "\n",
              "   SNR_Joints_Failed_Count SNR_Failed_Joint_List  \n",
              "0                        0                    []  \n",
              "1                        0                    []  \n",
              "2                        0                    []  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "SNR INTERPRETATION GUIDE\n",
            "================================================================================\n",
            "\n",
            "SNR > 30 dB:  EXCELLENT - Publication-quality, minimal noise\n",
            "SNR 20-30 dB: GOOD - Acceptable for research, moderate noise\n",
            "SNR 15-20 dB: ACCEPTABLE - Usable but review recommended\n",
            "SNR < 15 dB:  POOR - High noise, check capture environment\n",
            "\n",
            "================================================================================\n",
            "SNR DISTRIBUTION ACROSS RECORDINGS\n",
            "================================================================================\n",
            "\n",
            "True_Raw_SNR_Mean_dB\n",
            "POOR          0\n",
            "ACCEPTABLE    0\n",
            "GOOD          0\n",
            "EXCELLENT     3\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"SIGNAL QUALITY PROFILE (TRUE RAW SNR)\")\n",
        "\n",
        "cols_signal = [\n",
        "    'Run_ID',\n",
        "    'True_Raw_SNR_Mean_dB',\n",
        "    'True_Raw_SNR_Min_dB',\n",
        "    'True_Raw_SNR_Max_dB',\n",
        "    'SNR_Joints_Excellent_Count',\n",
        "    'SNR_Joints_Failed_Count',\n",
        "    'SNR_Failed_Joint_List'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_signal])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SNR INTERPRETATION GUIDE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nSNR > 30 dB:  EXCELLENT - Publication-quality, minimal noise\")\n",
        "print(f\"SNR 20-30 dB: GOOD - Acceptable for research, moderate noise\")\n",
        "print(f\"SNR 15-20 dB: ACCEPTABLE - Usable but review recommended\")\n",
        "print(f\"SNR < 15 dB:  POOR - High noise, check capture environment\")\n",
        "\n",
        "# Histogram of SNR distribution\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SNR DISTRIBUTION ACROSS RECORDINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "snr_bins = pd.cut(df_engineering['True_Raw_SNR_Mean_dB'], bins=[0, 15, 20, 30, 100], labels=['POOR', 'ACCEPTABLE', 'GOOD', 'EXCELLENT'])\n",
        "snr_counts = snr_bins.value_counts().sort_index()\n",
        "print(f\"\\n{snr_counts.to_string()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"processing\"></a>\n",
        "## 7. Processing Transparency\n",
        "\n",
        "**Purpose:** Document exactly what was done to the raw data.\n",
        "\n",
        "This section provides:\n",
        "- Interpolation methods used\n",
        "- Resampling parameters\n",
        "- Filtering strategy\n",
        "- 3-stage cleaning metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PROCESSING TRANSPARENCY\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run_ID</th>\n",
              "      <th>Interpolation_Method_Positions</th>\n",
              "      <th>Interpolation_Method_Rotations</th>\n",
              "      <th>Resampling_Target_Hz</th>\n",
              "      <th>Temporal_Grid_Std_ms</th>\n",
              "      <th>Filtering_Mode</th>\n",
              "      <th>Filter_Cutoff_Weighted_Avg_Hz</th>\n",
              "      <th>Filter_Residual_RMS_mm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001</td>\n",
              "      <td>CubicSpline</td>\n",
              "      <td>SLERP</td>\n",
              "      <td>120.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3_stage_pipeline</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005</td>\n",
              "      <td>CubicSpline</td>\n",
              "      <td>SLERP</td>\n",
              "      <td>120.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3_stage_pipeline</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000</td>\n",
              "      <td>CubicSpline</td>\n",
              "      <td>SLERP</td>\n",
              "      <td>120.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3_stage_pipeline</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Run_ID  \\\n",
              "0  671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001   \n",
              "1  671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005   \n",
              "2  671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000   \n",
              "\n",
              "  Interpolation_Method_Positions Interpolation_Method_Rotations  \\\n",
              "0                    CubicSpline                          SLERP   \n",
              "1                    CubicSpline                          SLERP   \n",
              "2                    CubicSpline                          SLERP   \n",
              "\n",
              "   Resampling_Target_Hz  Temporal_Grid_Std_ms    Filtering_Mode  \\\n",
              "0                 120.0                   0.0  3_stage_pipeline   \n",
              "1                 120.0                   0.0  3_stage_pipeline   \n",
              "2                 120.0                   0.0  3_stage_pipeline   \n",
              "\n",
              "   Filter_Cutoff_Weighted_Avg_Hz  Filter_Residual_RMS_mm  \n",
              "0                            0.0                     0.0  \n",
              "1                            0.0                     0.0  \n",
              "2                            0.0                     0.0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "3-STAGE CLEANING METRICS\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run_ID</th>\n",
              "      <th>Stage1_Total_Artifacts_Detected</th>\n",
              "      <th>Stage1_Artifact_Percent</th>\n",
              "      <th>Stage2_Hampel_Outliers</th>\n",
              "      <th>Stage2_Hampel_Percent</th>\n",
              "      <th>Stage3_Winter_Cutoff_Min_Hz</th>\n",
              "      <th>Stage3_Winter_Cutoff_Max_Hz</th>\n",
              "      <th>Stage3_Winter_Cutoff_Mean_Hz</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001</td>\n",
              "      <td>7453</td>\n",
              "      <td>0.427</td>\n",
              "      <td>5416</td>\n",
              "      <td>0.310</td>\n",
              "      <td>14.5</td>\n",
              "      <td>15.2</td>\n",
              "      <td>14.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005</td>\n",
              "      <td>8676</td>\n",
              "      <td>0.501</td>\n",
              "      <td>4270</td>\n",
              "      <td>0.247</td>\n",
              "      <td>14.5</td>\n",
              "      <td>15.2</td>\n",
              "      <td>14.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000</td>\n",
              "      <td>9880</td>\n",
              "      <td>0.547</td>\n",
              "      <td>6156</td>\n",
              "      <td>0.341</td>\n",
              "      <td>14.5</td>\n",
              "      <td>15.2</td>\n",
              "      <td>14.87</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Run_ID  \\\n",
              "0  671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001   \n",
              "1  671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005   \n",
              "2  671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000   \n",
              "\n",
              "   Stage1_Total_Artifacts_Detected  Stage1_Artifact_Percent  \\\n",
              "0                             7453                    0.427   \n",
              "1                             8676                    0.501   \n",
              "2                             9880                    0.547   \n",
              "\n",
              "   Stage2_Hampel_Outliers  Stage2_Hampel_Percent  Stage3_Winter_Cutoff_Min_Hz  \\\n",
              "0                    5416                  0.310                         14.5   \n",
              "1                    4270                  0.247                         14.5   \n",
              "2                    6156                  0.341                         14.5   \n",
              "\n",
              "   Stage3_Winter_Cutoff_Max_Hz  Stage3_Winter_Cutoff_Mean_Hz  \n",
              "0                         15.2                         14.69  \n",
              "1                         15.2                         14.66  \n",
              "2                         15.2                         14.87  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing Summary:\n",
            "  Stage 1 (Artifact Detection):\n",
            "    Total artifacts: 26009 frames\n",
            "    Mean rate: 0.492%\n",
            "\n",
            "  Stage 2 (Hampel Filter):\n",
            "    Total outliers: 15842 frames\n",
            "    Mean rate: 0.299%\n",
            "\n",
            "  Stage 3 (Adaptive Winter):\n",
            "    Cutoff range: 14.5 - 15.2 Hz\n",
            "    Mean cutoff: 14.7 Hz\n",
            "\n",
            "  Filter Residual (Price of Smoothing):\n",
            "    Mean RMS: 0.00 mm\n",
            "    Range: 0.00 - 0.00 mm\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"PROCESSING TRANSPARENCY\")\n",
        "\n",
        "cols_processing = [\n",
        "    'Run_ID',\n",
        "    'Interpolation_Method_Positions',\n",
        "    'Interpolation_Method_Rotations',\n",
        "    'Resampling_Target_Hz',\n",
        "    'Temporal_Grid_Std_ms',\n",
        "    'Filtering_Mode',\n",
        "    'Filter_Cutoff_Weighted_Avg_Hz',\n",
        "    'Filter_Residual_RMS_mm'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_processing])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3-STAGE CLEANING METRICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "cols_cleaning = [\n",
        "    'Run_ID',\n",
        "    'Stage1_Total_Artifacts_Detected',\n",
        "    'Stage1_Artifact_Percent',\n",
        "    'Stage2_Hampel_Outliers',\n",
        "    'Stage2_Hampel_Percent',\n",
        "    'Stage3_Winter_Cutoff_Min_Hz',\n",
        "    'Stage3_Winter_Cutoff_Max_Hz',\n",
        "    'Stage3_Winter_Cutoff_Mean_Hz'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_cleaning])\n",
        "\n",
        "print(\"\\nProcessing Summary:\")\n",
        "print(f\"  Stage 1 (Artifact Detection):\")\n",
        "print(f\"    Total artifacts: {df_engineering['Stage1_Total_Artifacts_Detected'].sum()} frames\")\n",
        "print(f\"    Mean rate: {df_engineering['Stage1_Artifact_Percent'].mean():.3f}%\")\n",
        "\n",
        "print(f\"\\n  Stage 2 (Hampel Filter):\")\n",
        "print(f\"    Total outliers: {df_engineering['Stage2_Hampel_Outliers'].sum()} frames\")\n",
        "print(f\"    Mean rate: {df_engineering['Stage2_Hampel_Percent'].mean():.3f}%\")\n",
        "\n",
        "print(f\"\\n  Stage 3 (Adaptive Winter):\")\n",
        "print(f\"    Cutoff range: {df_engineering['Stage3_Winter_Cutoff_Min_Hz'].min():.1f} - {df_engineering['Stage3_Winter_Cutoff_Max_Hz'].max():.1f} Hz\")\n",
        "print(f\"    Mean cutoff: {df_engineering['Stage3_Winter_Cutoff_Mean_Hz'].mean():.1f} Hz\")\n",
        "\n",
        "print(f\"\\n  Filter Residual (Price of Smoothing):\")\n",
        "print(f\"    Mean RMS: {df_engineering['Filter_Residual_RMS_mm'].mean():.2f} mm\")\n",
        "print(f\"    Range: {df_engineering['Filter_Residual_RMS_mm'].min():.2f} - {df_engineering['Filter_Residual_RMS_mm'].max():.2f} mm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"kinematics\"></a>\n",
        "## 8. Kinematic Extremes (Processed Output)\n",
        "\n",
        "**Purpose:** Report the final kinematic values after all processing.\n",
        "\n",
        "These are the peak velocities and accelerations extracted from the cleaned data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "KINEMATIC EXTREMES (PROCESSED OUTPUT)\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run_ID</th>\n",
              "      <th>Max_Angular_Velocity_deg_s</th>\n",
              "      <th>Max_Angular_Acceleration_deg_s2</th>\n",
              "      <th>Max_Linear_Velocity_mm_s</th>\n",
              "      <th>Max_Linear_Acceleration_mm_s2</th>\n",
              "      <th>Path_Length_Total_m</th>\n",
              "      <th>Intensity_Mean_m_per_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001</td>\n",
              "      <td>1456.67</td>\n",
              "      <td>34366.89</td>\n",
              "      <td>3400.89</td>\n",
              "      <td>47780.10</td>\n",
              "      <td>1089.32</td>\n",
              "      <td>0.2248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005</td>\n",
              "      <td>1311.73</td>\n",
              "      <td>28406.33</td>\n",
              "      <td>3853.08</td>\n",
              "      <td>46353.40</td>\n",
              "      <td>1105.28</td>\n",
              "      <td>0.2300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000</td>\n",
              "      <td>1693.70</td>\n",
              "      <td>40472.58</td>\n",
              "      <td>4299.80</td>\n",
              "      <td>57911.68</td>\n",
              "      <td>1082.41</td>\n",
              "      <td>0.2158</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Run_ID  Max_Angular_Velocity_deg_s  \\\n",
              "0  671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001                     1456.67   \n",
              "1  671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005                     1311.73   \n",
              "2  671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000                     1693.70   \n",
              "\n",
              "   Max_Angular_Acceleration_deg_s2  Max_Linear_Velocity_mm_s  \\\n",
              "0                         34366.89                   3400.89   \n",
              "1                         28406.33                   3853.08   \n",
              "2                         40472.58                   4299.80   \n",
              "\n",
              "   Max_Linear_Acceleration_mm_s2  Path_Length_Total_m  Intensity_Mean_m_per_s  \n",
              "0                       47780.10              1089.32                  0.2248  \n",
              "1                       46353.40              1105.28                  0.2300  \n",
              "2                       57911.68              1082.41                  0.2158  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "KINEMATIC SUMMARY STATISTICS\n",
            "================================================================================\n",
            "\n",
            "Angular Velocity:\n",
            "  Peak (across all recordings): 1693.70 deg/s\n",
            "  Mean of maxima: 1487.37 deg/s\n",
            "\n",
            "  Reference Values:\n",
            "    Normal movement: < 800 deg/s\n",
            "    Athletic: 800-1500 deg/s\n",
            "    Gaga dance (distal): up to 2250 deg/s\n",
            "    Tracking artifact threshold: > 2500 deg/s\n",
            "\n",
            "Angular Acceleration:\n",
            "  Peak: 40473 deg/s¬≤\n",
            "  Mean of maxima: 34415 deg/s¬≤\n",
            "\n",
            "  Reference Values:\n",
            "    Smooth movement: < 30,000 deg/s¬≤\n",
            "    Rapid transitions: 30,000-50,000 deg/s¬≤\n",
            "    Extreme/impact: > 50,000 deg/s¬≤\n",
            "\n",
            "Linear Acceleration:\n",
            "  Peak: 57912 mm/s¬≤ (57.9 m/s¬≤)\n",
            "  Mean of maxima: 50682 mm/s¬≤ (50.7 m/s¬≤)\n",
            "\n",
            "Path Length (Total Movement):\n",
            "  Total: 3277.0 meters\n",
            "  Mean per recording: 1092.3 meters\n",
            "  Range: 1082.4 - 1105.3 meters\n",
            "  Most active region: Wrists (117.56m average)\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"KINEMATIC EXTREMES (PROCESSED OUTPUT)\")\n",
        "\n",
        "cols_kinematics = [\n",
        "    'Run_ID',\n",
        "    'Max_Angular_Velocity_deg_s',\n",
        "    'Max_Angular_Acceleration_deg_s2',\n",
        "    'Max_Linear_Velocity_mm_s',\n",
        "    'Max_Linear_Acceleration_mm_s2',\n",
        "    'Path_Length_Total_m',\n",
        "    'Intensity_Mean_m_per_s'\n",
        "]\n",
        "\n",
        "# Only display columns that exist\n",
        "cols_to_display = [col for col in cols_kinematics if col in df_engineering.columns]\n",
        "display(df_engineering[cols_to_display])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KINEMATIC SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nAngular Velocity:\")\n",
        "print(f\"  Peak (across all recordings): {df_engineering['Max_Angular_Velocity_deg_s'].max():.2f} deg/s\")\n",
        "print(f\"  Mean of maxima: {df_engineering['Max_Angular_Velocity_deg_s'].mean():.2f} deg/s\")\n",
        "print(f\"\\n  Reference Values:\")\n",
        "print(f\"    Normal movement: < 800 deg/s\")\n",
        "print(f\"    Athletic: 800-1500 deg/s\")\n",
        "print(f\"    Gaga dance (distal): up to 2250 deg/s\")\n",
        "print(f\"    Tracking artifact threshold: > 2500 deg/s\")\n",
        "\n",
        "print(f\"\\nAngular Acceleration:\")\n",
        "print(f\"  Peak: {df_engineering['Max_Angular_Acceleration_deg_s2'].max():.0f} deg/s¬≤\")\n",
        "print(f\"  Mean of maxima: {df_engineering['Max_Angular_Acceleration_deg_s2'].mean():.0f} deg/s¬≤\")\n",
        "print(f\"\\n  Reference Values:\")\n",
        "print(f\"    Smooth movement: < 30,000 deg/s¬≤\")\n",
        "print(f\"    Rapid transitions: 30,000-50,000 deg/s¬≤\")\n",
        "print(f\"    Extreme/impact: > 50,000 deg/s¬≤\")\n",
        "\n",
        "print(f\"\\nLinear Acceleration:\")\n",
        "print(f\"  Peak: {df_engineering['Max_Linear_Acceleration_mm_s2'].max():.0f} mm/s¬≤ ({df_engineering['Max_Linear_Acceleration_mm_s2'].max()/1000:.1f} m/s¬≤)\")\n",
        "print(f\"  Mean of maxima: {df_engineering['Max_Linear_Acceleration_mm_s2'].mean():.0f} mm/s¬≤ ({df_engineering['Max_Linear_Acceleration_mm_s2'].mean()/1000:.1f} m/s¬≤)\")\n",
        "\n",
        "# Path Length Summary (using new anatomical region columns)\n",
        "if 'Path_Length_Total_m' in df_engineering.columns:\n",
        "    print(f\"\\nPath Length (Total Movement):\")\n",
        "    print(f\"  Total: {df_engineering['Path_Length_Total_m'].sum():.1f} meters\")\n",
        "    print(f\"  Mean per recording: {df_engineering['Path_Length_Total_m'].mean():.1f} meters\")\n",
        "    print(f\"  Range: {df_engineering['Path_Length_Total_m'].min():.1f} - {df_engineering['Path_Length_Total_m'].max():.1f} meters\")\n",
        "    \n",
        "    # Show most active region\n",
        "    region_cols = ['Path_Neck_m', 'Path_Shoulders_m', 'Path_Elbows_m', 'Path_Wrists_m', \n",
        "                   'Path_Spine_m', 'Path_Hips_m', 'Path_Knees_m', 'Path_Ankles_m']\n",
        "    if all(col in df_engineering.columns for col in region_cols):\n",
        "        region_means = {col.replace('Path_', '').replace('_m', ''): df_engineering[col].mean() \n",
        "                       for col in region_cols}\n",
        "        most_active = max(region_means.items(), key=lambda x: x[1])\n",
        "        print(f\"  Most active region: {most_active[0]} ({most_active[1]:.2f}m average)\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è Path Length: Not computed (missing Path_Length_Total_m column)\")\n",
        "    print(f\"   ‚Üí Re-run notebook 06_ultimate_kinematics.ipynb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"noise\"></a>\n",
        "## 9. Per-Joint Noise Profile\n",
        "\n",
        "**Purpose:** Root cause analysis for noisy segments.\n",
        "\n",
        "This section identifies:\n",
        "- Which joints have the most outlier frames?\n",
        "- Is the noise localized (one joint) or systemic (whole skeleton)?\n",
        "- Are the outliers sporadic glitches or sustained high-intensity movement?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PER-JOINT NOISE PROFILE\n",
            "================================================================================\n",
            "\n",
            "Analyzing: 671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001\n",
            "(Run with highest outlier rate: 2.872%)\n",
            "\n",
            "Per-Joint Outlier Profile:\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Joint</th>\n",
              "      <th>WARNING_Frames</th>\n",
              "      <th>ALERT_Frames</th>\n",
              "      <th>CRITICAL_Frames</th>\n",
              "      <th>Outlier_Percent</th>\n",
              "      <th>Classification</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>LeftHand</td>\n",
              "      <td>830</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>2.872</td>\n",
              "      <td>Systemic_Noise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>RightHand</td>\n",
              "      <td>371</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>1.287</td>\n",
              "      <td>Systemic_Noise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>LeftForeArm</td>\n",
              "      <td>345</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.127</td>\n",
              "      <td>Systemic_Noise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>RightForeArm</td>\n",
              "      <td>262</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0.899</td>\n",
              "      <td>Sporadic_Glitches</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>RightFoot</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.042</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LeftLeg</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.029</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>RightArm</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>RightShoulder</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>LeftArm</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>LeftShoulder</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hips</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Spine</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RightUpLeg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LeftFoot</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LeftUpLeg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Head</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Neck</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Spine1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>RightLeg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>Clean</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Joint  WARNING_Frames  ALERT_Frames  CRITICAL_Frames  \\\n",
              "14       LeftHand             830            49                0   \n",
              "18      RightHand             371            23                0   \n",
              "13    LeftForeArm             345             0                0   \n",
              "17   RightForeArm             262            13                0   \n",
              "10      RightFoot              13             0                0   \n",
              "6         LeftLeg               9             0                0   \n",
              "16       RightArm               0             0                0   \n",
              "15  RightShoulder               0             0                0   \n",
              "12        LeftArm               0             0                0   \n",
              "11   LeftShoulder               0             0                0   \n",
              "0            Hips               0             0                0   \n",
              "1           Spine               0             0                0   \n",
              "8      RightUpLeg               0             0                0   \n",
              "7        LeftFoot               0             0                0   \n",
              "5       LeftUpLeg               0             0                0   \n",
              "4            Head               0             0                0   \n",
              "3            Neck               0             0                0   \n",
              "2          Spine1               0             0                0   \n",
              "9        RightLeg               0             0                0   \n",
              "\n",
              "    Outlier_Percent     Classification  \n",
              "14            2.872     Systemic_Noise  \n",
              "18            1.287     Systemic_Noise  \n",
              "13            1.127     Systemic_Noise  \n",
              "17            0.899  Sporadic_Glitches  \n",
              "10            0.042              Clean  \n",
              "6             0.029              Clean  \n",
              "16            0.000              Clean  \n",
              "15            0.000              Clean  \n",
              "12            0.000              Clean  \n",
              "11            0.000              Clean  \n",
              "0             0.000              Clean  \n",
              "1             0.000              Clean  \n",
              "8             0.000              Clean  \n",
              "7             0.000              Clean  \n",
              "5             0.000              Clean  \n",
              "4             0.000              Clean  \n",
              "3             0.000              Clean  \n",
              "2             0.000              Clean  \n",
              "9             0.000              Clean  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Noise Locality Index: 2.75\n",
            "\n",
            "Interpretation:\n",
            "  MEDIUM - Regional problem (e.g., one limb)\n",
            "  ‚Üí Review calibration for affected body region\n",
            "\n",
            "Classification Summary:\n",
            "  Clean: 15 joints\n",
            "  Systemic_Noise: 3 joints\n",
            "  Sporadic_Glitches: 1 joints\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"PER-JOINT NOISE PROFILE\")\n",
        "\n",
        "# Extract per-joint profiles for one representative run\n",
        "if len(runs_data) > 0:\n",
        "    # Pick the run with highest outlier rate for demonstration\n",
        "    worst_run_idx = df_engineering['Outlier_Frames_Percent'].idxmax()\n",
        "    worst_run_id = df_engineering.loc[worst_run_idx, 'Run_ID']\n",
        "    \n",
        "    print(f\"\\nAnalyzing: {worst_run_id}\")\n",
        "    print(f\"(Run with highest outlier rate: {df_engineering.loc[worst_run_idx, 'Outlier_Frames_Percent']:.3f}%)\\n\")\n",
        "    \n",
        "    # Extract per-joint profile\n",
        "    joint_profile = extract_per_joint_noise_profile(runs_data[worst_run_id])\n",
        "    \n",
        "    if not joint_profile.empty:\n",
        "        # Sort by outlier percentage\n",
        "        joint_profile_sorted = joint_profile.sort_values('Outlier_Percent', ascending=False)\n",
        "        \n",
        "        print(\"Per-Joint Outlier Profile:\")\n",
        "        print(\"=\"*80)\n",
        "        display(joint_profile_sorted)\n",
        "        \n",
        "        # Compute noise locality index\n",
        "        locality_index = compute_noise_locality_index(joint_profile)\n",
        "        \n",
        "        print(f\"\\nNoise Locality Index: {locality_index:.2f}\")\n",
        "        print(f\"\\nInterpretation:\")\n",
        "        if locality_index > 5:\n",
        "            print(f\"  HIGH - Localized tracking issue in specific joint(s)\")\n",
        "            print(f\"  ‚Üí Check marker placement and occlusion for worst joints\")\n",
        "        elif locality_index > 2:\n",
        "            print(f\"  MEDIUM - Regional problem (e.g., one limb)\")\n",
        "            print(f\"  ‚Üí Review calibration for affected body region\")\n",
        "        else:\n",
        "            print(f\"  LOW - Systemic noise across skeleton\")\n",
        "            print(f\"  ‚Üí Check capture environment (lighting, camera calibration)\")\n",
        "        \n",
        "        # Classification summary\n",
        "        print(f\"\\nClassification Summary:\")\n",
        "        for classification, count in joint_profile['Classification'].value_counts().items():\n",
        "            print(f\"  {classification}: {count} joints\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No per-joint outlier data available for this run.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No runs available for analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"outliers\"></a>\n",
        "## 10. Outlier Distribution\n",
        "\n",
        "**Purpose:** Frame-level outlier patterns and event classification.\n",
        "\n",
        "This section documents:\n",
        "- Total outlier frames and percentage\n",
        "- Maximum consecutive outlier runs\n",
        "- Tier 1/2/3 event classification (Artifact/Burst/Flow)\n",
        "- Data retention after artifact exclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "OUTLIER DISTRIBUTION ANALYSIS\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run_ID</th>\n",
              "      <th>Total_Outlier_Frames</th>\n",
              "      <th>Outlier_Frames_Percent</th>\n",
              "      <th>Max_Consecutive_Outlier_Frames</th>\n",
              "      <th>Artifact_Events_Tier1</th>\n",
              "      <th>Burst_Events_Tier2</th>\n",
              "      <th>Flow_Events_Tier3</th>\n",
              "      <th>Artifact_Frame_Rate_Percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001</td>\n",
              "      <td>879</td>\n",
              "      <td>2.872</td>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>119</td>\n",
              "      <td>67</td>\n",
              "      <td>0.2614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005</td>\n",
              "      <td>732</td>\n",
              "      <td>2.411</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>83</td>\n",
              "      <td>34</td>\n",
              "      <td>0.3228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000</td>\n",
              "      <td>871</td>\n",
              "      <td>2.750</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>64</td>\n",
              "      <td>61</td>\n",
              "      <td>0.2021</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Run_ID  Total_Outlier_Frames  \\\n",
              "0  671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001                   879   \n",
              "1  671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005                   732   \n",
              "2  671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000                   871   \n",
              "\n",
              "   Outlier_Frames_Percent  Max_Consecutive_Outlier_Frames  \\\n",
              "0                   2.872                               0   \n",
              "1                   2.411                               0   \n",
              "2                   2.750                               0   \n",
              "\n",
              "   Artifact_Events_Tier1  Burst_Events_Tier2  Flow_Events_Tier3  \\\n",
              "0                     37                 119                 67   \n",
              "1                     40                  83                 34   \n",
              "2                     30                  64                 61   \n",
              "\n",
              "   Artifact_Frame_Rate_Percent  \n",
              "0                       0.2614  \n",
              "1                       0.3228  \n",
              "2                       0.2021  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "EVENT CLASSIFICATION (3-TIER SYSTEM)\n",
            "================================================================================\n",
            "\n",
            "Tier 1 - Artifacts (1-3 consecutive frames):\n",
            "  Total events: 107\n",
            "  Mean per recording: 35.7\n",
            "  Frame rate: 0.2621%\n",
            "  Interpretation: Sporadic tracking glitches (excluded from analysis)\n",
            "\n",
            "Tier 2 - Bursts (4-7 consecutive frames):\n",
            "  Total events: 266\n",
            "  Mean per recording: 88.7\n",
            "  Interpretation: Rapid movement transitions (preserved for Gaga analysis)\n",
            "\n",
            "Tier 3 - Flows (8+ consecutive frames):\n",
            "  Total events: 162\n",
            "  Mean per recording: 54.0\n",
            "  Interpretation: Sustained high-intensity movement (legitimate dance)\n",
            "\n",
            "================================================================================\n",
            "DATA RETENTION AFTER ARTIFACT EXCLUSION\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run_ID</th>\n",
              "      <th>Clean_Max_Velocity_deg_s</th>\n",
              "      <th>Clean_Mean_Velocity_deg_s</th>\n",
              "      <th>Velocity_Reduction_Percent</th>\n",
              "      <th>Data_Retained_Percent</th>\n",
              "      <th>Excluded_Frame_Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001</td>\n",
              "      <td>799.89</td>\n",
              "      <td>44.47</td>\n",
              "      <td>45.09</td>\n",
              "      <td>99.7386</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005</td>\n",
              "      <td>799.98</td>\n",
              "      <td>48.01</td>\n",
              "      <td>39.01</td>\n",
              "      <td>99.6772</td>\n",
              "      <td>98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000</td>\n",
              "      <td>799.66</td>\n",
              "      <td>40.14</td>\n",
              "      <td>52.79</td>\n",
              "      <td>99.7979</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Run_ID  Clean_Max_Velocity_deg_s  \\\n",
              "0  671_T1_P1_R1_Take 2026-01-06 03.57.12 PM_001                    799.89   \n",
              "1  671_T2_P1_R1_Take 2026-01-15 04.35.25 PM_005                    799.98   \n",
              "2  671_T3_P1_R1_Take 2026-02-03 08.05.01 PM_000                    799.66   \n",
              "\n",
              "   Clean_Mean_Velocity_deg_s  Velocity_Reduction_Percent  \\\n",
              "0                      44.47                       45.09   \n",
              "1                      48.01                       39.01   \n",
              "2                      40.14                       52.79   \n",
              "\n",
              "   Data_Retained_Percent  Excluded_Frame_Count  \n",
              "0                99.7386                    80  \n",
              "1                99.6772                    98  \n",
              "2                99.7979                    64  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Retention Summary:\n",
            "  Mean data retained: 99.7379%\n",
            "  Total frames excluded: 242\n",
            "  Mean velocity reduction: 45.63%\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"OUTLIER DISTRIBUTION ANALYSIS\")\n",
        "\n",
        "cols_outliers = [\n",
        "    'Run_ID',\n",
        "    'Total_Outlier_Frames',\n",
        "    'Outlier_Frames_Percent',\n",
        "    'Max_Consecutive_Outlier_Frames',\n",
        "    'Artifact_Events_Tier1',\n",
        "    'Burst_Events_Tier2',\n",
        "    'Flow_Events_Tier3',\n",
        "    'Artifact_Frame_Rate_Percent'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_outliers])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVENT CLASSIFICATION (3-TIER SYSTEM)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nTier 1 - Artifacts (1-3 consecutive frames):\")\n",
        "print(f\"  Total events: {df_engineering['Artifact_Events_Tier1'].sum()}\")\n",
        "print(f\"  Mean per recording: {df_engineering['Artifact_Events_Tier1'].mean():.1f}\")\n",
        "print(f\"  Frame rate: {df_engineering['Artifact_Frame_Rate_Percent'].mean():.4f}%\")\n",
        "print(f\"  Interpretation: Sporadic tracking glitches (excluded from analysis)\")\n",
        "\n",
        "print(f\"\\nTier 2 - Bursts (4-7 consecutive frames):\")\n",
        "print(f\"  Total events: {df_engineering['Burst_Events_Tier2'].sum()}\")\n",
        "print(f\"  Mean per recording: {df_engineering['Burst_Events_Tier2'].mean():.1f}\")\n",
        "print(f\"  Interpretation: Rapid movement transitions (preserved for Gaga analysis)\")\n",
        "\n",
        "print(f\"\\nTier 3 - Flows (8+ consecutive frames):\")\n",
        "print(f\"  Total events: {df_engineering['Flow_Events_Tier3'].sum()}\")\n",
        "print(f\"  Mean per recording: {df_engineering['Flow_Events_Tier3'].mean():.1f}\")\n",
        "print(f\"  Interpretation: Sustained high-intensity movement (legitimate dance)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA RETENTION AFTER ARTIFACT EXCLUSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "cols_retention = [\n",
        "    'Run_ID',\n",
        "    'Clean_Max_Velocity_deg_s',\n",
        "    'Clean_Mean_Velocity_deg_s',\n",
        "    'Velocity_Reduction_Percent',\n",
        "    'Data_Retained_Percent',\n",
        "    'Excluded_Frame_Count'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_retention])\n",
        "\n",
        "print(f\"\\nRetention Summary:\")\n",
        "print(f\"  Mean data retained: {df_engineering['Data_Retained_Percent'].mean():.4f}%\")\n",
        "print(f\"  Total frames excluded: {df_engineering['Excluded_Frame_Count'].sum()}\")\n",
        "print(f\"  Mean velocity reduction: {df_engineering['Velocity_Reduction_Percent'].mean():.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"export\"></a>\n",
        "## 11. Excel Export\n",
        "\n",
        "**Output:** `reports/Engineering_Audit_YYYYMMDD_HHMMSS.xlsx`\n",
        "\n",
        "**Sheets:**\n",
        "1. Engineering_Profile - All physical measurements (no scores)\n",
        "2. Methodology_Passport - Mathematical documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXPORT TO EXCEL\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Engineering Audit Created:\n",
            "   c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\reports\\Engineering_Audit_20260215_130905.xlsx\n",
            "\n",
            "   Sheets:\n",
            "   1. Engineering_Profile - 3 recordings √ó 94 measurements\n",
            "   2. Methodology_Passport - Mathematical documentation\n",
            "\n",
            "================================================================================\n",
            "NOTEBOOK COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Recordings Processed: 3\n",
            "Excel Output: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\reports\\Engineering_Audit_20260215_130905.xlsx\n",
            "\n",
            "This report contains ZERO synthetic scores and ZERO decision labels.\n",
            "All values are pure physical measurements for researcher interpretation.\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"EXPORT TO EXCEL\")\n",
        "\n",
        "# Create output path\n",
        "REPORTS_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "excel_path = os.path.join(REPORTS_DIR, f\"Engineering_Audit_{timestamp}.xlsx\")\n",
        "\n",
        "# Export to Excel\n",
        "with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
        "    workbook = writer.book\n",
        "    \n",
        "    # Formats\n",
        "    title_fmt = workbook.add_format({\n",
        "        'bold': True, 'font_size': 16, \n",
        "        'bg_color': '#2E75B6', 'font_color': 'white'\n",
        "    })\n",
        "    header_fmt = workbook.add_format({\n",
        "        'bold': True, 'bg_color': '#4472C4', \n",
        "        'font_color': 'white', 'text_wrap': True\n",
        "    })\n",
        "    \n",
        "    # ============================================================\n",
        "    # SHEET 1: ENGINEERING PROFILE\n",
        "    # ============================================================\n",
        "    df_engineering.to_excel(writer, index=False, sheet_name='Engineering_Profile')\n",
        "    \n",
        "    ws_profile = writer.sheets['Engineering_Profile']\n",
        "    for col_num, value in enumerate(df_engineering.columns):\n",
        "        ws_profile.write(0, col_num, value, header_fmt)\n",
        "    \n",
        "    # Auto-fit columns\n",
        "    for i, col in enumerate(df_engineering.columns):\n",
        "        max_len = max(df_engineering[col].astype(str).str.len().max(), len(str(col)))\n",
        "        ws_profile.set_column(i, i, min(max_len + 2, 50))\n",
        "    \n",
        "    # ============================================================\n",
        "    # SHEET 2: METHODOLOGY PASSPORT\n",
        "    # ============================================================\n",
        "    methodology_sheet = workbook.add_worksheet('Methodology_Passport')\n",
        "    \n",
        "    methodology_sheet.merge_range('A1:E1', 'METHODOLOGY PASSPORT - MATHEMATICAL DOCUMENTATION', title_fmt)\n",
        "    methodology_sheet.write('A2', f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    methodology_sheet.write('A3', f\"Pipeline Version: v3.0_3stage_signal_cleaning\")\n",
        "    \n",
        "    row = 5\n",
        "    \n",
        "    # Write methodology as structured text\n",
        "    methodology_sheet.write(row, 0, 'INTERPOLATION METHODS', header_fmt)\n",
        "    row += 1\n",
        "    \n",
        "    for key, method in METHODOLOGY_PASSPORT[\"interpolation\"].items():\n",
        "        methodology_sheet.write(row, 0, key.title())\n",
        "        row += 1\n",
        "        for field, value in method.items():\n",
        "            methodology_sheet.write(row, 1, field)\n",
        "            methodology_sheet.write(row, 2, str(value))\n",
        "            row += 1\n",
        "        row += 1\n",
        "    \n",
        "    methodology_sheet.set_column('A:A', 20)\n",
        "    methodology_sheet.set_column('B:B', 25)\n",
        "    methodology_sheet.set_column('C:E', 60)\n",
        "\n",
        "print(f\"\\n‚úÖ Engineering Audit Created:\")\n",
        "print(f\"   {excel_path}\")\n",
        "print(f\"\\n   Sheets:\")\n",
        "print(f\"   1. Engineering_Profile - {len(df_engineering)} recordings √ó {len(df_engineering.columns)} measurements\")\n",
        "print(f\"   2. Methodology_Passport - Mathematical documentation\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"NOTEBOOK COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nRecordings Processed: {len(df_engineering)}\")\n",
        "print(f\"Excel Output: {excel_path}\")\n",
        "print(f\"\\nThis report contains ZERO synthetic scores and ZERO decision labels.\")\n",
        "print(f\"All values are pure physical measurements for researcher interpretation.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
