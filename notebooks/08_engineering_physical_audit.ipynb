{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 08 - Engineering & Physical Audit\n",
        "\n",
        "**Philosophy:** Raw Data Only - Zero Scoring, Zero Decisions\n",
        "\n",
        "**Purpose:** High-fidelity engineering documentation for each motion capture session. This notebook provides pure physical measurements, mathematical methodology, and biomechanical profiles WITHOUT synthetic quality scores or decision labels.\n",
        "\n",
        "**Target Audience:** Researchers, biomechanists, and data scientists who need transparent access to:\n",
        "- Raw capture quality metrics (SNR, missing data, jitter)\n",
        "- Processing methodology (interpolation, filtering, differentiation formulas)\n",
        "- Structural integrity (skeleton stability, calibration offsets)\n",
        "- Kinematic extremes (peak velocities, accelerations)\n",
        "- Per-joint noise profiles\n",
        "\n",
        "**What This Report Does NOT Include:**\n",
        "- Quality scores (0-100)\n",
        "- Decision labels (ACCEPT/REVIEW/REJECT)\n",
        "- Synthetic grades (GOLD/SILVER/BRONZE)\n",
        "- Pass/Fail judgments\n",
        "\n",
        "**References:**\n",
        "- Cereatti et al. (2024) - Data lineage & provenance\n",
        "- Winter (2009) - Residual analysis\n",
        "- R√°cz et al. (2025) - Calibration layer\n",
        "- Shoemake (1985) - Quaternion interpolation\n",
        "- Savitzky & Golay (1964) - Smoothing differentiation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Setup & Data Loading](#setup)\n",
        "2. [Methodology Passport](#methodology) - Mathematical documentation\n",
        "3. [Data Lineage](#lineage) - Recording provenance\n",
        "4. [Capture Baseline](#baseline) - Raw state before processing\n",
        "5. [Structural Integrity](#structure) - Skeleton & calibration\n",
        "6. [Signal Quality Profile](#signal) - Pre-processing SNR\n",
        "7. [Processing Transparency](#processing) - What was done\n",
        "8. [Kinematic Extremes](#kinematics) - Processed output\n",
        "9. [Per-Joint Noise Profile](#noise) - Root cause analysis\n",
        "10. [Outlier Distribution](#outliers) - Frame-level patterns\n",
        "11. [Excel Export](#export) - Engineering audit log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"setup\"></a>\n",
        "## 1. Setup & Data Loading\n",
        "\n",
        "Load all JSON files **once** and reuse throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project Root: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\n",
            "Git Hash: d5c8380\n",
            "Timestamp: 2026-01-29 15:01:20\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# IMPORTS & PATH SETUP\n",
        "# ============================================================\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from IPython.display import display, HTML, Markdown\n",
        "\n",
        "# Setup paths\n",
        "if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "else:\n",
        "    PROJECT_ROOT = os.path.abspath(os.getcwd())\n",
        "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
        "if SRC_PATH not in sys.path:\n",
        "    sys.path.insert(0, SRC_PATH)\n",
        "\n",
        "# Import utility module\n",
        "from utils_nb07 import (\n",
        "    load_all_runs, \n",
        "    filter_complete_runs, \n",
        "    build_engineering_profile_row,\n",
        "    extract_per_joint_noise_profile,\n",
        "    extract_bone_stability_profile,\n",
        "    extract_selected_segments,\n",
        "    compute_noise_locality_index,\n",
        "    get_git_hash,\n",
        "    print_section_header,\n",
        "    METHODOLOGY_PASSPORT\n",
        ")\n",
        "\n",
        "print(f\"Project Root: {PROJECT_ROOT}\")\n",
        "print(f\"Git Hash: {get_git_hash(PROJECT_ROOT)}\")\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading JSON files...\n",
            "Found 1 total runs\n",
            "Complete runs: 1\n",
            "\n",
            "Steps available per run:\n",
            "\n",
            "  734_T3_P2_R1_Take 2025-12-30 04.12.54 PM_002\n",
            "    ‚úÖ Found: ['step_01', 'step_02', 'step_03', 'step_04', 'step_05', 'step_06']\n",
            "    ‚Üí This run is COMPLETE and will be processed\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD ALL DATA (ONCE)\n",
        "# ============================================================\n",
        "DERIV_ROOT = os.path.join(PROJECT_ROOT, \"derivatives\")\n",
        "\n",
        "# Load all JSON files\n",
        "print(\"Loading JSON files...\")\n",
        "all_runs = load_all_runs(DERIV_ROOT)\n",
        "print(f\"Found {len(all_runs)} total runs\")\n",
        "\n",
        "# Filter to complete runs (require step_01 and step_06)\n",
        "runs_data = filter_complete_runs(all_runs, required_steps=[\"step_01\", \"step_06\"])\n",
        "print(f\"Complete runs: {len(runs_data)}\")\n",
        "\n",
        "# Show available steps per run\n",
        "print(\"\\nSteps available per run:\")\n",
        "expected_steps = ['step_01', 'step_02', 'step_03', 'step_04', 'step_05', 'step_06']\n",
        "\n",
        "if len(all_runs) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: No runs found at all!\")\n",
        "    print(f\"\\nSearched in: {DERIV_ROOT}\")\n",
        "    print(\"\\nExpected structure:\")\n",
        "    print(\"  derivatives/\")\n",
        "    print(\"    ‚îú‚îÄ‚îÄ step_01_parse/\")\n",
        "    print(\"    ‚îÇ   ‚îî‚îÄ‚îÄ {run_id}__step01_loader_report.json\")\n",
        "    print(\"    ‚îú‚îÄ‚îÄ step_02_preprocess/\")\n",
        "    print(\"    ‚îÇ   ‚îî‚îÄ‚îÄ {run_id}__preprocess_summary.json\")\n",
        "    print(\"    ‚îú‚îÄ‚îÄ step_03_resample/\")\n",
        "    print(\"    ‚îú‚îÄ‚îÄ step_04_filtering/\")\n",
        "    print(\"    ‚îú‚îÄ‚îÄ step_05_reference/\")\n",
        "    print(\"    ‚îî‚îÄ‚îÄ step_06_kinematics/\")\n",
        "    print(\"        ‚îî‚îÄ‚îÄ ultimate/\")\n",
        "    print(\"            ‚îî‚îÄ‚îÄ {run_id}__outlier_validation.json\")\n",
        "    print(\"\\nPlease run the full pipeline (notebooks 01-06) first.\")\n",
        "else:\n",
        "    for run_id, steps in all_runs.items():\n",
        "        steps_list = sorted(steps.keys())\n",
        "        missing = [s for s in expected_steps if s not in steps_list]\n",
        "        \n",
        "        # Truncate run_id for display\n",
        "        display_id = run_id[:60] + \"...\" if len(run_id) > 60 else run_id\n",
        "        print(f\"\\n  {display_id}\")\n",
        "        print(f\"    ‚úÖ Found: {steps_list}\")\n",
        "        \n",
        "        if missing:\n",
        "            print(f\"    ‚ö†Ô∏è Missing: {missing}\")\n",
        "            if run_id not in runs_data:\n",
        "                print(f\"    ‚Üí This run is INCOMPLETE and will be skipped\")\n",
        "        else:\n",
        "            print(f\"    ‚Üí This run is COMPLETE and will be processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Building engineering profiles for 1 complete runs...\n",
            "\n",
            "‚úÖ Engineering DataFrame: 1 runs √ó 68 measurements\n",
            "\n",
            "Column groups:\n",
            "  - Lineage: Run_ID, Subject_ID, Session_ID, Pipeline_Version\n",
            "  - Baseline: Duration, Sampling_Rate, Raw_Missing_%, SNR\n",
            "  - Structure: Bone_CV%, Skeleton_Segments, Height, Mass\n",
            "  - Processing: Interpolation, Filtering, Resampling\n",
            "  - Kinematics: Max_Velocity, Max_Acceleration, Path_Length\n",
            "  - Outliers: Total_Frames, Classification, Data_Retained\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BUILD ENGINEERING PROFILE DATAFRAME\n",
        "# ============================================================\n",
        "\n",
        "# Check if we have any complete runs\n",
        "if len(runs_data) == 0:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚ö†Ô∏è ERROR: NO COMPLETE RUNS FOUND\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nDiagnostics:\")\n",
        "    print(f\"  Total runs discovered: {len(all_runs)}\")\n",
        "    print(f\"  Runs with step_01 AND step_06: 0\")\n",
        "    print(\"\\nShowing what's available for each run:\\n\")\n",
        "    \n",
        "    for run_id, steps in all_runs.items():\n",
        "        print(f\"  Run: {run_id[:70]}\")\n",
        "        print(f\"    Steps found: {sorted(steps.keys())}\")\n",
        "        missing = [s for s in ['step_01', 'step_02', 'step_03', 'step_04', 'step_05', 'step_06'] if s not in steps]\n",
        "        if missing:\n",
        "            print(f\"    ‚ö†Ô∏è Missing: {missing}\")\n",
        "        print()\n",
        "    \n",
        "    print(\"\\nTo fix this issue:\")\n",
        "    print(\"  1. Ensure all pipeline steps (01-06) have been run for your sessions\")\n",
        "    print(\"  2. Check that JSON files exist in derivatives/step_01_parse/ and derivatives/step_06_kinematics/ultimate/\")\n",
        "    print(\"  3. Verify the run_id prefixes match across all steps\")\n",
        "    print(\"\\nCannot proceed without complete runs. Stopping execution.\")\n",
        "    raise ValueError(\"No complete runs found. Please run the full pipeline (steps 01-06) first.\")\n",
        "\n",
        "# Extract pure physical measurements (NO SCORING)\n",
        "print(f\"\\nBuilding engineering profiles for {len(runs_data)} complete runs...\")\n",
        "engineering_rows = [build_engineering_profile_row(run_id, steps) for run_id, steps in runs_data.items()]\n",
        "df_engineering = pd.DataFrame(engineering_rows)\n",
        "\n",
        "# Sort by subject and session for easy review\n",
        "if len(df_engineering) > 0:\n",
        "    df_engineering = df_engineering.sort_values([\"Subject_ID\", \"Session_ID\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n‚úÖ Engineering DataFrame: {len(df_engineering)} runs √ó {len(df_engineering.columns)} measurements\")\n",
        "print(f\"\\nColumn groups:\")\n",
        "print(f\"  - Lineage: Run_ID, Subject_ID, Session_ID, Pipeline_Version\")\n",
        "print(f\"  - Baseline: Duration, Sampling_Rate, Raw_Missing_%, SNR\")\n",
        "print(f\"  - Structure: Bone_CV%, Skeleton_Segments, Height, Mass\")\n",
        "print(f\"  - Processing: Interpolation, Filtering, Resampling\")\n",
        "print(f\"  - Kinematics: Max_Velocity, Max_Acceleration, Path_Length\")\n",
        "print(f\"  - Outliers: Total_Frames, Classification, Data_Retained\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"methodology\"></a>\n",
        "## 2. Methodology Passport\n",
        "\n",
        "**Purpose:** Document the mathematical methods used to derive all reported values.\n",
        "\n",
        "This section provides explicit formulas, implementation details, and references for:\n",
        "- Quaternion interpolation (SLERP)\n",
        "- Angular velocity extraction\n",
        "- Angular acceleration differentiation\n",
        "- 3-stage signal cleaning pipeline\n",
        "- Resampling strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "METHODOLOGY PASSPORT - MATHEMATICAL DOCUMENTATION\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "INTERPOLATION METHODS\n",
            "================================================================================\n",
            "\n",
            "üìê Rotation Interpolation: SLERP (Spherical Linear Interpolation)\n",
            "   Formula: q(t) = sin((1-t)Œ∏)/sin(Œ∏) ¬∑ q‚ÇÄ + sin(tŒ∏)/sin(Œ∏) ¬∑ q‚ÇÅ\n",
            "   Constraint: Maintains unit quaternion: ||q|| = 1\n",
            "   Geodesic: Shortest path on SO(3) manifold\n",
            "   Reference: Shoemake (1985)\n",
            "   Implementation: scipy.spatial.transform.Slerp\n",
            "\n",
            "üìê Position Interpolation: CubicSpline\n",
            "   Formula: p(t) = a‚ÇÄ + a‚ÇÅt + a‚ÇÇt¬≤ + a‚ÇÉt¬≥ (piecewise)\n",
            "   Continuity: C¬≤ (smooth velocity and acceleration)\n",
            "   Constraint: Natural boundary conditions\n",
            "   Implementation: scipy.interpolate.CubicSpline\n",
            "\n",
            "================================================================================\n",
            "DIFFERENTIATION METHODS\n",
            "================================================================================\n",
            "\n",
            "üîÑ Angular Velocity: Quaternion Derivative\n",
            "   Formula: œâ = 2 ¬∑ (dq/dt) ¬∑ q*\n",
            "   Derivation: qÃá via finite differences, then œâ = 2qÃáq* (quaternion conjugate)\n",
            "   Units: deg/s\n",
            "   Note: Extracts instantaneous axis-angle velocity from quaternion time series\n",
            "\n",
            "üîÑ Angular Acceleration: Savitzky-Golay Filter\n",
            "   Formula: Œ± = d/dt(œâ) via least-squares polynomial fitting\n",
            "   Window: 0.175s (21 frames @ 120Hz)\n",
            "   Polynomial Order: 3\n",
            "   Units: deg/s¬≤\n",
            "   Reference: Savitzky & Golay (1964)\n",
            "   Implementation: scipy.signal.savgol_filter with mode='interp'\n",
            "\n",
            "üìè Linear Velocity: CubicSpline Analytical Derivative\n",
            "   Formula: v = dp/dt (analytical derivative of cubic spline)\n",
            "   Note: Exact derivative, not finite-difference approximation\n",
            "\n",
            "üìè Linear Acceleration: CubicSpline Second Derivative\n",
            "   Formula: a = d¬≤p/dt¬≤ (analytical second derivative)\n",
            "   Note: Exact second derivative from spline coefficients\n",
            "\n",
            "================================================================================\n",
            "3-STAGE SIGNAL CLEANING PIPELINE (v3.0)\n",
            "================================================================================\n",
            "\n",
            "Philosophy: Artifact removal with movement preservation\n",
            "Version: v3.0_3stage_signal_cleaning\n",
            "\n",
            "üîç Stage 1: Z-Score + Velocity Threshold\n",
            "   Velocity Limit: 5000.0 mm/s\n",
            "   Z-Score Threshold: 5.0œÉ\n",
            "   Interpolation: PCHIP (Piecewise Cubic Hermite Interpolating Polynomial)\n",
            "   Purpose: Remove physically impossible spikes (tracking glitches)\n",
            "\n",
            "üîç Stage 2: Sliding Window Median Filter\n",
            "   Window Size: 5 frames\n",
            "   Sigma Threshold: 3.0œÉ\n",
            "   Purpose: Remove isolated outliers while preserving edges\n",
            "   Note: Median-based, robust to non-Gaussian noise\n",
            "\n",
            "üîç Stage 3: Winter's Residual Analysis (1990)\n",
            "   Strategy: Per-Region Adaptive Cutoff Selection\n",
            "   Frequency Range: 1.0-20.0 Hz\n",
            "   Filter Type: Butterworth Low-Pass\n",
            "   Filter Order: 2\n",
            "   Implementation: Zero-lag (filtfilt) - forward-backward pass\n",
            "   Rationale: Different body regions have different frequency content (distal faster than proximal)\n",
            "\n",
            "   Body Regions:\n",
            "     - head: Head markers\n",
            "     - upper_proximal: Shoulders, upper arms\n",
            "     - trunk: Spine segments\n",
            "     - lower_distal: Feet, ankles\n",
            "     - lower_proximal: Thighs, hips\n",
            "     - upper_distal: Hands, forearms\n",
            "\n",
            "================================================================================\n",
            "RESAMPLING STRATEGY\n",
            "================================================================================\n",
            "\n",
            "Target Frequency: 120.0 Hz\n",
            "Purpose: Uniform temporal grid for frequency-domain analysis\n",
            "Method: Interpolate to exact 1/120s intervals (8.333ms)\n",
            "Positions: CubicSpline\n",
            "Rotations: SLERP\n",
            "Validation: Time grid standard deviation < 0.001ms\n",
            "\n",
            "================================================================================\n",
            "REFERENCE ALIGNMENT\n",
            "================================================================================\n",
            "\n",
            "Method: ISB/CAST Static Pose Detection\n",
            "Reference: R√°cz et al. (2025) - Anatomical Calibration Layer\n",
            "Detection: Sliding window variance minimization (1-2 second stable period)\n",
            "Offset Correction: V-pose to T-pose quaternion transformation\n",
            "Bilateral Correction: Left/Right arm abduction offset removal\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"METHODOLOGY PASSPORT - MATHEMATICAL DOCUMENTATION\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INTERPOLATION METHODS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Rotation Interpolation\n",
        "rot_method = METHODOLOGY_PASSPORT[\"interpolation\"][\"rotations\"]\n",
        "print(f\"\\nüìê Rotation Interpolation: {rot_method['method']}\")\n",
        "print(f\"   Formula: {rot_method['formula']}\")\n",
        "print(f\"   Constraint: {rot_method['constraint']}\")\n",
        "print(f\"   Geodesic: {rot_method['geodesic']}\")\n",
        "print(f\"   Reference: {rot_method['reference']}\")\n",
        "print(f\"   Implementation: {rot_method['implementation']}\")\n",
        "\n",
        "# Position Interpolation\n",
        "pos_method = METHODOLOGY_PASSPORT[\"interpolation\"][\"positions\"]\n",
        "print(f\"\\nüìê Position Interpolation: {pos_method['method']}\")\n",
        "print(f\"   Formula: {pos_method['formula']}\")\n",
        "print(f\"   Continuity: {pos_method['continuity']}\")\n",
        "print(f\"   Constraint: {pos_method['constraint']}\")\n",
        "print(f\"   Implementation: {pos_method['implementation']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DIFFERENTIATION METHODS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Angular Velocity\n",
        "ang_vel = METHODOLOGY_PASSPORT[\"differentiation\"][\"angular_velocity\"]\n",
        "print(f\"\\nüîÑ Angular Velocity: {ang_vel['method']}\")\n",
        "print(f\"   Formula: {ang_vel['formula']}\")\n",
        "print(f\"   Derivation: {ang_vel['derivation']}\")\n",
        "print(f\"   Units: {ang_vel['units']}\")\n",
        "print(f\"   Note: {ang_vel['note']}\")\n",
        "\n",
        "# Angular Acceleration\n",
        "ang_accel = METHODOLOGY_PASSPORT[\"differentiation\"][\"angular_acceleration\"]\n",
        "print(f\"\\nüîÑ Angular Acceleration: {ang_accel['method']}\")\n",
        "print(f\"   Formula: {ang_accel['formula']}\")\n",
        "print(f\"   Window: {ang_accel['window_sec']}s ({ang_accel['window_frames']} frames @ 120Hz)\")\n",
        "print(f\"   Polynomial Order: {ang_accel['polynomial_order']}\")\n",
        "print(f\"   Units: {ang_accel['units']}\")\n",
        "print(f\"   Reference: {ang_accel['reference']}\")\n",
        "print(f\"   Implementation: {ang_accel['implementation']}\")\n",
        "\n",
        "# Linear Derivatives\n",
        "lin_vel = METHODOLOGY_PASSPORT[\"differentiation\"][\"linear_velocity\"]\n",
        "lin_accel = METHODOLOGY_PASSPORT[\"differentiation\"][\"linear_acceleration\"]\n",
        "print(f\"\\nüìè Linear Velocity: {lin_vel['method']}\")\n",
        "print(f\"   Formula: {lin_vel['formula']}\")\n",
        "print(f\"   Note: {lin_vel['note']}\")\n",
        "print(f\"\\nüìè Linear Acceleration: {lin_accel['method']}\")\n",
        "print(f\"   Formula: {lin_accel['formula']}\")\n",
        "print(f\"   Note: {lin_accel['note']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3-STAGE SIGNAL CLEANING PIPELINE (v3.0)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "filtering = METHODOLOGY_PASSPORT[\"filtering\"]\n",
        "print(f\"\\nPhilosophy: {filtering['philosophy']}\")\n",
        "print(f\"Version: {filtering['pipeline_version']}\")\n",
        "\n",
        "# Stage 1\n",
        "stage1 = filtering[\"stage1_artifact_detection\"]\n",
        "print(f\"\\nüîç Stage 1: {stage1['method']}\")\n",
        "print(f\"   Velocity Limit: {stage1['velocity_limit_mm_s']} mm/s\")\n",
        "print(f\"   Z-Score Threshold: {stage1['zscore_threshold']}œÉ\")\n",
        "print(f\"   Interpolation: {stage1['interpolation']}\")\n",
        "print(f\"   Purpose: {stage1['purpose']}\")\n",
        "\n",
        "# Stage 2\n",
        "stage2 = filtering[\"stage2_hampel\"]\n",
        "print(f\"\\nüîç Stage 2: {stage2['method']}\")\n",
        "print(f\"   Window Size: {stage2['window_size']} frames\")\n",
        "print(f\"   Sigma Threshold: {stage2['n_sigma']}œÉ\")\n",
        "print(f\"   Purpose: {stage2['purpose']}\")\n",
        "print(f\"   Note: {stage2['note']}\")\n",
        "\n",
        "# Stage 3\n",
        "stage3 = filtering[\"stage3_adaptive_winter\"]\n",
        "print(f\"\\nüîç Stage 3: {stage3['method']}\")\n",
        "print(f\"   Strategy: {stage3['strategy']}\")\n",
        "print(f\"   Frequency Range: {stage3['fmin_hz']}-{stage3['fmax_hz']} Hz\")\n",
        "print(f\"   Filter Type: {stage3['filter_type']}\")\n",
        "print(f\"   Filter Order: {stage3['filter_order']}\")\n",
        "print(f\"   Implementation: {stage3['implementation']}\")\n",
        "print(f\"   Rationale: {stage3['rationale']}\")\n",
        "print(f\"\\n   Body Regions:\")\n",
        "for region, description in stage3[\"regions\"].items():\n",
        "    print(f\"     - {region}: {description}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESAMPLING STRATEGY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "resampling = METHODOLOGY_PASSPORT[\"resampling\"]\n",
        "print(f\"\\nTarget Frequency: {resampling['target_fs_hz']} Hz\")\n",
        "print(f\"Purpose: {resampling['purpose']}\")\n",
        "print(f\"Method: {resampling['method']}\")\n",
        "print(f\"Positions: {resampling['positions_method']}\")\n",
        "print(f\"Rotations: {resampling['rotations_method']}\")\n",
        "print(f\"Validation: {resampling['validation']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"REFERENCE ALIGNMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ref_align = METHODOLOGY_PASSPORT[\"reference_alignment\"]\n",
        "print(f\"\\nMethod: {ref_align['method']}\")\n",
        "print(f\"Reference: {ref_align['reference']}\")\n",
        "print(f\"Detection: {ref_align['detection']}\")\n",
        "print(f\"Offset Correction: {ref_align['offset_correction']}\")\n",
        "print(f\"Bilateral Correction: {ref_align['bilateral_correction']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"lineage\"></a>\n",
        "## 3. Data Lineage & Provenance\n",
        "\n",
        "**Purpose:** Ensure recording traceability from raw file to final result (Cereatti et al., 2024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "DATA LINEAGE & PROVENANCE\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run_ID</th>\n",
              "      <th>Subject_ID</th>\n",
              "      <th>Session_ID</th>\n",
              "      <th>Processing_Timestamp</th>\n",
              "      <th>Pipeline_Version</th>\n",
              "      <th>CSV_Source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>734_T3_P2_R1_Take 2025-12-30 04.12.54 PM_002</td>\n",
              "      <td>734</td>\n",
              "      <td>T3</td>\n",
              "      <td>2026-01-29 11:38</td>\n",
              "      <td>v2.6_calibration_enhanced</td>\n",
              "      <td>c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gag...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Run_ID Subject_ID Session_ID  \\\n",
              "0  734_T3_P2_R1_Take 2025-12-30 04.12.54 PM_002        734         T3   \n",
              "\n",
              "  Processing_Timestamp           Pipeline_Version  \\\n",
              "0     2026-01-29 11:38  v2.6_calibration_enhanced   \n",
              "\n",
              "                                          CSV_Source  \n",
              "0  c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gag...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset Summary:\n",
            "  Total Recordings: 1\n",
            "  Unique Subjects: 1\n",
            "  Unique Sessions: 1\n",
            "  Pipeline Version: v2.6_calibration_enhanced\n"
          ]
        }
      ],
      "source": [
        "print_section_header(\"DATA LINEAGE & PROVENANCE\")\n",
        "\n",
        "# Display provenance info\n",
        "cols_lineage = ['Run_ID', 'Subject_ID', 'Session_ID', 'Processing_Timestamp', 'Pipeline_Version', 'CSV_Source']\n",
        "display(df_engineering[cols_lineage])\n",
        "\n",
        "print(f\"\\nDataset Summary:\")\n",
        "print(f\"  Total Recordings: {len(df_engineering)}\")\n",
        "print(f\"  Unique Subjects: {df_engineering['Subject_ID'].nunique()}\")\n",
        "print(f\"  Unique Sessions: {df_engineering['Session_ID'].nunique()}\")\n",
        "print(f\"  Pipeline Version: {df_engineering['Pipeline_Version'].iloc[0] if len(df_engineering) > 0 else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"baseline\"></a>\n",
        "## 4. Capture Baseline Profile\n",
        "\n",
        "**Purpose:** Document the raw state of the data BEFORE any processing.\n",
        "\n",
        "This section represents the \"Ground Truth\" capture quality:\n",
        "- How much data was missing in the raw OptiTrack export?\n",
        "- What was the inherent Signal-to-Noise Ratio (SNR) of the raw tracking?\n",
        "- What was the native sampling rate and jitter?\n",
        "- What was the OptiTrack system calibration error?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 11.5: Anatomical Region View (Human-Readable)\n",
        "\n",
        "Group path lengths by anatomical regions for easier interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è Anatomical region columns not found. Re-run notebook 06 with Phase 2 updates.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ANATOMICAL REGION BREAKDOWN\n",
        "# ============================================================\n",
        "\n",
        "# Extract anatomical region columns\n",
        "anatomical_cols = [\n",
        "    \"Path_Neck_m\", \"Path_Shoulders_m\", \"Path_Elbows_m\", \"Path_Wrists_m\",\n",
        "    \"Path_Spine_m\", \"Path_Hips_m\", \"Path_Knees_m\", \"Path_Ankles_m\"\n",
        "]\n",
        "\n",
        "if all(col in df_engineering.columns for col in anatomical_cols):\n",
        "    # Create anatomical summary\n",
        "    region_data = []\n",
        "    \n",
        "    for _, row in df_engineering.iterrows():\n",
        "        regions = {\n",
        "            \"Run_ID\": row[\"Run_ID\"],\n",
        "            \"Neck\": row[\"Path_Neck_m\"],\n",
        "            \"Shoulders\": row[\"Path_Shoulders_m\"],\n",
        "            \"Elbows\": row[\"Path_Elbows_m\"],\n",
        "            \"Wrists\": row[\"Path_Wrists_m\"],\n",
        "            \"Spine\": row[\"Path_Spine_m\"],\n",
        "            \"Hips\": row[\"Path_Hips_m\"],\n",
        "            \"Knees\": row[\"Path_Knees_m\"],\n",
        "            \"Ankles\": row[\"Path_Ankles_m\"],\n",
        "        }\n",
        "        region_data.append(regions)\n",
        "    \n",
        "    df_anatomical = pd.DataFrame(region_data)\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"ANATOMICAL REGION PATH LENGTHS (meters)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nHuman-readable view of movement by body region:\")\n",
        "    print(\"\\nMapping:\")\n",
        "    print(\"  ‚Ä¢ Neck       ‚Üí Neck joint\")\n",
        "    print(\"  ‚Ä¢ Shoulders  ‚Üí Left/Right shoulder joints (max)\")\n",
        "    print(\"  ‚Ä¢ Elbows     ‚Üí Left/Right forearm segments (max)\")\n",
        "    print(\"  ‚Ä¢ Wrists     ‚Üí Left/Right hand/wrist joints (max)\")\n",
        "    print(\"  ‚Ä¢ Spine      ‚Üí Mid-back / thoracic region\")\n",
        "    print(\"  ‚Ä¢ Hips       ‚Üí Pelvis + hip joints\")\n",
        "    print(\"  ‚Ä¢ Knees      ‚Üí Left/Right shin segments (max)\")\n",
        "    print(\"  ‚Ä¢ Ankles     ‚Üí Left/Right foot/ankle joints (max)\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    \n",
        "    # Display table\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):\n",
        "        print(df_anatomical.to_string(index=False))\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"REGION SUMMARY (across all runs)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    region_summary = df_anatomical.drop(columns=[\"Run_ID\"]).describe().loc[[\"mean\", \"min\", \"max\"]]\n",
        "    print(region_summary.to_string())\n",
        "    \n",
        "    # Most active regions\n",
        "    mean_by_region = df_anatomical.drop(columns=[\"Run_ID\"]).mean().sort_values(ascending=False)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MOST ACTIVE REGIONS (ranked by average path length)\")\n",
        "    print(\"=\"*80)\n",
        "    for i, (region, value) in enumerate(mean_by_region.items(), 1):\n",
        "        print(f\"  {i}. {region:12s}: {value:.2f}m\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Anatomical region columns not found. Re-run notebook 06 with Phase 2 updates.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 11.6: Intensity Index (Phase 3)\n",
        "\n",
        "**Movement intensity normalized by duration** - allows fair comparison between sessions of different lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è Intensity Index columns not found. Re-run notebook 06 with Phase 3 updates.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# INTENSITY INDEX (Phase 3)\n",
        "# ============================================================\n",
        "\n",
        "intensity_cols = [\n",
        "    \"Intensity_Neck_m_per_s\", \"Intensity_Shoulders_m_per_s\", \"Intensity_Elbows_m_per_s\", \"Intensity_Wrists_m_per_s\",\n",
        "    \"Intensity_Spine_m_per_s\", \"Intensity_Hips_m_per_s\", \"Intensity_Knees_m_per_s\", \"Intensity_Ankles_m_per_s\"\n",
        "]\n",
        "\n",
        "if all(col in df_engineering.columns for col in intensity_cols):\n",
        "    print(\"=\"*80)\n",
        "    print(\"INTENSITY INDEX (meters per second)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nIntensity = Path Length / Duration\")\n",
        "    print(\"  ‚Üí Normalized measure of movement activity\")\n",
        "    print(\"  ‚Üí Allows comparison between sessions of different durations\")\n",
        "    print(\"  ‚Üí Units: m/s (average velocity-like measure)\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    \n",
        "    # Display intensity by anatomical region\n",
        "    intensity_data = []\n",
        "    for _, row in df_engineering.iterrows():\n",
        "        regions = {\n",
        "            \"Run_ID\": row[\"Run_ID\"],\n",
        "            \"Duration_sec\": row.get(\"Duration_sec\", 0),\n",
        "            \"Neck\": row[\"Intensity_Neck_m_per_s\"],\n",
        "            \"Shoulders\": row[\"Intensity_Shoulders_m_per_s\"],\n",
        "            \"Elbows\": row[\"Intensity_Elbows_m_per_s\"],\n",
        "            \"Wrists\": row[\"Intensity_Wrists_m_per_s\"],\n",
        "            \"Spine\": row[\"Intensity_Spine_m_per_s\"],\n",
        "            \"Hips\": row[\"Intensity_Hips_m_per_s\"],\n",
        "            \"Knees\": row[\"Intensity_Knees_m_per_s\"],\n",
        "            \"Ankles\": row[\"Intensity_Ankles_m_per_s\"],\n",
        "        }\n",
        "        intensity_data.append(regions)\n",
        "    \n",
        "    df_intensity = pd.DataFrame(intensity_data)\n",
        "    \n",
        "    print(\"\\nIntensity by Anatomical Region (m/s):\")\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None, 'display.precision', 4):\n",
        "        print(df_intensity.to_string(index=False))\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INTENSITY SUMMARY (across all runs)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    intensity_summary = df_intensity.drop(columns=[\"Run_ID\", \"Duration_sec\"]).describe().loc[[\"mean\", \"min\", \"max\"]]\n",
        "    print(intensity_summary.to_string())\n",
        "    \n",
        "    # Most intense regions\n",
        "    mean_intensity = df_intensity.drop(columns=[\"Run_ID\", \"Duration_sec\"]).mean().sort_values(ascending=False)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MOST INTENSE REGIONS (ranked by average intensity)\")\n",
        "    print(\"=\"*80)\n",
        "    for i, (region, value) in enumerate(mean_intensity.items(), 1):\n",
        "        print(f\"  {i}. {region:12s}: {value:.4f} m/s\")\n",
        "    \n",
        "    # Interpretation guide\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INTERPRETATION GUIDE\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nIntensity Index represents average speed of movement:\")\n",
        "    print(\"  ‚Ä¢ 0.10 - 0.30 m/s : Slow, controlled movements\")\n",
        "    print(\"  ‚Ä¢ 0.30 - 0.60 m/s : Moderate movement speed\")\n",
        "    print(\"  ‚Ä¢ 0.60 - 1.00 m/s : Fast, dynamic movements\")\n",
        "    print(\"  ‚Ä¢ >1.00 m/s       : Very fast movements (e.g., sports, rapid reaching)\")\n",
        "    print(\"\\nHigher intensity = more active movement per unit time\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Intensity Index columns not found. Re-run notebook 06 with Phase 3 updates.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 12: Cross-Session Analysis (Phase 4)\n",
        "\n",
        "**Multi-session comparison and subject-level insights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CROSS-SESSION ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "‚ö†Ô∏è Only 1 session(s) available.\n",
            "Cross-session analysis requires multiple sessions.\n",
            "\n",
            "To enable:\n",
            "  1. Process multiple sessions through the pipeline (notebooks 01-06)\n",
            "  2. Re-run this notebook\n",
            "\n",
            "Current session: 734_T3_P2_R1_Take 2025-12-30 04.12.54 PM_002\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# PHASE 4: CROSS-SESSION ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "if len(df_engineering) > 1:\n",
        "    print(\"=\"*80)\n",
        "    print(\"CROSS-SESSION ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nAnalyzing {len(df_engineering)} sessions across {df_engineering['Subject_ID'].nunique()} subject(s)\")\n",
        "    \n",
        "    # Group by subject\n",
        "    subjects = df_engineering.groupby('Subject_ID')\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUBJECT-LEVEL SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for subject_id, subject_data in subjects:\n",
        "        n_sessions = len(subject_data)\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Subject: {subject_id} ({n_sessions} sessions)\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Session list\n",
        "        print(\"\\nSessions:\")\n",
        "        for idx, row in subject_data.iterrows():\n",
        "            print(f\"  ‚Ä¢ {row['Session_ID']}: {row['Duration_sec']:.1f}s, {row['Total_Frames']} frames\")\n",
        "        \n",
        "        # Key metrics comparison\n",
        "        key_metrics = [\n",
        "            \"Path_Length_Total_m\",\n",
        "            \"Intensity_Mean_m_per_s\",\n",
        "            \"Bilateral_Symmetry_Mean\",\n",
        "            \"Raw_Missing_Data_Percent\",\n",
        "            \"Bone_Length_CV_Percent\",\n",
        "        ]\n",
        "        \n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(\"Key Metrics Across Sessions:\")\n",
        "        print(\"-\"*80)\n",
        "        \n",
        "        for metric in key_metrics:\n",
        "            if metric in subject_data.columns:\n",
        "                values = subject_data[metric].values\n",
        "                mean_val = values.mean()\n",
        "                std_val = values.std()\n",
        "                min_val = values.min()\n",
        "                max_val = values.max()\n",
        "                \n",
        "                # Coefficient of variation (CV%)\n",
        "                cv_pct = (std_val / mean_val * 100) if mean_val > 0 else 0\n",
        "                \n",
        "                print(f\"\\n{metric}:\")\n",
        "                print(f\"  Mean ¬± Std: {mean_val:.4f} ¬± {std_val:.4f}\")\n",
        "                print(f\"  Range: [{min_val:.4f}, {max_val:.4f}]\")\n",
        "                print(f\"  CV%: {cv_pct:.2f}%\")\n",
        "                \n",
        "                # Flag high variability\n",
        "                if cv_pct > 25:\n",
        "                    print(f\"  ‚ö†Ô∏è HIGH VARIABILITY (CV > 25%)\")\n",
        "        \n",
        "        # Movement patterns\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(\"Movement Patterns (Anatomical Regions):\")\n",
        "        print(\"-\"*80)\n",
        "        \n",
        "        region_cols = [\n",
        "            \"Intensity_Wrists_m_per_s\",\n",
        "            \"Intensity_Elbows_m_per_s\",\n",
        "            \"Intensity_Knees_m_per_s\",\n",
        "            \"Intensity_Ankles_m_per_s\",\n",
        "        ]\n",
        "        \n",
        "        for col in region_cols:\n",
        "            if col in subject_data.columns:\n",
        "                region_name = col.replace(\"Intensity_\", \"\").replace(\"_m_per_s\", \"\")\n",
        "                values = subject_data[col].values\n",
        "                print(f\"  {region_name:12s}: {values.mean():.4f} m/s (œÉ={values.std():.4f})\")\n",
        "    \n",
        "    # Anomaly Detection\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ANOMALY DETECTION\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nIdentifying sessions that deviate from subject baseline:\")\n",
        "    \n",
        "    for subject_id, subject_data in subjects:\n",
        "        if len(subject_data) < 2:\n",
        "            print(f\"\\n{subject_id}: Need ‚â•2 sessions for anomaly detection\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\n{subject_id}:\")\n",
        "        \n",
        "        # Check key metrics for outliers (Z-score method)\n",
        "        anomaly_metrics = [\n",
        "            \"Path_Length_Total_m\",\n",
        "            \"Intensity_Mean_m_per_s\",\n",
        "            \"Raw_Missing_Data_Percent\",\n",
        "        ]\n",
        "        \n",
        "        anomalies_found = False\n",
        "        \n",
        "        for metric in anomaly_metrics:\n",
        "            if metric in subject_data.columns:\n",
        "                values = subject_data[metric].values\n",
        "                mean = values.mean()\n",
        "                std = values.std()\n",
        "                \n",
        "                if std > 0:\n",
        "                    z_scores = (values - mean) / std\n",
        "                    \n",
        "                    # Flag outliers (|Z| > 2)\n",
        "                    outlier_mask = np.abs(z_scores) > 2\n",
        "                    \n",
        "                    if outlier_mask.any():\n",
        "                        anomalies_found = True\n",
        "                        outlier_sessions = subject_data[outlier_mask]['Session_ID'].values\n",
        "                        outlier_values = values[outlier_mask]\n",
        "                        outlier_z = z_scores[outlier_mask]\n",
        "                        \n",
        "                        print(f\"\\n  ‚ö†Ô∏è {metric}:\")\n",
        "                        for session, value, z in zip(outlier_sessions, outlier_values, outlier_z):\n",
        "                            direction = \"above\" if z > 0 else \"below\"\n",
        "                            print(f\"      {session}: {value:.4f} (Z={z:.2f}, {direction} baseline)\")\n",
        "        \n",
        "        if not anomalies_found:\n",
        "            print(\"  ‚úÖ No significant anomalies detected\")\n",
        "    \n",
        "    # Trend Analysis (if sessions are ordered by time)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TREND ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for subject_id, subject_data in subjects:\n",
        "        if len(subject_data) < 3:\n",
        "            print(f\"\\n{subject_id}: Need ‚â•3 sessions for trend analysis\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\n{subject_id}:\")\n",
        "        \n",
        "        # Assume sessions are ordered by Session_ID or Run_ID\n",
        "        subject_data_sorted = subject_data.sort_values('Session_ID')\n",
        "        \n",
        "        trend_metrics = [\n",
        "            (\"Path_Length_Total_m\", \"Total Movement\"),\n",
        "            (\"Intensity_Mean_m_per_s\", \"Movement Intensity\"),\n",
        "            (\"Bilateral_Symmetry_Mean\", \"Symmetry\"),\n",
        "        ]\n",
        "        \n",
        "        for metric, label in trend_metrics:\n",
        "            if metric in subject_data_sorted.columns:\n",
        "                values = subject_data_sorted[metric].values\n",
        "                \n",
        "                # Simple linear trend (correlation with session order)\n",
        "                session_order = np.arange(len(values))\n",
        "                correlation = np.corrcoef(session_order, values)[0, 1]\n",
        "                \n",
        "                # Trend direction\n",
        "                if correlation > 0.5:\n",
        "                    trend = \"üìà INCREASING\"\n",
        "                elif correlation < -0.5:\n",
        "                    trend = \"üìâ DECREASING\"\n",
        "                else:\n",
        "                    trend = \"‚û°Ô∏è STABLE\"\n",
        "                \n",
        "                print(f\"  {label:20s}: {trend} (r={correlation:.3f})\")\n",
        "\n",
        "else:\n",
        "    print(\"=\"*80)\n",
        "    print(\"CROSS-SESSION ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\n‚ö†Ô∏è Only {len(df_engineering)} session(s) available.\")\n",
        "    print(\"Cross-session analysis requires multiple sessions.\")\n",
        "    print(\"\\nTo enable:\")\n",
        "    print(\"  1. Process multiple sessions through the pipeline (notebooks 01-06)\")\n",
        "    print(\"  2. Re-run this notebook\")\n",
        "    print(f\"\\nCurrent session: {df_engineering['Run_ID'].iloc[0] if len(df_engineering) > 0 else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SECTION 13: Subject Profiles Export (Phase 4)\n",
        "\n",
        "Export aggregated subject-level profiles for longitudinal analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GENERATING SUBJECT PROFILES\n",
            "================================================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'build_subject_profile' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m subject_profiles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subject_id \u001b[38;5;129;01min\u001b[39;00m df_engineering[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubject_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m---> 14\u001b[0m     profile \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_subject_profile\u001b[49m(df_engineering, subject_id)\n\u001b[0;32m     15\u001b[0m     subject_profiles\u001b[38;5;241m.\u001b[39mappend(profile)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(subject_profiles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m subject profile(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'build_subject_profile' is not defined"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SUBJECT PROFILES EXPORT (Phase 4)\n",
        "# ============================================================\n",
        "\n",
        "if len(df_engineering) > 0:\n",
        "    print(\"=\"*80)\n",
        "    print(\"GENERATING SUBJECT PROFILES\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Build subject profiles\n",
        "    subject_profiles = []\n",
        "    \n",
        "    for subject_id in df_engineering['Subject_ID'].unique():\n",
        "        profile = build_subject_profile(df_engineering, subject_id)\n",
        "        subject_profiles.append(profile)\n",
        "    \n",
        "    print(f\"\\nGenerated {len(subject_profiles)} subject profile(s)\")\n",
        "    \n",
        "    # Save to JSON\n",
        "    subject_profiles_path = os.path.join(EXCEL_DIR, f\"Subject_Profiles_{timestamp_str}.json\")\n",
        "    \n",
        "    with open(subject_profiles_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(subject_profiles, f, indent=2)\n",
        "    \n",
        "    print(f\"‚úÖ Saved: {subject_profiles_path}\")\n",
        "    \n",
        "    # Display summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUBJECT PROFILES SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for profile in subject_profiles:\n",
        "        subject_id = profile.get('subject_id', 'Unknown')\n",
        "        n_sessions = profile.get('n_sessions', 0)\n",
        "        \n",
        "        print(f\"\\n{'-'*80}\")\n",
        "        print(f\"Subject: {subject_id} ({n_sessions} sessions)\")\n",
        "        print(f\"{'-'*80}\")\n",
        "        \n",
        "        # Key metrics\n",
        "        if 'Path_Length_Total_m_mean' in profile:\n",
        "            print(f\"\\nMovement:\")\n",
        "            print(f\"  Path Length:  {profile['Path_Length_Total_m_mean']:.2f}m (œÉ={profile['Path_Length_Total_m_std']:.2f})\")\n",
        "            print(f\"  Intensity:    {profile['Intensity_Mean_m_per_s_mean']:.4f} m/s (œÉ={profile['Intensity_Mean_m_per_s_std']:.4f})\")\n",
        "        \n",
        "        if 'Bilateral_Symmetry_Mean_mean' in profile:\n",
        "            print(f\"\\nSymmetry:\")\n",
        "            print(f\"  Mean Index:   {profile['Bilateral_Symmetry_Mean_mean']:.3f} (œÉ={profile['Bilateral_Symmetry_Mean_std']:.3f})\")\n",
        "        \n",
        "        if 'Raw_Missing_Data_Percent_mean' in profile:\n",
        "            print(f\"\\nData Quality:\")\n",
        "            print(f\"  Missing Data: {profile['Raw_Missing_Data_Percent_mean']:.2f}% (œÉ={profile['Raw_Missing_Data_Percent_std']:.2f}%)\")\n",
        "            print(f\"  Bone CV%:     {profile['Bone_Length_CV_Percent_mean']:.3f}% (œÉ={profile['Bone_Length_CV_Percent_std']:.3f}%)\")\n",
        "        \n",
        "        # Consistency assessment\n",
        "        if 'consistency_assessment' in profile:\n",
        "            print(f\"\\nConsistency:\")\n",
        "            for metric, assessment in profile['consistency_assessment'].items():\n",
        "                metric_short = metric.replace(\"_m\", \"\").replace(\"_Percent\", \"\").replace(\"_\", \" \")\n",
        "                print(f\"  {metric_short:30s}: {assessment}\")\n",
        "        \n",
        "        # Movement signature\n",
        "        if 'movement_signature' in profile:\n",
        "            print(f\"\\nMovement Pattern (Intensity by Region):\")\n",
        "            for region, stats in sorted(profile['movement_signature'].items(), key=lambda x: x[1]['mean'], reverse=True):\n",
        "                print(f\"  {region:12s}: {stats['mean']:.4f} m/s (œÉ={stats['std']:.4f})\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Subject profiles saved to: {subject_profiles_path}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No sessions available for subject profile generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_section_header(\"CAPTURE BASELINE PROFILE (RAW STATE)\")\n",
        "\n",
        "cols_baseline = [\n",
        "    'Run_ID',\n",
        "    'Total_Frames',\n",
        "    'Duration_sec',\n",
        "    'Native_Sampling_Rate_Hz',\n",
        "    'Raw_Missing_Data_Percent',\n",
        "    'OptiTrack_System_Error_mm',\n",
        "    'True_Raw_SNR_Mean_dB',\n",
        "    'True_Raw_SNR_Min_dB',\n",
        "    'True_Raw_SNR_Max_dB',\n",
        "    'SNR_Joints_Excellent_Count',\n",
        "    'SNR_Joints_Failed_Count'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_baseline])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BASELINE SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nRecording Duration:\")\n",
        "print(f\"  Total: {df_engineering['Duration_sec'].sum():.1f} seconds ({df_engineering['Duration_sec'].sum()/60:.1f} minutes)\")\n",
        "print(f\"  Mean: {df_engineering['Duration_sec'].mean():.1f} seconds\")\n",
        "print(f\"  Range: {df_engineering['Duration_sec'].min():.1f} - {df_engineering['Duration_sec'].max():.1f} seconds\")\n",
        "\n",
        "print(f\"\\nRaw Data Completeness:\")\n",
        "pristine_count = (df_engineering['Raw_Missing_Data_Percent'] == 0).sum()\n",
        "print(f\"  Pristine (0% missing): {pristine_count}/{len(df_engineering)} recordings\")\n",
        "print(f\"  Mean Missing: {df_engineering['Raw_Missing_Data_Percent'].mean():.3f}%\")\n",
        "print(f\"  Max Missing: {df_engineering['Raw_Missing_Data_Percent'].max():.3f}%\")\n",
        "\n",
        "print(f\"\\nInherent Signal Quality (Pre-Processing SNR):\")\n",
        "print(f\"  Mean SNR: {df_engineering['True_Raw_SNR_Mean_dB'].mean():.1f} dB\")\n",
        "print(f\"  Best Recording: {df_engineering['True_Raw_SNR_Max_dB'].max():.1f} dB\")\n",
        "print(f\"  Worst Recording: {df_engineering['True_Raw_SNR_Min_dB'].min():.1f} dB\")\n",
        "\n",
        "# Interpret SNR levels\n",
        "mean_snr = df_engineering['True_Raw_SNR_Mean_dB'].mean()\n",
        "if mean_snr >= 30:\n",
        "    snr_interpretation = \"EXCELLENT - Publication-quality capture\"\n",
        "elif mean_snr >= 20:\n",
        "    snr_interpretation = \"GOOD - Acceptable for research\"\n",
        "elif mean_snr >= 15:\n",
        "    snr_interpretation = \"ACCEPTABLE - Review recommended\"\n",
        "else:\n",
        "    snr_interpretation = \"POOR - Check capture environment\"\n",
        "\n",
        "print(f\"  Interpretation: {snr_interpretation}\")\n",
        "\n",
        "print(f\"\\nOptiTrack System Calibration:\")\n",
        "print(f\"  Mean Error: {df_engineering['OptiTrack_System_Error_mm'].mean():.3f} mm\")\n",
        "print(f\"  Max Error: {df_engineering['OptiTrack_System_Error_mm'].max():.3f} mm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"structure\"></a>\n",
        "## 5. Structural Integrity\n",
        "\n",
        "**Purpose:** Verify skeleton hierarchy and biomechanical stability.\n",
        "\n",
        "This section documents:\n",
        "- Skeleton completeness (all expected segments present?)\n",
        "- Bone length stability (CV% - rigid body assumption validity)\n",
        "- Subject anthropometry (height, mass)\n",
        "- Static pose calibration offsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_section_header(\"STRUCTURAL INTEGRITY PROFILE\")\n",
        "\n",
        "cols_structure = [\n",
        "    'Run_ID',\n",
        "    'Skeleton_Segments_Found',\n",
        "    'Skeleton_Segments_Missing',\n",
        "    'Bone_Length_CV_Percent',\n",
        "    'Worst_Bone_Segment',\n",
        "    'Subject_Height_cm',\n",
        "    'Subject_Mass_kg',\n",
        "    'Left_Arm_Offset_deg',\n",
        "    'Right_Arm_Offset_deg'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_structure])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SKELETON STABILITY ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nBone Length Coefficient of Variation:\")\n",
        "print(f\"  Mean CV: {df_engineering['Bone_Length_CV_Percent'].mean():.4f}%\")\n",
        "print(f\"  Range: {df_engineering['Bone_Length_CV_Percent'].min():.4f}% - {df_engineering['Bone_Length_CV_Percent'].max():.4f}%\")\n",
        "print(f\"\\n  Interpretation (R√°cz et al., 2025):\")\n",
        "print(f\"    CV < 0.5%:  Excellent rigidity (research-grade)\")\n",
        "print(f\"    CV 0.5-1%:  Good (acceptable soft tissue artifact)\")\n",
        "print(f\"    CV 1-2%:    Marginal (review recommended)\")\n",
        "print(f\"    CV > 2%:    Poor (tracking or marker placement issue)\")\n",
        "\n",
        "print(f\"\\nWorst Bone Segments (Most Variable):\")\n",
        "worst_bones = df_engineering.groupby('Worst_Bone_Segment')['Bone_Length_CV_Percent'].agg(['count', 'mean']).sort_values('count', ascending=False)\n",
        "for bone, stats in worst_bones.head(5).iterrows():\n",
        "    print(f\"  {bone}: {int(stats['count'])} recordings, Mean CV = {stats['mean']:.4f}%\")\n",
        "\n",
        "print(f\"\\nAnthropometry:\")\n",
        "print(f\"  Mean Height: {df_engineering['Subject_Height_cm'].mean():.1f} cm\")\n",
        "print(f\"  Height Range: {df_engineering['Subject_Height_cm'].min():.1f} - {df_engineering['Subject_Height_cm'].max():.1f} cm\")\n",
        "valid_mass = df_engineering['Subject_Mass_kg'][df_engineering['Subject_Mass_kg'] > 0]\n",
        "if len(valid_mass) > 0:\n",
        "    print(f\"  Mean Mass: {valid_mass.mean():.1f} kg\")\n",
        "\n",
        "print(f\"\\nStatic Pose Calibration Offsets:\")\n",
        "print(f\"  Left Arm Mean: {df_engineering['Left_Arm_Offset_deg'].mean():.2f}¬∞\")\n",
        "print(f\"  Right Arm Mean: {df_engineering['Right_Arm_Offset_deg'].mean():.2f}¬∞\")\n",
        "print(f\"  Max Bilateral Asymmetry: {abs(df_engineering['Left_Arm_Offset_deg'] - df_engineering['Right_Arm_Offset_deg']).max():.2f}¬∞\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display selected segments for one representative run\n",
        "if len(runs_data) > 0:\n",
        "    representative_run = list(runs_data.keys())[0]\n",
        "    selected_segments = extract_selected_segments(runs_data[representative_run])\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"SELECTED KINEMATIC SEGMENTS (19 Joints)\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nThese segments are used for rotation analysis and kinematic computation:\")\n",
        "    print(f\"\\nRepresentative Run: {representative_run[:60]}...\\n\")\n",
        "    \n",
        "    # Group by body region\n",
        "    regions = {\n",
        "        \"Trunk\": [s for s in selected_segments if s in ['Hips', 'Spine', 'Spine1', 'Neck', 'Head']],\n",
        "        \"Left Upper Limb\": [s for s in selected_segments if s.startswith('Left') and s in ['LeftShoulder', 'LeftArm', 'LeftForeArm', 'LeftHand']],\n",
        "        \"Right Upper Limb\": [s for s in selected_segments if s.startswith('Right') and s in ['RightShoulder', 'RightArm', 'RightForeArm', 'RightHand']],\n",
        "        \"Left Lower Limb\": [s for s in selected_segments if s.startswith('Left') and s in ['LeftUpLeg', 'LeftLeg', 'LeftFoot']],\n",
        "        \"Right Lower Limb\": [s for s in selected_segments if s.startswith('Right') and s in ['RightUpLeg', 'RightLeg', 'RightFoot']]\n",
        "    }\n",
        "    \n",
        "    for region, segments in regions.items():\n",
        "        if segments:\n",
        "            print(f\"  {region}:\")\n",
        "            for seg in segments:\n",
        "                print(f\"    - {seg}\")\n",
        "    \n",
        "    print(f\"\\nTotal: {len(selected_segments)} segments\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"signal\"></a>\n",
        "## 6. Signal Quality Profile\n",
        "\n",
        "**Purpose:** TRUE RAW SNR - Capture quality assessment BEFORE any filtering.\n",
        "\n",
        "Method: Raw data frequency analysis (signal: 0.5-10Hz, noise: 15-50Hz)\n",
        "\n",
        "This measures inherent capture quality, NOT filtering effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_section_header(\"SIGNAL QUALITY PROFILE (TRUE RAW SNR)\")\n",
        "\n",
        "cols_signal = [\n",
        "    'Run_ID',\n",
        "    'True_Raw_SNR_Mean_dB',\n",
        "    'True_Raw_SNR_Min_dB',\n",
        "    'True_Raw_SNR_Max_dB',\n",
        "    'SNR_Joints_Excellent_Count',\n",
        "    'SNR_Joints_Failed_Count',\n",
        "    'SNR_Failed_Joint_List'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_signal])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SNR INTERPRETATION GUIDE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nSNR > 30 dB:  EXCELLENT - Publication-quality, minimal noise\")\n",
        "print(f\"SNR 20-30 dB: GOOD - Acceptable for research, moderate noise\")\n",
        "print(f\"SNR 15-20 dB: ACCEPTABLE - Usable but review recommended\")\n",
        "print(f\"SNR < 15 dB:  POOR - High noise, check capture environment\")\n",
        "\n",
        "# Histogram of SNR distribution\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SNR DISTRIBUTION ACROSS RECORDINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "snr_bins = pd.cut(df_engineering['True_Raw_SNR_Mean_dB'], bins=[0, 15, 20, 30, 100], labels=['POOR', 'ACCEPTABLE', 'GOOD', 'EXCELLENT'])\n",
        "snr_counts = snr_bins.value_counts().sort_index()\n",
        "print(f\"\\n{snr_counts.to_string()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"processing\"></a>\n",
        "## 7. Processing Transparency\n",
        "\n",
        "**Purpose:** Document exactly what was done to the raw data.\n",
        "\n",
        "This section provides:\n",
        "- Interpolation methods used\n",
        "- Resampling parameters\n",
        "- Filtering strategy\n",
        "- 3-stage cleaning metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_section_header(\"PROCESSING TRANSPARENCY\")\n",
        "\n",
        "cols_processing = [\n",
        "    'Run_ID',\n",
        "    'Interpolation_Method_Positions',\n",
        "    'Interpolation_Method_Rotations',\n",
        "    'Resampling_Target_Hz',\n",
        "    'Temporal_Grid_Std_ms',\n",
        "    'Filtering_Mode',\n",
        "    'Filter_Cutoff_Weighted_Avg_Hz',\n",
        "    'Filter_Residual_RMS_mm'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_processing])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3-STAGE CLEANING METRICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "cols_cleaning = [\n",
        "    'Run_ID',\n",
        "    'Stage1_Total_Artifacts_Detected',\n",
        "    'Stage1_Artifact_Percent',\n",
        "    'Stage2_Hampel_Outliers',\n",
        "    'Stage2_Hampel_Percent',\n",
        "    'Stage3_Winter_Cutoff_Min_Hz',\n",
        "    'Stage3_Winter_Cutoff_Max_Hz',\n",
        "    'Stage3_Winter_Cutoff_Mean_Hz'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_cleaning])\n",
        "\n",
        "print(\"\\nProcessing Summary:\")\n",
        "print(f\"  Stage 1 (Artifact Detection):\")\n",
        "print(f\"    Total artifacts: {df_engineering['Stage1_Total_Artifacts_Detected'].sum()} frames\")\n",
        "print(f\"    Mean rate: {df_engineering['Stage1_Artifact_Percent'].mean():.3f}%\")\n",
        "\n",
        "print(f\"\\n  Stage 2 (Hampel Filter):\")\n",
        "print(f\"    Total outliers: {df_engineering['Stage2_Hampel_Outliers'].sum()} frames\")\n",
        "print(f\"    Mean rate: {df_engineering['Stage2_Hampel_Percent'].mean():.3f}%\")\n",
        "\n",
        "print(f\"\\n  Stage 3 (Adaptive Winter):\")\n",
        "print(f\"    Cutoff range: {df_engineering['Stage3_Winter_Cutoff_Min_Hz'].min():.1f} - {df_engineering['Stage3_Winter_Cutoff_Max_Hz'].max():.1f} Hz\")\n",
        "print(f\"    Mean cutoff: {df_engineering['Stage3_Winter_Cutoff_Mean_Hz'].mean():.1f} Hz\")\n",
        "\n",
        "print(f\"\\n  Filter Residual (Price of Smoothing):\")\n",
        "print(f\"    Mean RMS: {df_engineering['Filter_Residual_RMS_mm'].mean():.2f} mm\")\n",
        "print(f\"    Range: {df_engineering['Filter_Residual_RMS_mm'].min():.2f} - {df_engineering['Filter_Residual_RMS_mm'].max():.2f} mm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"kinematics\"></a>\n",
        "## 8. Kinematic Extremes (Processed Output)\n",
        "\n",
        "**Purpose:** Report the final kinematic values after all processing.\n",
        "\n",
        "These are the peak velocities and accelerations extracted from the cleaned data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_section_header(\"KINEMATIC EXTREMES (PROCESSED OUTPUT)\")\n",
        "\n",
        "cols_kinematics = [\n",
        "    'Run_ID',\n",
        "    'Max_Angular_Velocity_deg_s',\n",
        "    'Max_Angular_Acceleration_deg_s2',\n",
        "    'Max_Linear_Velocity_mm_s',\n",
        "    'Max_Linear_Acceleration_mm_s2',\n",
        "    'Path_Length_Hips_mm',\n",
        "    'Intensity_Index'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_kinematics])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KINEMATIC SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nAngular Velocity:\")\n",
        "print(f\"  Peak (across all recordings): {df_engineering['Max_Angular_Velocity_deg_s'].max():.2f} deg/s\")\n",
        "print(f\"  Mean of maxima: {df_engineering['Max_Angular_Velocity_deg_s'].mean():.2f} deg/s\")\n",
        "print(f\"\\n  Reference Values:\")\n",
        "print(f\"    Normal movement: < 800 deg/s\")\n",
        "print(f\"    Athletic: 800-1500 deg/s\")\n",
        "print(f\"    Gaga dance (distal): up to 2250 deg/s\")\n",
        "print(f\"    Tracking artifact threshold: > 2500 deg/s\")\n",
        "\n",
        "print(f\"\\nAngular Acceleration:\")\n",
        "print(f\"  Peak: {df_engineering['Max_Angular_Acceleration_deg_s2'].max():.0f} deg/s¬≤\")\n",
        "print(f\"  Mean of maxima: {df_engineering['Max_Angular_Acceleration_deg_s2'].mean():.0f} deg/s¬≤\")\n",
        "print(f\"\\n  Reference Values:\")\n",
        "print(f\"    Smooth movement: < 30,000 deg/s¬≤\")\n",
        "print(f\"    Rapid transitions: 30,000-50,000 deg/s¬≤\")\n",
        "print(f\"    Extreme/impact: > 50,000 deg/s¬≤\")\n",
        "\n",
        "print(f\"\\nLinear Acceleration:\")\n",
        "print(f\"  Peak: {df_engineering['Max_Linear_Acceleration_mm_s2'].max():.0f} mm/s¬≤ ({df_engineering['Max_Linear_Acceleration_mm_s2'].max()/1000:.1f} m/s¬≤)\")\n",
        "print(f\"  Mean of maxima: {df_engineering['Max_Linear_Acceleration_mm_s2'].mean():.0f} mm/s¬≤ ({df_engineering['Max_Linear_Acceleration_mm_s2'].mean()/1000:.1f} m/s¬≤)\")\n",
        "\n",
        "# Note about Path Length\n",
        "if df_engineering['Path_Length_Hips_mm'].sum() == 0:\n",
        "    print(f\"\\n‚ö†Ô∏è Path Length: Not computed in current pipeline version\")\n",
        "    print(f\"   (Requires upstream fix in 06_rotvec_omega.ipynb)\")\n",
        "else:\n",
        "    print(f\"\\nPath Length (Hips):\")\n",
        "    print(f\"  Total: {df_engineering['Path_Length_Hips_mm'].sum()/1000:.1f} meters\")\n",
        "    print(f\"  Mean per recording: {df_engineering['Path_Length_Hips_mm'].mean()/1000:.1f} meters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"noise\"></a>\n",
        "## 9. Per-Joint Noise Profile\n",
        "\n",
        "**Purpose:** Root cause analysis for noisy segments.\n",
        "\n",
        "This section identifies:\n",
        "- Which joints have the most outlier frames?\n",
        "- Is the noise localized (one joint) or systemic (whole skeleton)?\n",
        "- Are the outliers sporadic glitches or sustained high-intensity movement?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_section_header(\"PER-JOINT NOISE PROFILE\")\n",
        "\n",
        "# Extract per-joint profiles for one representative run\n",
        "if len(runs_data) > 0:\n",
        "    # Pick the run with highest outlier rate for demonstration\n",
        "    worst_run_idx = df_engineering['Outlier_Frames_Percent'].idxmax()\n",
        "    worst_run_id = df_engineering.loc[worst_run_idx, 'Run_ID']\n",
        "    \n",
        "    print(f\"\\nAnalyzing: {worst_run_id}\")\n",
        "    print(f\"(Run with highest outlier rate: {df_engineering.loc[worst_run_idx, 'Outlier_Frames_Percent']:.3f}%)\\n\")\n",
        "    \n",
        "    # Extract per-joint profile\n",
        "    joint_profile = extract_per_joint_noise_profile(runs_data[worst_run_id])\n",
        "    \n",
        "    if not joint_profile.empty:\n",
        "        # Sort by outlier percentage\n",
        "        joint_profile_sorted = joint_profile.sort_values('Outlier_Percent', ascending=False)\n",
        "        \n",
        "        print(\"Per-Joint Outlier Profile:\")\n",
        "        print(\"=\"*80)\n",
        "        display(joint_profile_sorted)\n",
        "        \n",
        "        # Compute noise locality index\n",
        "        locality_index = compute_noise_locality_index(joint_profile)\n",
        "        \n",
        "        print(f\"\\nNoise Locality Index: {locality_index:.2f}\")\n",
        "        print(f\"\\nInterpretation:\")\n",
        "        if locality_index > 5:\n",
        "            print(f\"  HIGH - Localized tracking issue in specific joint(s)\")\n",
        "            print(f\"  ‚Üí Check marker placement and occlusion for worst joints\")\n",
        "        elif locality_index > 2:\n",
        "            print(f\"  MEDIUM - Regional problem (e.g., one limb)\")\n",
        "            print(f\"  ‚Üí Review calibration for affected body region\")\n",
        "        else:\n",
        "            print(f\"  LOW - Systemic noise across skeleton\")\n",
        "            print(f\"  ‚Üí Check capture environment (lighting, camera calibration)\")\n",
        "        \n",
        "        # Classification summary\n",
        "        print(f\"\\nClassification Summary:\")\n",
        "        for classification, count in joint_profile['Classification'].value_counts().items():\n",
        "            print(f\"  {classification}: {count} joints\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No per-joint outlier data available for this run.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No runs available for analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"outliers\"></a>\n",
        "## 10. Outlier Distribution\n",
        "\n",
        "**Purpose:** Frame-level outlier patterns and event classification.\n",
        "\n",
        "This section documents:\n",
        "- Total outlier frames and percentage\n",
        "- Maximum consecutive outlier runs\n",
        "- Tier 1/2/3 event classification (Artifact/Burst/Flow)\n",
        "- Data retention after artifact exclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_section_header(\"OUTLIER DISTRIBUTION ANALYSIS\")\n",
        "\n",
        "cols_outliers = [\n",
        "    'Run_ID',\n",
        "    'Total_Outlier_Frames',\n",
        "    'Outlier_Frames_Percent',\n",
        "    'Max_Consecutive_Outlier_Frames',\n",
        "    'Artifact_Events_Tier1',\n",
        "    'Burst_Events_Tier2',\n",
        "    'Flow_Events_Tier3',\n",
        "    'Artifact_Frame_Rate_Percent'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_outliers])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVENT CLASSIFICATION (3-TIER SYSTEM)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nTier 1 - Artifacts (1-3 consecutive frames):\")\n",
        "print(f\"  Total events: {df_engineering['Artifact_Events_Tier1'].sum()}\")\n",
        "print(f\"  Mean per recording: {df_engineering['Artifact_Events_Tier1'].mean():.1f}\")\n",
        "print(f\"  Frame rate: {df_engineering['Artifact_Frame_Rate_Percent'].mean():.4f}%\")\n",
        "print(f\"  Interpretation: Sporadic tracking glitches (excluded from analysis)\")\n",
        "\n",
        "print(f\"\\nTier 2 - Bursts (4-7 consecutive frames):\")\n",
        "print(f\"  Total events: {df_engineering['Burst_Events_Tier2'].sum()}\")\n",
        "print(f\"  Mean per recording: {df_engineering['Burst_Events_Tier2'].mean():.1f}\")\n",
        "print(f\"  Interpretation: Rapid movement transitions (preserved for Gaga analysis)\")\n",
        "\n",
        "print(f\"\\nTier 3 - Flows (8+ consecutive frames):\")\n",
        "print(f\"  Total events: {df_engineering['Flow_Events_Tier3'].sum()}\")\n",
        "print(f\"  Mean per recording: {df_engineering['Flow_Events_Tier3'].mean():.1f}\")\n",
        "print(f\"  Interpretation: Sustained high-intensity movement (legitimate dance)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA RETENTION AFTER ARTIFACT EXCLUSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "cols_retention = [\n",
        "    'Run_ID',\n",
        "    'Clean_Max_Velocity_deg_s',\n",
        "    'Clean_Mean_Velocity_deg_s',\n",
        "    'Velocity_Reduction_Percent',\n",
        "    'Data_Retained_Percent',\n",
        "    'Excluded_Frame_Count'\n",
        "]\n",
        "\n",
        "display(df_engineering[cols_retention])\n",
        "\n",
        "print(f\"\\nRetention Summary:\")\n",
        "print(f\"  Mean data retained: {df_engineering['Data_Retained_Percent'].mean():.4f}%\")\n",
        "print(f\"  Total frames excluded: {df_engineering['Excluded_Frame_Count'].sum()}\")\n",
        "print(f\"  Mean velocity reduction: {df_engineering['Velocity_Reduction_Percent'].mean():.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a id=\"export\"></a>\n",
        "## 11. Excel Export\n",
        "\n",
        "**Output:** `reports/Engineering_Audit_YYYYMMDD_HHMMSS.xlsx`\n",
        "\n",
        "**Sheets:**\n",
        "1. Engineering_Profile - All physical measurements (no scores)\n",
        "2. Methodology_Passport - Mathematical documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_section_header(\"EXPORT TO EXCEL\")\n",
        "\n",
        "# Create output path\n",
        "REPORTS_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "excel_path = os.path.join(REPORTS_DIR, f\"Engineering_Audit_{timestamp}.xlsx\")\n",
        "\n",
        "# Export to Excel\n",
        "with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
        "    workbook = writer.book\n",
        "    \n",
        "    # Formats\n",
        "    title_fmt = workbook.add_format({\n",
        "        'bold': True, 'font_size': 16, \n",
        "        'bg_color': '#2E75B6', 'font_color': 'white'\n",
        "    })\n",
        "    header_fmt = workbook.add_format({\n",
        "        'bold': True, 'bg_color': '#4472C4', \n",
        "        'font_color': 'white', 'text_wrap': True\n",
        "    })\n",
        "    \n",
        "    # ============================================================\n",
        "    # SHEET 1: ENGINEERING PROFILE\n",
        "    # ============================================================\n",
        "    df_engineering.to_excel(writer, index=False, sheet_name='Engineering_Profile')\n",
        "    \n",
        "    ws_profile = writer.sheets['Engineering_Profile']\n",
        "    for col_num, value in enumerate(df_engineering.columns):\n",
        "        ws_profile.write(0, col_num, value, header_fmt)\n",
        "    \n",
        "    # Auto-fit columns\n",
        "    for i, col in enumerate(df_engineering.columns):\n",
        "        max_len = max(df_engineering[col].astype(str).str.len().max(), len(str(col)))\n",
        "        ws_profile.set_column(i, i, min(max_len + 2, 50))\n",
        "    \n",
        "    # ============================================================\n",
        "    # SHEET 2: METHODOLOGY PASSPORT\n",
        "    # ============================================================\n",
        "    methodology_sheet = workbook.add_worksheet('Methodology_Passport')\n",
        "    \n",
        "    methodology_sheet.merge_range('A1:E1', 'METHODOLOGY PASSPORT - MATHEMATICAL DOCUMENTATION', title_fmt)\n",
        "    methodology_sheet.write('A2', f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    methodology_sheet.write('A3', f\"Pipeline Version: v3.0_3stage_signal_cleaning\")\n",
        "    \n",
        "    row = 5\n",
        "    \n",
        "    # Write methodology as structured text\n",
        "    methodology_sheet.write(row, 0, 'INTERPOLATION METHODS', header_fmt)\n",
        "    row += 1\n",
        "    \n",
        "    for key, method in METHODOLOGY_PASSPORT[\"interpolation\"].items():\n",
        "        methodology_sheet.write(row, 0, key.title())\n",
        "        row += 1\n",
        "        for field, value in method.items():\n",
        "            methodology_sheet.write(row, 1, field)\n",
        "            methodology_sheet.write(row, 2, str(value))\n",
        "            row += 1\n",
        "        row += 1\n",
        "    \n",
        "    methodology_sheet.set_column('A:A', 20)\n",
        "    methodology_sheet.set_column('B:B', 25)\n",
        "    methodology_sheet.set_column('C:E', 60)\n",
        "\n",
        "print(f\"\\n‚úÖ Engineering Audit Created:\")\n",
        "print(f\"   {excel_path}\")\n",
        "print(f\"\\n   Sheets:\")\n",
        "print(f\"   1. Engineering_Profile - {len(df_engineering)} recordings √ó {len(df_engineering.columns)} measurements\")\n",
        "print(f\"   2. Methodology_Passport - Mathematical documentation\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"NOTEBOOK COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nRecordings Processed: {len(df_engineering)}\")\n",
        "print(f\"Excel Output: {excel_path}\")\n",
        "print(f\"\\nThis report contains ZERO synthetic scores and ZERO decision labels.\")\n",
        "print(f\"All values are pure physical measurements for researcher interpretation.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
