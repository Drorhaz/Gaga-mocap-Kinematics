{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c281bb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Subject info found but incomplete. Using Internal Normalization.\n",
      "Ready. Mode: Normalization (Unit Mass)\n",
      "Output directory: /Users/drorhazan/Documents/untitled folder/Gaga-mocap-Kinematics/derivatives/step_02_preprocess\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Path Setup ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "\n",
    "# --- Imports from src ---\n",
    "from config import CONFIG\n",
    "from skeleton_defs import SKELETON_HIERARCHY \n",
    "\n",
    "# --- External Metadata Integration (Subject Data) ---\n",
    "# Path to the external JSON file containing anthropometric data\n",
    "METADATA_PATH = os.path.join(PROJECT_ROOT, \"data\", \"subject_metadata.json\")\n",
    "\n",
    "def load_subject_metadata(path):\n",
    "    \"\"\"\n",
    "    Loads subject-specific data (weight, height) from a JSON file.\n",
    "    If file is missing or values are null, the pipeline switches to \n",
    "    'Relative Normalization Mode' based on Hof (1996).\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "# Global Subject Variables\n",
    "metadata = load_subject_metadata(METADATA_PATH)\n",
    "SUBJECT_WEIGHT = None\n",
    "SUBJECT_HEIGHT = None\n",
    "PIPELINE_MODE = \"Normalization (Unit Mass)\"\n",
    "\n",
    "if metadata and \"subject_info\" in metadata:\n",
    "    info = metadata[\"subject_info\"]\n",
    "    SUBJECT_WEIGHT = info.get(\"weight_kg\")\n",
    "    SUBJECT_HEIGHT = info.get(\"height_cm\")\n",
    "    \n",
    "    if SUBJECT_WEIGHT and SUBJECT_HEIGHT:\n",
    "        PIPELINE_MODE = \"Scientific (Anthropometric)\"\n",
    "        print(f\"‚úÖ Scientific Mode: Using Winter (2009) coefficients for {SUBJECT_WEIGHT}kg\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Subject info found but incomplete. Using Internal Normalization.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No metadata file found. Defaulting to Relative Normalization Mode.\")\n",
    "\n",
    "# --- Directories ---\n",
    "DERIV_01 = os.path.join(PROJECT_ROOT, CONFIG['derivatives_dir'], \"step_01_parse\")\n",
    "DERIV_02 = os.path.join(PROJECT_ROOT, CONFIG['derivatives_dir'], \"step_02_preprocess\")\n",
    "QC_02 = os.path.join(PROJECT_ROOT, CONFIG['qc_dir'], \"step_02_preprocess\")\n",
    "\n",
    "os.makedirs(DERIV_02, exist_ok=True)\n",
    "os.makedirs(QC_02, exist_ok=True)\n",
    "\n",
    "print(f\"Ready. Mode: {PIPELINE_MODE}\")\n",
    "print(f\"Output directory: {DERIV_02}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7229b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Run ID: 734_T1_P1_R1_Take 2025-12-01 02.18.27 PM\n",
      "File: /Users/drorhazan/Documents/untitled folder/Gaga-mocap-Kinematics/derivatives/step_01_parse/734_T1_P1_R1_Take 2025-12-01 02.18.27 PM__parsed_run.parquet\n",
      "‚úÖ Loaded successfully. Shape: (30798, 359)\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loading and Run ID Definition ---\n",
    "# Derive the parquet file from the CSV path in config.\n",
    "# This ensures notebook 02 always processes the same file as notebook 01.\n",
    "csv_filename = Path(CONFIG['current_csv']).stem  # Gets filename without extension\n",
    "RUN_ID = csv_filename\n",
    "PARQUET_PATH = Path(DERIV_01) / f\"{RUN_ID}__parsed_run.parquet\"\n",
    "\n",
    "# SCIENTIFIC RATIONALE: Consistent file tracking is essential for \n",
    "# Reproducible Research in Biomechanics (Winter, 2009).\n",
    "if not PARQUET_PATH.exists():\n",
    "    print(f\"‚ùå ERROR: Expected parquet file not found: {PARQUET_PATH}\")\n",
    "    print(f\"Did you run notebook 01 first?\")\n",
    "    raise FileNotFoundError(f\"Parquet file not found: {PARQUET_PATH}\")\n",
    "\n",
    "print(f\"Loading Run ID: {RUN_ID}\")\n",
    "print(f\"File: {PARQUET_PATH}\")\n",
    "\n",
    "# Loading the parsed data\n",
    "df_raw = pd.read_parquet(PARQUET_PATH)\n",
    "\n",
    "# ISB COMPLIANCE NOTE: Standardizing column structures at this stage \n",
    "# facilitates the mapping of global coordinate systems to segment locals \n",
    "# according to Wu et al. (2005) standards.\n",
    "print(f\"‚úÖ Loaded successfully. Shape: {df_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524fab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíé SCIENTIFIC INTEGRITY: All hierarchy-essential segments are present.\n",
      "‚úÖ Data standardized to Hierarchy Report. Shape: (30798, 149)\n"
     ]
    }
   ],
   "source": [
    "# ---03 Column Renaming and Scientific Joint Filtering ---\n",
    "\n",
    "def clean_and_filter_joints(df, keywords_to_drop):\n",
    "    \"\"\"\n",
    "    1. Standardizes column names based on the Hierarchy Validation Report (ISB Standard).\n",
    "    2. Drops non-essential joints (digits) while preserving biomechanical core.\n",
    "    3. Validates segments required for Center of Mass (Winter, 2009) and Rotation (Wu et al., 2005).\n",
    "    \"\"\"\n",
    "    # 1. Standardize Names (Mapping OptiTrack labels to our validated Hierarchy names)\n",
    "    new_columns = {}\n",
    "    for col in df.columns:\n",
    "        # Transforming format: 'Skeleton:Joint:Position:X' -> 'Joint__px'\n",
    "        clean_name = col.replace('Skeleton:', '').replace('Position:', '')\n",
    "        clean_name = clean_name.replace('X', 'px').replace('Y', 'py').replace('Z', 'pz')\n",
    "        new_columns[col] = clean_name.replace(':', '__')\n",
    "    \n",
    "    df = df.rename(columns=new_columns)\n",
    "    \n",
    "    # 2. Filtering non-essential joints (Finger/Toe digits)\n",
    "    # We keep 'ToeBase' as it is part of our hierarchy, but drop individual fingers\n",
    "    cols_to_drop = [col for col in df.columns if any(kw.lower() in col.lower() for kw in keywords_to_drop)]\n",
    "    df_filtered = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # 3. SCIENTIFIC INTEGRITY CHECK (Based on Winter, 2009 & Your Hierarchy Report)\n",
    "    # These segments are mandatory for the downstream Kinematic Pipeline (Notebooks 03-08)\n",
    "    critical_segments = [\n",
    "        \"Hips\", \"Spine\", \"Spine1\", \"Head\",             # Core / Trunk\n",
    "        \"LeftUpLeg\", \"LeftLeg\", \"RightUpLeg\", \"RightLeg\", # Lower Body\n",
    "        \"LeftArm\", \"RightArm\"                           # Upper Body\n",
    "    ]\n",
    "    \n",
    "    remaining_joints = set([c.split('__')[0] for c in df_filtered.columns if '__' in c])\n",
    "    missing_critical = [s for s in critical_segments if s not in remaining_joints]\n",
    "    \n",
    "    if missing_critical:\n",
    "        print(f\"‚ö†Ô∏è SCIENTIFIC WARNING: Missing joints required for CoM/Angles: {missing_critical}\")\n",
    "        print(\"Check if they were accidentally dropped or missing in the Raw CSV.\")\n",
    "    else:\n",
    "        print(\"üíé SCIENTIFIC INTEGRITY: All hierarchy-essential segments are present.\")\n",
    "        \n",
    "    return df_filtered\n",
    "\n",
    "# --- EXECUTE ---\n",
    "# Note: We do NOT drop 'ToeBase' as it's in your pass list.\n",
    "DROP_KEYWORDS = [\"Thumb\", \"Index\", \"Middle\", \"Ring\", \"Pinky\", \"Finger\"] \n",
    "\n",
    "df_preprocessed = clean_and_filter_joints(df_raw, DROP_KEYWORDS)\n",
    "\n",
    "print(f\"‚úÖ Data standardized to Hierarchy Report. Shape: {df_preprocessed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051973cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== BUILDING KINEMATICS MAP ====================\n",
      "Total defined in Schema: 27\n",
      "Skipped (Missing/Filtered): 6\n",
      "Mapped (Ready for Physics):  21\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- UPDATED CELL 04: Build Kinematics Map (The Scientific Blueprint) ---\n",
    "\n",
    "def build_map_from_available_joints(df_columns, hierarchy_dict):\n",
    "    \"\"\"\n",
    "    Scans the current DataFrame columns and builds the kinematics map.\n",
    "    This ensures NB 06 and NB 08 only attempt calculations on valid joint-chains.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} BUILDING KINEMATICS MAP {'='*20}\")\n",
    "    \n",
    "    kinematics_map = {}\n",
    "    \n",
    "    # Identify joints that survived filtering in Cell 3\n",
    "    # We look for the base name before the '__px' suffix\n",
    "    existing_segments = set([c.split('__')[0] for c in df_columns if '__' in c])\n",
    "    \n",
    "    skipped_count = 0\n",
    "    kept_count = 0\n",
    "    \n",
    "    for segment, info in hierarchy_dict.items():\n",
    "        parent = info['parent']\n",
    "        angle_name = info['angle_name']\n",
    "        \n",
    "        # 1. Check if joint exists in current data\n",
    "        if segment not in existing_segments:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        # 2. BIOMECHANICAL INTEGRITY: Parent must exist for relative calculations\n",
    "        if parent is not None and parent not in existing_segments:\n",
    "            print(f\"‚ö†Ô∏è SCIENTIFIC WARNING: Orphaned Joint '{segment}'.\")\n",
    "            print(f\"   Cannot calculate '{angle_name}' because parent '{parent}' was filtered out.\")\n",
    "            continue\n",
    "\n",
    "        # 3. Validation Passed: Add to map for downstream Notebooks\n",
    "        kinematics_map[segment] = {\n",
    "            \"parent\": parent,\n",
    "            \"angle_name\": angle_name,\n",
    "            \"is_global\": (parent is None)\n",
    "        }\n",
    "        kept_count += 1\n",
    "\n",
    "    print(f\"Total defined in Schema: {len(hierarchy_dict)}\")\n",
    "    print(f\"Skipped (Missing/Filtered): {skipped_count}\")\n",
    "    print(f\"Mapped (Ready for Physics):  {kept_count}\")\n",
    "    print(f\"{'='*45}\\n\")\n",
    "    \n",
    "    return kinematics_map\n",
    "\n",
    "# --- EXECUTE ---\n",
    "# Note: Use df_preprocessed from Cell 3\n",
    "kinematics_map = build_map_from_available_joints(df_preprocessed.columns, SKELETON_HIERARCHY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57de5925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Scientific Gap Filling (Max Gap: 10 frames)...\n",
      "‚úÖ Gap Filling Complete.\n",
      "üìä Quality Control: Remaining NaNs (Critical Gaps): 0\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 05: Gap Filling and Rotational Re-normalization ---\n",
    "# RATIONALE: Biomechanical analysis requires continuous derivative signals (velocity/acceleration).\n",
    "# Small gaps (<100ms) are safely interpolated to prevent signal fragmentation.\n",
    "\n",
    "# Configuration: 10 frames at 120Hz = 83.3ms. \n",
    "# Scientific Limit: Skurowski (2021) suggests avoiding interpolation for gaps > 100ms in dynamic movement.\n",
    "MAX_GAP_SIZE = 10  \n",
    "\n",
    "def fill_missing_data(df, max_gap):\n",
    "    \"\"\"\n",
    "    1. Performs linear interpolation for small kinematic gaps.\n",
    "    2. Re-normalizes quaternions to maintain unit length (Rotational Integrity).\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Linear Interpolation\n",
    "    # Linear method is standard for positional data and small rotational increments.\n",
    "    df_clean = df_clean.interpolate(method='linear', limit=max_gap, limit_direction='both')\n",
    "    \n",
    "    # 2. Quaternion Re-normalization\n",
    "    # RATIONALE: Linear interpolation of quaternions (LERP) leads to non-unit vectors.\n",
    "    # Without re-normalization, angular velocity calculations in NB 06 will be distorted.\n",
    "    quat_cols = [c for c in df_clean.columns if c.endswith(('__qx', '__qy', '__qz', '__qw'))]\n",
    "    segments = set(c.split('__')[0] for c in quat_cols)\n",
    "    \n",
    "    for seg in segments:\n",
    "        try:\n",
    "            qx, qy = df_clean[f\"{seg}__qx\"], df_clean[f\"{seg}__qy\"]\n",
    "            qz, qw = df_clean[f\"{seg}__qz\"], df_clean[f\"{seg}__qw\"]\n",
    "            \n",
    "            norms = np.sqrt(qx**2 + qy**2 + qz**2 + qw**2)\n",
    "            norms[norms == 0] = 1.0  # Avoid division by zero\n",
    "            \n",
    "            df_clean[f\"{seg}__qx\"] /= norms\n",
    "            df_clean[f\"{seg}__qy\"] /= norms\n",
    "            df_clean[f\"{seg}__qz\"] /= norms\n",
    "            df_clean[f\"{seg}__qw\"] /= norms\n",
    "        except KeyError:\n",
    "            continue # Skip if a specific quaternion component is missing\n",
    "            \n",
    "    return df_clean\n",
    "\n",
    "print(f\"Running Scientific Gap Filling (Max Gap: {MAX_GAP_SIZE} frames)...\")\n",
    "# Note: df_preprocessed continues the chain from Cell 03 standard naming\n",
    "df_preprocessed = fill_missing_data(df_preprocessed, MAX_GAP_SIZE)\n",
    "\n",
    "# Check remaining NaNs (Signifies gaps larger than MAX_GAP_SIZE)\n",
    "remaining_nans = df_preprocessed.isna().sum().sum()\n",
    "print(f\"‚úÖ Gap Filling Complete.\")\n",
    "print(f\"üìä Quality Control: Remaining NaNs (Critical Gaps): {remaining_nans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2532259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== DATA RELIABILITY REPORT ====================\n",
      "Overall Dataset Integrity:\n",
      "- Original Missing Data: 0.0%\n",
      "- Interpolated Data:      0.0%\n",
      "- Remaining Unsolved Gaps: 0.0%\n",
      "\n",
      "Per-Joint Integrity Check (Top 5 Interpolated):\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 06: Missing Data Scientific Report ---\n",
    "\n",
    "def print_missing_data_report(df_raw, df_filled):\n",
    "    \"\"\"\n",
    "    Compares raw and filled data to provide a 'Data Reliability Score'.\n",
    "    Essential for Methodological Transparency (Winter, 2009).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} DATA RELIABILITY REPORT {'='*20}\")\n",
    "    \n",
    "    total_cells = df_raw.size\n",
    "    nans_before = df_raw.isna().sum().sum()\n",
    "    nans_after = df_filled.isna().sum().sum()\n",
    "    \n",
    "    interpolated_points = nans_before - nans_after\n",
    "    interpolation_ratio = (interpolated_points / total_cells) * 100\n",
    "    \n",
    "    print(f\"Overall Dataset Integrity:\")\n",
    "    print(f\"- Original Missing Data: {round((nans_before/total_cells)*100, 3)}%\")\n",
    "    print(f\"- Interpolated Data:      {round(interpolation_ratio, 3)}%\")\n",
    "    print(f\"- Remaining Unsolved Gaps: {round((nans_after/total_cells)*100, 3)}%\")\n",
    "    \n",
    "    # Per-Joint Reliability (Focus on Core Joints)\n",
    "    print(f\"\\nPer-Joint Integrity Check (Top 5 Interpolated):\")\n",
    "    diff = (df_raw.isna().sum() - df_filled.isna().sum()).sort_values(ascending=False)\n",
    "    for joint, count in diff.head(5).items():\n",
    "        if count > 0:\n",
    "            percentage = (count / len(df_raw)) * 100\n",
    "            print(f\"  * {joint.replace('__px', ''):<15} : {round(percentage, 2)}% Interpolated\")\n",
    "\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "# --- EXECUTE ---\n",
    "# We compare df_raw (pre-cleaning/filling) to our current df_preprocessed\n",
    "print_missing_data_report(df_raw, df_preprocessed)\n",
    "print(\"note: Original data = 0% - Optitrack confidence-based cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6239938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== BONE LENGTH QC (Scientific Validation) ====================\n",
      "Checked 20 bones.\n",
      "‚úÖ SUCCESS: Rigid body integrity confirmed. Ready for Kinematic Derivatives.\n",
      "\n",
      "========================= DETAILED SEGMENT ANALYSIS =========================\n",
      "Bone Segment                   | Mean (mm)  | CV (%)   | Status\n",
      "-----------------------------------------------------------------\n",
      "Hips->Spine                    | 80827.4    | 4.89     | WARN üü°   \n",
      "Neck->Head                     | 137793.9   | 2.37     | WARN üü°   \n",
      "Spine->Spine1                  | 213042.1   | 1.85     | PASS   \n",
      "Spine1->Neck                   | 220539.0   | 1.49     | PASS   \n",
      "Spine1->LeftShoulder           | 174661.6   | 0.0      | PASS ‚≠ê\n",
      "RightArm->RightForeArm         | 251313.9   | 0.0      | PASS ‚≠ê\n",
      "RightShoulder->RightArm        | 160475.6   | 0.0      | PASS ‚≠ê\n",
      "Spine1->RightShoulder          | 174661.7   | 0.0      | PASS ‚≠ê\n",
      "LeftForeArm->LeftHand          | 233159.2   | 0.0      | PASS ‚≠ê\n",
      "LeftArm->LeftForeArm           | 251313.9   | 0.0      | PASS ‚≠ê\n",
      "LeftShoulder->LeftArm          | 160475.6   | 0.0      | PASS ‚≠ê\n",
      "RightLeg->RightFoot            | 389935.6   | 0.0      | PASS ‚≠ê\n",
      "RightFoot->RightToeBase        | 148857.8   | 0.0      | PASS ‚≠ê\n",
      "RightUpLeg->RightLeg           | 422341.9   | 0.0      | PASS ‚≠ê\n",
      "Hips->RightUpLeg               | 91056.8    | 0.0      | PASS ‚≠ê\n",
      "LeftFoot->LeftToeBase          | 148857.7   | 0.0      | PASS ‚≠ê\n",
      "LeftLeg->LeftFoot              | 389935.6   | 0.0      | PASS ‚≠ê\n",
      "LeftUpLeg->LeftLeg             | 422341.9   | 0.0      | PASS ‚≠ê\n",
      "Hips->LeftUpLeg                | 91056.8    | 0.0      | PASS ‚≠ê\n",
      "RightForeArm->RightHand        | 233159.2   | 0.0      | PASS ‚≠ê\n",
      "-----------------------------------------------------------------\n",
      "üìä SCIENTIFIC SUMMARY: Mean Segment CV across all bones: 0.53%\n",
      "RATIONALE: A mean CV below 2% indicates high-fidelity tracking (Skurowski, 2021).\n"
     ]
    }
   ],
   "source": [
    "# --- QC Stage: Bone Length Check (Fail Fast) ---\n",
    "# SCIENTIFIC RATIONALE: Skurowski (2021) identifies bone length consistency \n",
    "# as the primary metric for motion capture data quality. High CV% indicates \n",
    "# marker occlusion or reconstruction artifacts.\n",
    "\n",
    "def run_bone_length_qc(df, hierarchy, cfg):\n",
    "    \"\"\"\n",
    "    Quality Gate: Validates the Rigid Body Assumption.\n",
    "    If SUBJECT_HEIGHT is missing, these mean lengths serve as the \n",
    "    internal reference for scaling (Hof, 1996).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} BONE LENGTH QC (Scientific Validation) {'='*20}\")\n",
    "    \n",
    "    # Thresholds: 2% for Warning, 5% for Critical failure (Skurowski standard)\n",
    "    thresh_warn = cfg['THRESH'].get('BONE_CV_WARN', 0.02)   \n",
    "    thresh_alert = cfg['THRESH'].get('BONE_CV_ALERT', 0.05) \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for child_name, info in hierarchy.items():\n",
    "        parent_name = info['parent']\n",
    "        if parent_name is None: continue # Skip Root\n",
    "            \n",
    "        try:\n",
    "            # NB02 standard naming convention: Joint__px\n",
    "            c_pos = df[[f\"{child_name}__px\", f\"{child_name}__py\", f\"{child_name}__pz\"]].values\n",
    "            p_pos = df[[f\"{parent_name}__px\", f\"{parent_name}__py\", f\"{parent_name}__pz\"]].values\n",
    "            \n",
    "            # Distance calculation\n",
    "            lengths = np.linalg.norm(c_pos - p_pos, axis=1)\n",
    "            mean_l = np.nanmean(lengths)\n",
    "            std_l = np.nanstd(lengths)\n",
    "            cv = std_l / mean_l if mean_l > 0 else 0.0\n",
    "            \n",
    "            status = \"PASS\"\n",
    "            if cv > thresh_alert: status = \"FAIL üî¥\"\n",
    "            elif cv > thresh_warn: status = \"WARN üü°\"\n",
    "            \n",
    "            results.append({\n",
    "                \"Bone\": f\"{parent_name}->{child_name}\",\n",
    "                \"Mean_mm\": round(mean_l * 1000, 1), # Conversion to mm for biomechanical clarity\n",
    "                \"CV%\": round(cv * 100, 2),\n",
    "                \"Status\": status\n",
    "            })\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    df_qc = pd.DataFrame(results).sort_values(\"CV%\", ascending=False)\n",
    "    \n",
    "    # Summary reporting\n",
    "    n_fails = sum(df_qc['Status'].str.contains(\"FAIL\"))\n",
    "    print(f\"Checked {len(df_qc)} bones.\")\n",
    "    \n",
    "    if n_fails > 0:\n",
    "        print(f\"‚õî SCIENTIFIC ALERT: {n_fails} bones exceed the 5% instability threshold.\")\n",
    "    else:\n",
    "        print(\"‚úÖ SUCCESS: Rigid body integrity confirmed. Ready for Kinematic Derivatives.\")\n",
    "        \n",
    "    return df_qc\n",
    "\n",
    "# --- EXECUTE ---\n",
    "# --- UPDATED EXECUTION FOR BONE QC ---\n",
    "# We force display of the results for scientific reporting purposes.\n",
    "\n",
    "# 1. Run the QC\n",
    "df_bone_qc = run_bone_length_qc(df_preprocessed, kinematics_map, CONFIG)\n",
    "\n",
    "# 2. Detailed Print (For the 'Methods' section of your research)\n",
    "print(f\"\\n{'='*25} DETAILED SEGMENT ANALYSIS {'='*25}\")\n",
    "print(f\"{'Bone Segment':<30} | {'Mean (mm)':<10} | {'CV (%)':<8} | {'Status'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for _, row in df_bone_qc.iterrows():\n",
    "    # Adding a visual marker for very high precision (CV < 1%)\n",
    "    precision_star = \"‚≠ê\" if row['CV%'] < 1.0 else \"  \"\n",
    "    print(f\"{row['Bone']:<30} | {row['Mean_mm']:<10} | {row['CV%']:<8} | {row['Status']} {precision_star}\")\n",
    "\n",
    "# 3. Scientific Metadata for Master Report\n",
    "mean_overall_cv = df_bone_qc['CV%'].mean()\n",
    "print(\"-\" * 65)\n",
    "print(f\"üìä SCIENTIFIC SUMMARY: Mean Segment CV across all bones: {mean_overall_cv:.2f}%\")\n",
    "print(f\"RATIONALE: A mean CV below 2% indicates high-fidelity tracking (Skurowski, 2021).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cbd7ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ PERSISTENCE SUCCESS!\n",
      "üìä Kinematic Data: /Users/drorhazan/Documents/untitled folder/Gaga-mocap-Kinematics/derivatives/step_02_preprocess/734_T1_P1_R1_Take 2025-12-01 02.18.27 PM__preprocessed.parquet\n",
      "üß¨ Kinematics Map: /Users/drorhazan/Documents/untitled folder/Gaga-mocap-Kinematics/derivatives/step_02_preprocess/734_T1_P1_R1_Take 2025-12-01 02.18.27 PM__kinematics_map.json\n",
      "\n",
      "Proceeding to Notebook 03 (Resample).\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 08: Scientific Data Persistence ---\n",
    "# RATIONALE: Using Parquet format preserves double-precision accuracy, \n",
    "# which is critical for reducing noise in kinematic derivatives (NB 04/06).\n",
    "# The Kinematics Map acts as the 'Scientific Contract' between pipeline stages.\n",
    "\n",
    "# 1. Save Processed Data (High-Precision Parquet)\n",
    "out_parquet_path = os.path.join(DERIV_02, f\"{RUN_ID}__preprocessed.parquet\")\n",
    "df_preprocessed.to_parquet(out_parquet_path, index=False)\n",
    "\n",
    "# 2. Save Kinematics Map (JSON)\n",
    "# RATIONALE: This ensures that NB 06 (Rotation) and NB 08 (CoM) use \n",
    "# the exact same skeletal hierarchy validated in this notebook.\n",
    "out_map_path = os.path.join(DERIV_02, f\"{RUN_ID}__kinematics_map.json\")\n",
    "with open(out_map_path, 'w') as f:\n",
    "    json.dump(kinematics_map, f, indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ PERSISTENCE SUCCESS!\")\n",
    "print(f\"üìä Kinematic Data: {out_parquet_path}\")\n",
    "print(f\"üß¨ Kinematics Map: {out_map_path}\")\n",
    "print(\"\\nProceeding to Notebook 03 (Resample).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e2c5563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== PREPROCESS SUMMARY EXPORTED ====================\n",
      "‚úÖ Summary Path: /Users/drorhazan/Documents/untitled folder/Gaga-mocap-Kinematics/derivatives/step_02_preprocess/734_T1_P1_R1_Take 2025-12-01 02.18.27 PM__preprocess_summary.json\n",
      "üìä Bone QC Mean CV: 0.53% (Status: GOLD)\n",
      "üìâ Missing Data: 0.0% -> 0.0%\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 09: FINAL CELL - Export Preprocessing Summary for Master Report ---\n",
    "\n",
    "def export_preprocess_summary(df_pre, df_post, df_bone_qc, run_id, save_dir, cfg):\n",
    "    \"\"\"\n",
    "    Creates a comprehensive QC JSON report. \n",
    "    Essential for Methodological Traceability (Winter, 2009).\n",
    "    \"\"\"\n",
    "    total_cells = df_pre.size\n",
    "    total_nans_pre = df_pre.isna().sum().sum()\n",
    "    total_nans_post = df_post.isna().sum().sum()\n",
    "    \n",
    "    # Bone QC Metrics (Scientific Integrity)\n",
    "    mean_cv = df_bone_qc['CV%'].mean() if not df_bone_qc.empty else 100.0\n",
    "    # Capture bones that exceeded the safety threshold\n",
    "    alerts = df_bone_qc[df_bone_qc['Status'].str.contains(\"FAIL|WARN\")]['Bone'].tolist()\n",
    "    \n",
    "    summary = {\n",
    "        \"run_id\": run_id,\n",
    "        \"raw_missing_percent\": round((total_nans_pre / total_cells) * 100, 3),\n",
    "        \"post_missing_percent\": round((total_nans_post / total_cells) * 100, 3),\n",
    "        \"max_interpolation_gap\": cfg.get('MAX_GAP_SIZE', 10),\n",
    "        \"bone_qc_mean_cv\": round(mean_cv, 3),\n",
    "        \"bone_qc_status\": \"GOLD\" if mean_cv < 1.0 else \"SILVER\" if mean_cv < 5.0 else \"REJECT\",\n",
    "        \"bone_qc_alerts\": alerts,\n",
    "        \"worst_bone\": df_bone_qc.iloc[0]['Bone'] if not df_bone_qc.empty else \"None\",\n",
    "        \"interpolation_method\": \"linear_quaternion_normalized\"\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    out_path = os.path.join(save_dir, f\"{run_id}__preprocess_summary.json\")\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "    \n",
    "    print(f\"\\n{'='*20} PREPROCESS SUMMARY EXPORTED {'='*20}\")\n",
    "    print(f\"‚úÖ Summary Path: {out_path}\")\n",
    "    print(f\"üìä Bone QC Mean CV: {summary['bone_qc_mean_cv']}% (Status: {summary['bone_qc_status']})\")\n",
    "    print(f\"üìâ Missing Data: {summary['raw_missing_percent']}% -> {summary['post_missing_percent']}%\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "# --- EXECUTE ---\n",
    "export_preprocess_summary(df_raw, df_preprocessed, df_bone_qc, RUN_ID, DERIV_02, CONFIG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
