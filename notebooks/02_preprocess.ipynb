{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c281bb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready. Output directory: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\derivatives\\step_02_preprocess\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Path Setup ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "\n",
    "# --- Imports from src ---\n",
    "from config import CONFIG\n",
    "from skeleton_defs import SKELETON_HIERARCHY  # The file we created\n",
    "# (Optional: Import interpolation utils if you have them in src)\n",
    "\n",
    "# --- Directories ---\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "DERIV_01 = os.path.join(PROJECT_ROOT, CONFIG['derivatives_dir'], \"step_01_parse\")\n",
    "DERIV_02 = os.path.join(PROJECT_ROOT, CONFIG['derivatives_dir'], \"step_02_preprocess\")\n",
    "QC_02 = os.path.join(PROJECT_ROOT, CONFIG['qc_dir'], \"step_02_preprocess\")\n",
    "\n",
    "os.makedirs(DERIV_02, exist_ok=True)\n",
    "os.makedirs(QC_02, exist_ok=True)\n",
    "\n",
    "print(\"Ready. Output directory:\", DERIV_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7229b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Run ID: 734_T1_P2_R1_Take 2025-12-01 02.28.24 PM\n",
      "File: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\derivatives\\step_01_parse\\734_T1_P2_R1_Take 2025-12-01 02.28.24 PM__parsed_run.parquet\n",
      "Shape: (19617, 359)\n"
     ]
    }
   ],
   "source": [
    "# Derive the parquet file from the CSV path in config\n",
    "# This ensures notebook 02 always processes the same file as notebook 01\n",
    "csv_filename = Path(CONFIG['current_csv']).stem  # Gets filename without extension\n",
    "RUN_ID = csv_filename\n",
    "PARQUET_PATH = Path(DERIV_01) / f\"{RUN_ID}__parsed_run.parquet\"\n",
    "\n",
    "if not PARQUET_PATH.exists():\n",
    "    print(f\"âŒ ERROR: Expected parquet file not found: {PARQUET_PATH}\")\n",
    "    print(f\"Did you run notebook 01 first?\")\n",
    "    raise FileNotFoundError(f\"Parquet file not found: {PARQUET_PATH}\")\n",
    "\n",
    "print(f\"Loading Run ID: {RUN_ID}\")\n",
    "print(f\"File: {PARQUET_PATH}\")\n",
    "\n",
    "df_raw = pd.read_parquet(PARQUET_PATH)\n",
    "print(f\"Shape: {df_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "524fab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== JOINT FILTERING & VERIFICATION ====================\n",
      "ğŸ“‰ Dropped 210 columns matching: ['Thumb', 'Index', 'Middle', 'Ring', 'Pinky']\n",
      "\n",
      "ğŸ“Š STATUS REPORT:\n",
      "   Original Columns: 359\n",
      "   Current Columns:  149\n",
      "   Unique Joints Remaining: 21\n",
      "\n",
      "âœ… LIST OF JOINTS KEPT (21):\n",
      "----------------------------------------\n",
      "   Head, Hips, LeftArm, LeftFoot\n",
      "   LeftForeArm, LeftHand, LeftLeg, LeftShoulder\n",
      "   LeftToeBase, LeftUpLeg, Neck, RightArm\n",
      "   RightFoot, RightForeArm, RightHand, RightLeg\n",
      "   RightShoulder, RightToeBase, RightUpLeg, Spine\n",
      "   Spine1\n",
      "----------------------------------------\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Filter Out Fingers/Toes & Print Remaining Joints ---\n",
    "\n",
    "def filter_and_verify_joints(df, keywords_to_drop):\n",
    "    \"\"\"\n",
    "    1. Removes columns containing specific keywords (Fingers, Toes).\n",
    "    2. Prints a clean list of the remaining joints for verification.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} JOINT FILTERING & VERIFICATION {'='*20}\")\n",
    "    \n",
    "    # --- 1. Filter ---\n",
    "    initial_cols = len(df.columns)\n",
    "    cols_to_drop = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if any(kw in col for kw in keywords_to_drop):\n",
    "            cols_to_drop.append(col)\n",
    "            \n",
    "    if cols_to_drop:\n",
    "        df_clean = df.drop(columns=cols_to_drop)\n",
    "        dropped_count = len(cols_to_drop)\n",
    "        print(f\"ğŸ“‰ Dropped {dropped_count} columns matching: {keywords_to_drop}\")\n",
    "    else:\n",
    "        df_clean = df\n",
    "        print(\"â„¹ï¸ No columns matched the filter.\")\n",
    "\n",
    "    # --- 2. Extract Remaining Joint Names ---\n",
    "    # We look at the prefix before '__' (e.g., 'Hips__px' -> 'Hips')\n",
    "    remaining_joints = sorted(list(set([c.split('__')[0] for c in df_clean.columns if '__' in c])))\n",
    "    \n",
    "    # --- 3. Print Report ---\n",
    "    print(f\"\\nğŸ“Š STATUS REPORT:\")\n",
    "    print(f\"   Original Columns: {initial_cols}\")\n",
    "    print(f\"   Current Columns:  {len(df_clean.columns)}\")\n",
    "    print(f\"   Unique Joints Remaining: {len(remaining_joints)}\")\n",
    "    \n",
    "    print(f\"\\nâœ… LIST OF JOINTS KEPT ({len(remaining_joints)}):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Print in nice columns or list\n",
    "    # Printing 4 joints per row for readability\n",
    "    for i in range(0, len(remaining_joints), 4):\n",
    "        print(f\"   {', '.join(remaining_joints[i:i+4])}\")\n",
    "        \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# ××™×œ×™× ×©×× ×”×Ÿ ××•×¤×™×¢×•×ª ×‘×©× ×”××¤×¨×§ - ×”×•× ×™×™××—×§.\n",
    "# ×©×™× ×œ×‘: \"Toe\" ××•×—×§ ×’× ××ª \"ToeBase\". ×× ××ª×” ×¨×•×¦×” ××ª ToeBase, ×ª×•×¨×™×“ ××ª \"Toe\" ××”×¨×©×™××”.\n",
    "DROP_KEYWORDS = [\"Thumb\", \"Index\", \"Middle\", \"Ring\", \"Pinky\"] \n",
    "# ×”×•×¡×£ ××ª \"Toe\" ×œ×¨×©×™××” ×œ××¢×œ×” ×× ××ª×” ×¨×•×¦×” ×œ×”×¢×™×£ ×’× ×‘×”×•× ×•×ª\n",
    "\n",
    "# --- EXECUTE ---\n",
    "df_raw = filter_and_verify_joints(df_raw, DROP_KEYWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "051973cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== BUILDING KINEMATICS MAP ====================\n",
      "Total defined in Schema: 27\n",
      "Skipped (Filtered out):  6\n",
      "Mapped (Ready for calc): 21\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- UPDATED CELL 3: Build Map form Remaining Joints ---\n",
    "\n",
    "def build_map_from_available_joints(df_columns, hierarchy_dict):\n",
    "    \"\"\"\n",
    "    Scans the current DataFrame columns and builds the kinematics map \n",
    "    ONLY for the joints that actually exist in the data.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} BUILDING KINEMATICS MAP {'='*20}\")\n",
    "    \n",
    "    kinematics_map = {}\n",
    "    \n",
    "    \n",
    "    existing_segments = set([c.split('__')[0] for c in df_columns if '__' in c])\n",
    "    \n",
    "    skipped_count = 0\n",
    "    kept_count = 0\n",
    "    \n",
    "    for segment, info in hierarchy_dict.items():\n",
    "        parent = info['parent']\n",
    "        angle_name = info['angle_name']\n",
    "        \n",
    "        \n",
    "        if segment not in existing_segments:\n",
    "            \n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        if parent is not None and parent not in existing_segments:\n",
    "            print(f\"âš ï¸ WARNING: Orphaned Joint '{segment}' (Parent '{parent}' is missing). Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 3. ×”×•×¡×¤×” ×œ××¤×”\n",
    "        kinematics_map[segment] = {\n",
    "            \"parent\": parent,\n",
    "            \"angle_name\": angle_name,\n",
    "            \"is_global\": (parent is None)\n",
    "        }\n",
    "        kept_count += 1\n",
    "\n",
    "    print(f\"Total defined in Schema: {len(hierarchy_dict)}\")\n",
    "    print(f\"Skipped (Filtered out):  {skipped_count}\")\n",
    "    print(f\"Mapped (Ready for calc): {kept_count}\")\n",
    "    print(f\"{'='*45}\\n\")\n",
    "    \n",
    "    return kinematics_map\n",
    "\n",
    "# --- EXECUTE ---\n",
    "kinematics_map = build_map_from_available_joints(df_raw.columns, SKELETON_HIERARCHY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57de5925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Gap Filling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gap Filling Complete.\n",
      "Remaining NaNs (Large Gaps): 0\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MAX_GAP_SIZE = 10  # Max frames to interpolate (approx 83ms at 120Hz)\n",
    "\n",
    "def fill_missing_data(df, max_gap):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Linear Interpolate (Works well for Pos and small Rot gaps)\n",
    "    # We interpolate everything first\n",
    "    df_clean = df_clean.interpolate(method='linear', limit=max_gap, limit_direction='both')\n",
    "    \n",
    "    # 2. Normalize Quaternions (Critical after Lerp!)\n",
    "    # Iterate only over rotation columns to re-normalize\n",
    "    quat_cols = [c for c in df_clean.columns if c.endswith(('__qx', '__qy', '__qz', '__qw'))]\n",
    "    segments = set(c.split('__')[0] for c in quat_cols)\n",
    "    \n",
    "    for seg in segments:\n",
    "        qx = df_clean[f\"{seg}__qx\"]\n",
    "        qy = df_clean[f\"{seg}__qy\"]\n",
    "        qz = df_clean[f\"{seg}__qz\"]\n",
    "        qw = df_clean[f\"{seg}__qw\"]\n",
    "        \n",
    "        # Calculate norm\n",
    "        norms = np.sqrt(qx**2 + qy**2 + qz**2 + qw**2)\n",
    "        \n",
    "        # Avoid division by zero (though interpolate handles NaNs)\n",
    "        norms[norms == 0] = 1.0 \n",
    "        \n",
    "        # Normalize\n",
    "        df_clean[f\"{seg}__qx\"] /= norms\n",
    "        df_clean[f\"{seg}__qy\"] /= norms\n",
    "        df_clean[f\"{seg}__qz\"] /= norms\n",
    "        df_clean[f\"{seg}__qw\"] /= norms\n",
    "        \n",
    "    return df_clean\n",
    "\n",
    "print(\"Running Gap Filling...\")\n",
    "df_preprocessed = fill_missing_data(df_raw, MAX_GAP_SIZE)\n",
    "\n",
    "# Check remaining NaNs (Large gaps)\n",
    "remaining_nans = df_preprocessed.isna().sum().sum()\n",
    "print(f\"Gap Filling Complete.\")\n",
    "print(f\"Remaining NaNs (Large Gaps): {remaining_nans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2532259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== QC REPORT: MISSING DATA ====================\n",
      "Run ID:       734_T1_P2_R1_Take 2025-12-01 02.28.24 PM\n",
      "Total Frames: 19617\n",
      "Total Values: 2922933\n",
      "Total NaNs:   0\n",
      "Cleanliness:  100.0000% (Target: 100%)\n",
      "\n",
      "âœ… PERFECT: No missing data detected.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5 Replacement: Statistical QC Report ---\n",
    "\n",
    "def print_missing_data_report(df, run_id):\n",
    "    total_cells = df.size\n",
    "    total_nans = df.isna().sum().sum()\n",
    "    nan_percent = (total_nans / total_cells) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*20} QC REPORT: MISSING DATA {'='*20}\")\n",
    "    print(f\"Run ID:       {run_id}\")\n",
    "    print(f\"Total Frames: {len(df)}\")\n",
    "    print(f\"Total Values: {total_cells}\")\n",
    "    print(f\"Total NaNs:   {total_nans}\")\n",
    "    print(f\"Cleanliness:  {100 - nan_percent:.4f}% (Target: 100%)\")\n",
    "    \n",
    "    if total_nans == 0:\n",
    "        print(\"\\nâœ… PERFECT: No missing data detected.\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  WARNING: Data Gaps Detected!\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Column Name':<35} | {'Missing Frames':<15} | {'% Missing':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Find columns with NaNs and sort by severity\n",
    "        missing_counts = df.isna().sum()\n",
    "        missing_cols = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "        \n",
    "        for col, count in missing_cols.items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"{col:<35} | {count:<15} | {pct:.2f}%\")\n",
    "            \n",
    "        print(\"-\" * 60)\n",
    "        print(\"Note: Gaps larger than the interpolation limit remain as NaNs.\")\n",
    "\n",
    "# Run the report\n",
    "print_missing_data_report(df_preprocessed, RUN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6239938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== BONE LENGTH QC ====================\n",
      "Checked 20 bones.\n",
      "âš ï¸ WARNING: 1 bones show slight instability.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bone</th>\n",
       "      <th>Mean_mm</th>\n",
       "      <th>CV%</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hips-&gt;Spine</td>\n",
       "      <td>81.4</td>\n",
       "      <td>3.41</td>\n",
       "      <td>WARN ğŸŸ¡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neck-&gt;Head</td>\n",
       "      <td>145.1</td>\n",
       "      <td>1.93</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spine-&gt;Spine1</td>\n",
       "      <td>244.2</td>\n",
       "      <td>1.14</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spine1-&gt;Neck</td>\n",
       "      <td>256.7</td>\n",
       "      <td>1.10</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Spine1-&gt;LeftShoulder</td>\n",
       "      <td>198.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RightArm-&gt;RightForeArm</td>\n",
       "      <td>325.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RightShoulder-&gt;RightArm</td>\n",
       "      <td>157.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Spine1-&gt;RightShoulder</td>\n",
       "      <td>198.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LeftForeArm-&gt;LeftHand</td>\n",
       "      <td>190.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LeftArm-&gt;LeftForeArm</td>\n",
       "      <td>325.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LeftShoulder-&gt;LeftArm</td>\n",
       "      <td>157.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RightLeg-&gt;RightFoot</td>\n",
       "      <td>379.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RightFoot-&gt;RightToeBase</td>\n",
       "      <td>154.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RightUpLeg-&gt;RightLeg</td>\n",
       "      <td>402.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hips-&gt;RightUpLeg</td>\n",
       "      <td>94.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LeftFoot-&gt;LeftToeBase</td>\n",
       "      <td>154.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LeftLeg-&gt;LeftFoot</td>\n",
       "      <td>379.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LeftUpLeg-&gt;LeftLeg</td>\n",
       "      <td>402.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hips-&gt;LeftUpLeg</td>\n",
       "      <td>94.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RightForeArm-&gt;RightHand</td>\n",
       "      <td>190.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Bone  Mean_mm   CV%  Status\n",
       "0               Hips->Spine     81.4  3.41  WARN ğŸŸ¡\n",
       "3                Neck->Head    145.1  1.93    PASS\n",
       "1             Spine->Spine1    244.2  1.14    PASS\n",
       "2              Spine1->Neck    256.7  1.10    PASS\n",
       "12     Spine1->LeftShoulder    198.8  0.00    PASS\n",
       "18   RightArm->RightForeArm    325.2  0.00    PASS\n",
       "17  RightShoulder->RightArm    157.9  0.00    PASS\n",
       "16    Spine1->RightShoulder    198.8  0.00    PASS\n",
       "15    LeftForeArm->LeftHand    190.8  0.00    PASS\n",
       "14     LeftArm->LeftForeArm    325.2  0.00    PASS\n",
       "13    LeftShoulder->LeftArm    157.9  0.00    PASS\n",
       "10      RightLeg->RightFoot    379.4  0.00    PASS\n",
       "11  RightFoot->RightToeBase    154.2  0.00    PASS\n",
       "9      RightUpLeg->RightLeg    402.3  0.00    PASS\n",
       "8          Hips->RightUpLeg     94.3  0.00    PASS\n",
       "7     LeftFoot->LeftToeBase    154.2  0.00    PASS\n",
       "6         LeftLeg->LeftFoot    379.4  0.00    PASS\n",
       "5        LeftUpLeg->LeftLeg    402.3  0.00    PASS\n",
       "4           Hips->LeftUpLeg     94.3  0.00    PASS\n",
       "19  RightForeArm->RightHand    190.8  0.00    PASS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- QC Stage: Bone Length Check (Fail Fast) ---\n",
    "\n",
    "def run_bone_length_qc(df, hierarchy, cfg):\n",
    "    \"\"\"\n",
    "    Quality Gate: Checks if limbs stretch/shrink over time.\n",
    "    Since we assume a Rigid Body, distance between Parent->Child must be constant.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} BONE LENGTH QC {'='*20}\")\n",
    "    \n",
    "    # Thresholds from config\n",
    "    thresh_warn = cfg['THRESH'].get('BONE_CV_WARN', 0.02)   # Default 2%\n",
    "    thresh_alert = cfg['THRESH'].get('BONE_CV_ALERT', 0.05) # Default 5%\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Calculate for each defined bone in hierarchy\n",
    "    for child_name, info in hierarchy.items():\n",
    "        parent_name = info['parent']\n",
    "        \n",
    "        # Skip root (no parent)\n",
    "        if parent_name is None:\n",
    "            continue\n",
    "            \n",
    "        # Get columns from the processed dataframe\n",
    "        # Note: In NB02 columns are named \"Joint__px\", \"Joint__py\", etc.\n",
    "        try:\n",
    "            c_pos = df[[f\"{child_name}__px\", f\"{child_name}__py\", f\"{child_name}__pz\"]].values\n",
    "            p_pos = df[[f\"{parent_name}__px\", f\"{parent_name}__py\", f\"{parent_name}__pz\"]].values\n",
    "        except KeyError:\n",
    "            # If joint was excluded or missing\n",
    "            continue\n",
    "        \n",
    "        # Euclidean Distance\n",
    "        lengths = np.linalg.norm(c_pos - p_pos, axis=1)\n",
    "        \n",
    "        # Stats\n",
    "        mean_l = np.nanmean(lengths)\n",
    "        std_l = np.nanstd(lengths)\n",
    "        \n",
    "        # Coefficient of Variation (CV)\n",
    "        cv = std_l / mean_l if mean_l > 0 else 0.0\n",
    "        \n",
    "        status = \"PASS\"\n",
    "        if cv > thresh_alert:\n",
    "            status = \"FAIL ğŸ”´\"\n",
    "        elif cv > thresh_warn:\n",
    "            status = \"WARN ğŸŸ¡\"\n",
    "            \n",
    "        results.append({\n",
    "            \"Bone\": f\"{parent_name}->{child_name}\",\n",
    "            \"Mean_mm\": round(mean_l, 1),\n",
    "            \"CV%\": round(cv * 100, 2),\n",
    "            \"Status\": status\n",
    "        })\n",
    "\n",
    "    # Report\n",
    "    df_qc = pd.DataFrame(results).sort_values(\"CV%\", ascending=False)\n",
    "    \n",
    "    n_fails = sum(df_qc['Status'].str.contains(\"FAIL\"))\n",
    "    n_warns = sum(df_qc['Status'].str.contains(\"WARN\"))\n",
    "    \n",
    "    print(f\"Checked {len(df_qc)} bones.\")\n",
    "    if n_fails > 0:\n",
    "        print(f\"â›” CRITICAL: {n_fails} bones failed rigidity check!\")\n",
    "        print(\"Top offenders:\")\n",
    "        display(df_qc.head(51))\n",
    "        # Optional: Raise error to stop pipeline if strictly required\n",
    "        # raise ValueError(\"Data quality too low: Bones are stretching.\")\n",
    "    elif n_warns > 0:\n",
    "        print(f\"âš ï¸ WARNING: {n_warns} bones show slight instability.\")\n",
    "        display(df_qc.head(51))\n",
    "    else:\n",
    "        print(\"âœ… SUCCESS: Rigid body assumption holds perfectly.\")\n",
    "        \n",
    "    return df_qc\n",
    "\n",
    "# Execute on the processed dataframe (df_proc) using the map we built earlier (kinematics_map)\n",
    "df_bone_qc = run_bone_length_qc(df_preprocessed, kinematics_map, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cbd7ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… SUCCESS!\n",
      "Data saved to: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\derivatives\\step_02_preprocess\\734_T1_P2_R1_Take 2025-12-01 02.28.24 PM__preprocessed.parquet\n",
      "Map saved to:  c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\derivatives\\step_02_preprocess\\734_T1_P2_R1_Take 2025-12-01 02.28.24 PM__kinematics_map.json\n",
      "Proceed to Notebook 03 (Resample).\n"
     ]
    }
   ],
   "source": [
    "# 1. Save Data (Parquet)\n",
    "out_parquet_path = os.path.join(DERIV_02, f\"{RUN_ID}__preprocessed.parquet\")\n",
    "df_preprocessed.to_parquet(out_parquet_path, index=False)\n",
    "\n",
    "# 2. Save Kinematics Map (JSON)\n",
    "# This map tells the next notebook WHAT angles to compute\n",
    "out_map_path = os.path.join(DERIV_02, f\"{RUN_ID}__kinematics_map.json\")\n",
    "with open(out_map_path, 'w') as f:\n",
    "    json.dump(kinematics_map, f, indent=4)\n",
    "\n",
    "print(f\"\\nâœ… SUCCESS!\")\n",
    "print(f\"Data saved to: {out_parquet_path}\")\n",
    "print(f\"Map saved to:  {out_map_path}\")\n",
    "print(\"Proceed to Notebook 03 (Resample).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e2c5563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocess Summary exported to: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\derivatives\\step_02_preprocess\\734_T1_P2_R1_Take 2025-12-01 02.28.24 PM__preprocess_summary.json\n"
     ]
    }
   ],
   "source": [
    "# --- FINAL CELL: Export Preprocessing Summary for Master Report ---\n",
    "\n",
    "def export_preprocess_summary(df_raw, df_proc, df_bone_qc, run_id, save_dir, cfg):\n",
    "    # 1. ×—×™×©×•×‘ × ×ª×•× ×™ ×¤×¢×¨×™× (Gaps)\n",
    "    total_cells = df_raw.size\n",
    "    total_nans_pre = df_raw.isna().sum().sum()\n",
    "    total_nans_post = df_proc.isna().sum().sum()\n",
    "    \n",
    "    # 2. ×¡×™×›×•× Bone QC\n",
    "    mean_cv = df_bone_qc['CV%'].mean() if not df_bone_qc.empty else 0\n",
    "    alerts = len(df_bone_qc[df_bone_qc['Status'].str.contains(\"FAIL|ALERT\")])\n",
    "    \n",
    "    summary = {\n",
    "        \"run_id\": run_id,\n",
    "        \"raw_missing_percent\": round((total_nans_pre / total_cells) * 100, 2),\n",
    "        \"post_missing_percent\": round((total_nans_post / total_cells) * 100, 2),\n",
    "        \"max_interpolation_gap\": cfg.get('MAX_GAP_SIZE', 10),\n",
    "        \"bone_qc_mean_cv\": round(mean_cv, 3),\n",
    "        \"bone_qc_alerts\": alerts,\n",
    "        \"worst_bone\": df_bone_qc.iloc[0]['Bone'] if not df_bone_qc.empty else \"None\",\n",
    "        \"interpolation_method\": \"linear\" # ××• ××” ×©×”×’×“×¨×ª\n",
    "    }\n",
    "    \n",
    "    # ×©××™×¨×” ×œ-JSON\n",
    "    out_path = os.path.join(save_dir, f\"{run_id}__preprocess_summary.json\")\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "    \n",
    "    print(f\"âœ… Preprocess Summary exported to: {out_path}\")\n",
    "\n",
    "# ×”×¤×¢×œ×”\n",
    "export_preprocess_summary(df_raw, df_preprocessed, df_bone_qc, RUN_ID, DERIV_02, CONFIG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
