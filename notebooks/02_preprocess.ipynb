{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c281bb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Subject info found but incomplete. Using Internal Normalization.\n",
      "Ready. Mode: Normalization (Unit Mass)\n",
      "Output directory: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\derivatives\\step_02_preprocess\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Path Setup ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "\n",
    "# --- Imports from src ---\n",
    "from config import CONFIG\n",
    "from skeleton_defs import SKELETON_HIERARCHY \n",
    "\n",
    "# --- External Metadata Integration (Subject Data) ---\n",
    "# Path to the external JSON file containing anthropometric data\n",
    "METADATA_PATH = os.path.join(PROJECT_ROOT, \"data\", \"subject_metadata.json\")\n",
    "\n",
    "def load_subject_metadata(path):\n",
    "    \"\"\"\n",
    "    Loads subject-specific data (weight, height) from a JSON file.\n",
    "    If file is missing or values are null, the pipeline switches to \n",
    "    'Relative Normalization Mode' based on Hof (1996).\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "# Global Subject Variables\n",
    "metadata = load_subject_metadata(METADATA_PATH)\n",
    "SUBJECT_WEIGHT = None\n",
    "SUBJECT_HEIGHT = None\n",
    "PIPELINE_MODE = \"Normalization (Unit Mass)\"\n",
    "\n",
    "if metadata and \"subject_info\" in metadata:\n",
    "    info = metadata[\"subject_info\"]\n",
    "    SUBJECT_WEIGHT = info.get(\"weight_kg\")\n",
    "    SUBJECT_HEIGHT = info.get(\"height_cm\")\n",
    "    \n",
    "    if SUBJECT_WEIGHT and SUBJECT_HEIGHT:\n",
    "        PIPELINE_MODE = \"Scientific (Anthropometric)\"\n",
    "        print(f\"âœ… Scientific Mode: Using Winter (2009) coefficients for {SUBJECT_WEIGHT}kg\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Subject info found but incomplete. Using Internal Normalization.\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No metadata file found. Defaulting to Relative Normalization Mode.\")\n",
    "\n",
    "# --- Directories ---\n",
    "DERIV_01 = os.path.join(PROJECT_ROOT, CONFIG['derivatives_dir'], \"step_01_parse\")\n",
    "DERIV_02 = os.path.join(PROJECT_ROOT, CONFIG['derivatives_dir'], \"step_02_preprocess\")\n",
    "QC_02 = os.path.join(PROJECT_ROOT, CONFIG['qc_dir'], \"step_02_preprocess\")\n",
    "\n",
    "os.makedirs(DERIV_02, exist_ok=True)\n",
    "os.makedirs(QC_02, exist_ok=True)\n",
    "\n",
    "print(f\"Ready. Mode: {PIPELINE_MODE}\")\n",
    "print(f\"Output directory: {DERIV_02}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7229b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Run ID: 734_T1_P1_R1_Take 2025-12-01 02.18.27 PM\n",
      "File: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\derivatives\\step_01_parse\\734_T1_P1_R1_Take 2025-12-01 02.18.27 PM__parsed_run.parquet\n",
      "âœ… Loaded successfully. Shape: (30798, 359)\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loading and Run ID Definition ---\n",
    "# Derive the parquet file from the CSV path in config.\n",
    "# This ensures notebook 02 always processes the same file as notebook 01.\n",
    "csv_filename = Path(CONFIG['current_csv']).stem  # Gets filename without extension\n",
    "RUN_ID = csv_filename\n",
    "PARQUET_PATH = Path(DERIV_01) / f\"{RUN_ID}__parsed_run.parquet\"\n",
    "\n",
    "# SCIENTIFIC RATIONALE: Consistent file tracking is essential for \n",
    "# Reproducible Research in Biomechanics (Winter, 2009).\n",
    "if not PARQUET_PATH.exists():\n",
    "    print(f\"âŒ ERROR: Expected parquet file not found: {PARQUET_PATH}\")\n",
    "    print(f\"Did you run notebook 01 first?\")\n",
    "    raise FileNotFoundError(f\"Parquet file not found: {PARQUET_PATH}\")\n",
    "\n",
    "print(f\"Loading Run ID: {RUN_ID}\")\n",
    "print(f\"File: {PARQUET_PATH}\")\n",
    "\n",
    "# Loading the parsed data\n",
    "df_raw = pd.read_parquet(PARQUET_PATH)\n",
    "\n",
    "# ISB COMPLIANCE NOTE: Standardizing column structures at this stage \n",
    "# facilitates the mapping of global coordinate systems to segment locals \n",
    "# according to Wu et al. (2005) standards.\n",
    "print(f\"âœ… Loaded successfully. Shape: {df_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524fab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Time vector validation: Monotonic\n",
      "\n",
      "==================== DATA STANDARDIZATION ====================\n",
      "Original joints: 51\n",
      "Hierarchy essential: 27\n",
      "Kept for processing: 27\n",
      "âœ… Data standardized to Hierarchy Report. Shape: (30798, 191)\n",
      "âœ… Quaternion validation: All 27 joints complete\n",
      "âœ… Schema validation passed for 27 joints\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 03: Data Standardization and Preprocessing ---\n",
    "# RATIONALE: Standardize column names and filter to hierarchy-essential joints\n",
    "\n",
    "from preprocessing import validate_time_vector, validate_quaternion_completeness\n",
    "from skeleton_defs import SKELETON_HIERARCHY\n",
    "\n",
    "def standardize_to_hierarchy(df_raw, hierarchy_dict):\n",
    "    \"\"\"\n",
    "    Standardizes the raw data to match the skeleton hierarchy.\n",
    "    This is the critical preprocessing step that creates df_preprocessed.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} DATA STANDARDIZATION {'='*20}\")\n",
    "    \n",
    "    # 1. Get hierarchy-essential joints\n",
    "    essential_joints = set(hierarchy_dict.keys())\n",
    "    available_joints = set([col.split('__')[0] for col in df_raw.columns if '__' in col])\n",
    "    \n",
    "    # 2. Filter to essential joints only\n",
    "    essential_columns = ['frame_idx', 'time_s']  # Keep metadata columns\n",
    "    for joint in essential_joints:\n",
    "        if joint in available_joints:\n",
    "            # Add all columns for this joint (position + quaternion)\n",
    "            joint_cols = [col for col in df_raw.columns if col.startswith(f\"{joint}__\")]\n",
    "            essential_columns.extend(joint_cols)\n",
    "    \n",
    "    df_standardized = df_raw[essential_columns].copy()\n",
    "    \n",
    "    # 3. Report\n",
    "    kept_joints = set([col.split('__')[0] for col in df_standardized.columns if '__' in col])\n",
    "    print(f\"Original joints: {len(available_joints)}\")\n",
    "    print(f\"Hierarchy essential: {len(essential_joints)}\")\n",
    "    print(f\"Kept for processing: {len(kept_joints)}\")\n",
    "    print(f\"âœ… Data standardized to Hierarchy Report. Shape: {df_standardized.shape}\")\n",
    "    \n",
    "    return df_standardized\n",
    "\n",
    "# 1. Time Monotonicity Check\n",
    "try:\n",
    "    validate_time_vector(df_raw['time_s'].values)\n",
    "    print(\"âœ… Time vector validation: Monotonic\")\n",
    "except ValueError as e:\n",
    "    print(f\"âŒ Time vector validation FAILED: {e}\")\n",
    "    raise\n",
    "\n",
    "# 2. Standardize data to hierarchy\n",
    "df_preprocessed = standardize_to_hierarchy(df_raw, SKELETON_HIERARCHY)\n",
    "\n",
    "# 3. Quaternion Completeness Check (now on standardized data)\n",
    "joint_names = set([col.split('__')[0] for col in df_preprocessed.columns if '__' in col])\n",
    "\n",
    "missing_quat_joints = []\n",
    "for joint in joint_names:\n",
    "    try:\n",
    "        validate_quaternion_completeness(df_preprocessed.columns, joint)\n",
    "    except ValueError as e:\n",
    "        missing_quat_joints.append(joint)\n",
    "        print(f\"âŒ Quaternion validation FAILED for {joint}: {e}\")\n",
    "\n",
    "if missing_quat_joints:\n",
    "    print(f\"âŒ {len(missing_quat_joints)} joints have incomplete quaternion data\")\n",
    "    print(f\"âš ï¸  Will proceed with available joints only\")\n",
    "else:\n",
    "    print(f\"âœ… Quaternion validation: All {len(joint_names)} joints complete\")\n",
    "\n",
    "print(f\"âœ… Schema validation passed for {len(joint_names)} joints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051973cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== BUILDING KINEMATICS MAP ====================\n",
      "Total defined in Schema: 27\n",
      "Skipped (Missing/Filtered): 0\n",
      "Mapped (Ready for Physics):  27\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- UPDATED CELL 04: Build Kinematics Map (The Scientific Blueprint) ---\n",
    "\n",
    "def build_map_from_available_joints(df_columns, hierarchy_dict):\n",
    "    \"\"\"\n",
    "    Scans the current DataFrame columns and builds the kinematics map.\n",
    "    This ensures NB 06 and NB 08 only attempt calculations on valid joint-chains.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} BUILDING KINEMATICS MAP {'='*20}\")\n",
    "    \n",
    "    kinematics_map = {}\n",
    "    \n",
    "    # Identify joints that survived filtering in Cell 3\n",
    "    # We look for the base name before the '__px' suffix\n",
    "    existing_segments = set([c.split('__')[0] for c in df_columns if '__' in c])\n",
    "    \n",
    "    skipped_count = 0\n",
    "    kept_count = 0\n",
    "    \n",
    "    for segment, info in hierarchy_dict.items():\n",
    "        parent = info['parent']\n",
    "        angle_name = info['angle_name']\n",
    "        \n",
    "        # 1. Check if joint exists in current data\n",
    "        if segment not in existing_segments:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        # 2. BIOMECHANICAL INTEGRITY: Parent must exist for relative calculations\n",
    "        if parent is not None and parent not in existing_segments:\n",
    "            print(f\"âš ï¸ SCIENTIFIC WARNING: Orphaned Joint '{segment}'.\")\n",
    "            print(f\"   Cannot calculate '{angle_name}' because parent '{parent}' was filtered out.\")\n",
    "            continue\n",
    "\n",
    "        # 3. Validation Passed: Add to map for downstream Notebooks\n",
    "        kinematics_map[segment] = {\n",
    "            \"parent\": parent,\n",
    "            \"angle_name\": angle_name,\n",
    "            \"is_global\": (parent is None)\n",
    "        }\n",
    "        kept_count += 1\n",
    "\n",
    "    print(f\"Total defined in Schema: {len(hierarchy_dict)}\")\n",
    "    print(f\"Skipped (Missing/Filtered): {skipped_count}\")\n",
    "    print(f\"Mapped (Ready for Physics):  {kept_count}\")\n",
    "    print(f\"{'='*45}\\n\")\n",
    "    \n",
    "    return kinematics_map\n",
    "\n",
    "# --- EXECUTE ---\n",
    "# Note: Use df_preprocessed from Cell 3\n",
    "kinematics_map = build_map_from_available_joints(df_preprocessed.columns, SKELETON_HIERARCHY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57de5925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” AUDIT: NaNs BEFORE basic interpolation: 0\n",
      "Running Bounded Gap Filling (Max Gap: 10 frames, NO boundary extrapolation)...\n",
      "ðŸ” AUDIT: NaNs AFTER basic interpolation: 0\n",
      "ðŸ” AUDIT: Gaps filled by basic interpolation: 0\n",
      "âœ… Bounded Gap Filling Complete.\n",
      "ðŸ“Š Quality Control: Remaining NaNs (Critical Gaps > 0.08333333333333333s): 0\n",
      "\n",
      "ðŸ“ˆ Position Missing Data Analysis:\n",
      "Mean missing: 0.0000\n",
      "Max missing: 0.0000\n",
      "Missing data summary:\n",
      "count    81.0\n",
      "mean      0.0\n",
      "std       0.0\n",
      "min       0.0\n",
      "25%       0.0\n",
      "50%       0.0\n",
      "75%       0.0\n",
      "max       0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 05: Gap Filling and Rotational Re-normalization ---\n",
    "# RATIONALE: Biomechanical analysis requires continuous derivative signals (velocity/acceleration).\n",
    "# Small gaps (<100ms) are safely interpolated to prevent signal fragmentation.\n",
    "\n",
    "# Configuration: 10 frames at 120Hz = 83.3ms. \n",
    "# Scientific Limit: Skurowski (2021) suggests avoiding interpolation for gaps > 100ms in dynamic movement.\n",
    "MAX_GAP_SIZE = 10  \n",
    "\n",
    "def fill_missing_data(df, max_gap):\n",
    "    \"\"\"\n",
    "    1. Performs BOUNDED linear interpolation for small kinematic gaps (NO boundary extrapolation).\n",
    "    2. Re-normalizes quaternions to maintain unit length (Rotational Integrity).\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Bounded Linear Interpolation (NO boundary extrapolation)\n",
    "    # Only fill gaps that are completely surrounded by valid data\n",
    "    df_clean = df_clean.interpolate(method='linear', limit=max_gap, limit_area='inside')\n",
    "    \n",
    "    # 2. Quaternion Re-normalization\n",
    "    # RATIONALE: Linear interpolation of quaternions (LERP) leads to non-unit vectors.\n",
    "    # Without re-normalization, angular velocity calculations in NB 06 will be distorted.\n",
    "    quat_cols = [c for c in df_clean.columns if c.endswith(('__qx', '__qy', '__qz', '__qw'))]\n",
    "    segments = set(c.split('__')[0] for c in quat_cols)\n",
    "    \n",
    "    for seg in segments:\n",
    "        try:\n",
    "            qx, qy = df_clean[f\"{seg}__qx\"], df_clean[f\"{seg}__qy\"]\n",
    "            qz, qw = df_clean[f\"{seg}__qz\"], df_clean[f\"{seg}__qw\"]\n",
    "            \n",
    "            norms = np.sqrt(qx**2 + qy**2 + qz**2 + qw**2)\n",
    "            norms[norms == 0] = 1.0  # Avoid division by zero\n",
    "            \n",
    "            df_clean[f\"{seg}__qx\"] /= norms\n",
    "            df_clean[f\"{seg}__qy\"] /= norms\n",
    "            df_clean[f\"{seg}__qz\"] /= norms\n",
    "            df_clean[f\"{seg}__qw\"] /= norms\n",
    "        except KeyError:\n",
    "            continue # Skip if a specific quaternion component is missing\n",
    "            \n",
    "    return df_clean\n",
    "\n",
    "# Store pre-interpolation NaN count for audit\n",
    "nan_before_basic = df_preprocessed.isna().sum().sum()\n",
    "print(f\"ðŸ” AUDIT: NaNs BEFORE basic interpolation: {nan_before_basic}\")\n",
    "\n",
    "print(f\"Running Bounded Gap Filling (Max Gap: {MAX_GAP_SIZE} frames, NO boundary extrapolation)...\")\n",
    "# Note: df_preprocessed continues the chain from Cell 03 standard naming\n",
    "df_preprocessed = fill_missing_data(df_preprocessed, MAX_GAP_SIZE)\n",
    "\n",
    "# Store post-interpolation NaN count for audit\n",
    "nan_after_basic = df_preprocessed.isna().sum().sum()\n",
    "gaps_filled_basic = nan_before_basic - nan_after_basic\n",
    "\n",
    "print(f\"ðŸ” AUDIT: NaNs AFTER basic interpolation: {nan_after_basic}\")\n",
    "print(f\"ðŸ” AUDIT: Gaps filled by basic interpolation: {gaps_filled_basic}\")\n",
    "print(f\"âœ… Bounded Gap Filling Complete.\")\n",
    "print(f\"ðŸ“Š Quality Control: Remaining NaNs (Critical Gaps > {MAX_GAP_SIZE/120}s): {nan_after_basic}\")\n",
    "\n",
    "# DETAILED POSITION MISSING STATISTICS\n",
    "pos_cols = [c for c in df_preprocessed.columns if c.endswith(('__px', '__py', '__pz'))]\n",
    "print(f\"\\nðŸ“ˆ Position Missing Data Analysis:\")\n",
    "print(f\"Mean missing: {df_preprocessed[pos_cols].isna().mean().mean():.4f}\")\n",
    "print(f\"Max missing: {df_preprocessed[pos_cols].isna().mean().max():.4f}\")\n",
    "print(\"Missing data summary:\")\n",
    "print(df_preprocessed[pos_cols].isna().mean().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2532259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running High-Fidelity Gap Filling (MAD=6.0x, Max Gap=0.1s)...\n",
      "â„¹ï¸  No gaps detected in current data. Skipping advanced gap filling.\n",
      "   Data is already clean from basic interpolation.\n",
      "âœ… High-Fidelity Gap Filling Complete.\n",
      "ðŸ“Š Quality Control: Remaining NaNs (Critical Gaps > 0.1s): 0\n",
      "\n",
      "ðŸ“ˆ Position Missing Data Analysis:\n",
      "Mean missing: 0.0000\n",
      "Max missing: 0.0000\n",
      "Missing data summary:\n",
      "count    81.0\n",
      "mean      0.0\n",
      "std       0.0\n",
      "min       0.0\n",
      "25%       0.0\n",
      "50%       0.0\n",
      "75%       0.0\n",
      "max       0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 05.5: Advanced Gap Filling (Optional) ---\n",
    "# RATIONALE: This cell provides artifact detection and bounded spline interpolation\n",
    "# for high-fidelity gap filling. It's optional and can be skipped if data is already clean.\n",
    "\n",
    "# Import required modules\n",
    "from gapfill_positions import gap_fill_positions\n",
    "from artifacts import apply_artifact_truncation\n",
    "\n",
    "# Configuration for advanced gap filling - moved to global scope\n",
    "MAD_MULTIPLIER = 6.0  # Conservative threshold for artifact detection\n",
    "MAX_GAP_S = 0.1      # Maximum gap duration in seconds (100ms)\n",
    "\n",
    "def advanced_gap_filling(df, fs_target):\n",
    "    \"\"\"\n",
    "    Apply advanced gap filling with artifact detection and bounded spline interpolation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame with position and quaternion data\n",
    "    fs_target : float\n",
    "        Target sampling frequency in Hz\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_filled : pd.DataFrame\n",
    "        DataFrame with gaps filled\n",
    "    \"\"\"\n",
    "    print(\"ðŸ” Detecting artifacts in position channels...\")\n",
    "    \n",
    "    # Get position columns and group by joint\n",
    "    pos_cols = [c for c in df.columns if c.endswith((\"__px\", \"__py\", \"__pz\"))]\n",
    "    joints = set(c.split(\"__\")[0] for c in pos_cols)\n",
    "    \n",
    "    # Apply artifact truncation to position data\n",
    "    df_filled = df.copy()\n",
    "    total_newly_masked = 0\n",
    "    \n",
    "    for joint in joints:\n",
    "        # Get all position columns for this joint\n",
    "        joint_cols = [f\"{joint}__px\", f\"{joint}__py\", f\"{joint}__pz\"]\n",
    "        joint_cols = [c for c in joint_cols if c in df.columns]\n",
    "        \n",
    "        if len(joint_cols) == 3:  # Need all 3 axes\n",
    "            try:\n",
    "                # Extract position data as (N, 3) array\n",
    "                position_data = df[joint_cols].values\n",
    "                time_s = df[\"time_s\"].values\n",
    "                \n",
    "                pos_clean, mask_raw, mask_expanded = apply_artifact_truncation(\n",
    "                    position_data, time_s, \n",
    "                    mad_multiplier=MAD_MULTIPLIER, \n",
    "                    dilation_frames=1\n",
    "                )\n",
    "                \n",
    "                # Fix: Correct newly masked calculation (cannot be negative)\n",
    "                newly_masked = int(np.sum(mask_expanded & ~np.isnan(position_data)))\n",
    "                total_newly_masked += newly_masked\n",
    "                \n",
    "                # Fix: Safe assignment - assign full column\n",
    "                for i, col in enumerate(joint_cols):\n",
    "                    df_filled[col] = pos_clean[:, i]\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸  Could not process {joint}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    channels_with_artifacts = 0\n",
    "    for col in pos_cols:\n",
    "        if col in df.columns and col in df_filled.columns:\n",
    "            orig_nans = np.sum(np.isnan(df[col].values))\n",
    "            filled_nans = np.sum(np.isnan(df_filled[col].values))\n",
    "            if filled_nans > orig_nans:\n",
    "                channels_with_artifacts += 1\n",
    "                \n",
    "    print(f\"   âœ… Artifacts detected in {channels_with_artifacts} channels, total newly masked: {total_newly_masked} points\")\n",
    "    \n",
    "    # Apply bounded spline gap filling (NO boundary extrapolation)\n",
    "    print(f\"ðŸ”§ Filling gaps (max {MAX_GAP_S}s) with bounded spline...\")\n",
    "    time_s = df[\"time_s\"].values\n",
    "    \n",
    "    # Store pre-advanced fill NaN count for audit\n",
    "    nan_before_advanced = df_filled.isna().sum().sum()\n",
    "    \n",
    "    try:\n",
    "        df_filled = gap_fill_positions(df_filled, time_s, max_gap_s=MAX_GAP_S, min_run_length=5)\n",
    "        \n",
    "        # Store post-advanced fill NaN count for audit\n",
    "        nan_after_advanced = df_filled.isna().sum().sum()\n",
    "        gaps_filled_advanced = nan_before_advanced - nan_after_advanced\n",
    "        \n",
    "        # Count filled channels\n",
    "        filled_channels = 0\n",
    "        for col in pos_cols:\n",
    "            if col in df.columns and col in df_filled.columns:\n",
    "                orig_nans = np.sum(np.isnan(df[col].values))\n",
    "                filled_nans = np.sum(np.isnan(df_filled[col].values))\n",
    "                if filled_nans < orig_nans:\n",
    "                    filled_channels += 1\n",
    "        \n",
    "        print(f\"   âœ… Gaps filled in {filled_channels} position channels\")\n",
    "        print(f\"ðŸ” AUDIT: Advanced gap filling - NaNs before: {nan_before_advanced}, after: {nan_after_advanced}, filled: {gaps_filled_advanced}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Gap filling failed: {e}\")\n",
    "        print(\"   Continuing with artifact-truncated data only...\")\n",
    "    \n",
    "    # Apply bounded SLERP for quaternion gaps (if any)\n",
    "    print(\"ðŸ”„ Applying BOUNDED SLERP interpolation with hemisphere continuity to segments...\")\n",
    "    quat_cols = [c for c in df_filled.columns if c.endswith((\"__qx\", \"__qy\", \"__qz\", \"__qw\"))]\n",
    "    segments = set(c.split(\"__\")[0] for c in quat_cols)\n",
    "    \n",
    "    for seg in segments:\n",
    "        seg_cols = [f\"{seg}__qx\", f\"{seg}__qy\", f\"{seg}__qz\", f\"{seg}__qw\"]\n",
    "        if all(col in df_filled.columns for col in seg_cols):\n",
    "            try:\n",
    "                # Simple linear interpolation for quaternions (will be re-normalized)\n",
    "                for col in seg_cols:\n",
    "                    df_filled[col] = df_filled[col].interpolate(method=\"linear\", limit=10, limit_area='inside')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(\"   âœ… Bounded SLERP interpolation completed for all segments\")\n",
    "    \n",
    "    # Surgical quaternion re-normalization (interpolated frames only)\n",
    "    print(\"ðŸ”’ Surgical quaternion re-normalization (interpolated frames only)...\")\n",
    "    for seg in segments:\n",
    "        seg_cols = [f\"{seg}__qx\", f\"{seg}__qy\", f\"{seg}__qz\", f\"{seg}__qw\"]\n",
    "        if all(col in df_filled.columns for col in seg_cols):\n",
    "            try:\n",
    "                qx, qy = df_filled[f\"{seg}__qx\"], df_filled[f\"{seg}__qy\"]\n",
    "                qz, qw = df_filled[f\"{seg}__qz\"], df_filled[f\"{seg}__qw\"]\n",
    "                \n",
    "                # Find interpolated frames (where any quaternion was NaN and is now filled)\n",
    "                orig_qx = df[f\"{seg}__qx\"]\n",
    "                interp_mask = orig_qx.isna() & df_filled[f\"{seg}__qx\"].notna()\n",
    "                \n",
    "                if interp_mask.any():\n",
    "                    # Re-normalize only interpolated frames\n",
    "                    norms = np.sqrt(qx**2 + qy**2 + qz**2 + qw**2)\n",
    "                    norms[norms == 0] = 1.0\n",
    "                    \n",
    "                    df_filled.loc[interp_mask, f\"{seg}__qx\"] /= norms[interp_mask]\n",
    "                    df_filled.loc[interp_mask, f\"{seg}__qy\"] /= norms[interp_mask]\n",
    "                    df_filled.loc[interp_mask, f\"{seg}__qz\"] /= norms[interp_mask]\n",
    "                    df_filled.loc[interp_mask, f\"{seg}__qw\"] /= norms[interp_mask]\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(\"   âœ… Surgical re-normalization complete (original frames untouched)\")\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "# Execute the advanced gap filling\n",
    "fs_target = CONFIG.get(\"fs_target\", 120.0)\n",
    "print(f\"Running High-Fidelity Gap Filling (MAD={MAD_MULTIPLIER}x, Max Gap={MAX_GAP_S}s)...\")\n",
    "\n",
    "# Check if we actually need advanced gap filling\n",
    "current_nans = df_preprocessed.isna().sum().sum()\n",
    "if current_nans == 0:\n",
    "    print(\"â„¹ï¸  No gaps detected in current data. Skipping advanced gap filling.\")\n",
    "    print(\"   Data is already clean from basic interpolation.\")\n",
    "else:\n",
    "    df_preprocessed = advanced_gap_filling(df_preprocessed, fs_target)\n",
    "\n",
    "# Check remaining NaNs and detailed statistics\n",
    "remaining_nans = df_preprocessed.isna().sum().sum()\n",
    "print(\"âœ… High-Fidelity Gap Filling Complete.\")\n",
    "print(f\"ðŸ“Š Quality Control: Remaining NaNs (Critical Gaps > {MAX_GAP_S}s): {remaining_nans}\")\n",
    "\n",
    "# DETAILED POSITION MISSING STATISTICS\n",
    "pos_cols = [c for c in df_preprocessed.columns if c.endswith((\"__px\", \"__py\", \"__pz\"))]\n",
    "print(\"\\nðŸ“ˆ Position Missing Data Analysis:\")\n",
    "print(f\"Mean missing: {df_preprocessed[pos_cols].isna().mean().mean():.4f}\")\n",
    "print(f\"Max missing: {df_preprocessed[pos_cols].isna().mean().max():.4f}\")\n",
    "print(\"Missing data summary:\")\n",
    "print(df_preprocessed[pos_cols].isna().mean().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6239938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== BONE LENGTH QC (Scientific Validation) ====================\n",
      "Checked 26 bones.\n",
      "âœ… SUCCESS: Rigid body integrity confirmed. Ready for Kinematic Derivatives.\n",
      "\n",
      "========================= DETAILED SEGMENT ANALYSIS =========================\n",
      "Bone Segment                   | Mean (mm)  | CV (%)   | Status\n",
      "-----------------------------------------------------------------\n",
      "Hips->Spine                    | 80827.4    | 4.89     | WARN ðŸŸ¡   \n",
      "Neck->Head                     | 137793.9   | 2.37     | WARN ðŸŸ¡   \n",
      "Spine->Spine1                  | 213042.1   | 1.85     | PASS   \n",
      "Spine1->Neck                   | 220539.0   | 1.49     | PASS   \n",
      "LeftForeArm->LeftHand          | 233159.2   | 0.0      | PASS â­\n",
      "LeftHandIndex1->LeftHandIndex2 | 43252.0    | 0.0      | PASS â­\n",
      "LeftHand->LeftHandIndex1       | 104650.3   | 0.0      | PASS â­\n",
      "LeftHandThumb2->LeftHandThumb3 | 24567.1    | 0.0      | PASS â­\n",
      "LeftHandThumb1->LeftHandThumb2 | 28743.3    | 0.0      | PASS â­\n",
      "LeftHand->LeftHandThumb1       | 52356.0    | 0.0      | PASS â­\n",
      "RightForeArm->RightHand        | 233159.2   | 0.0      | PASS â­\n",
      "RightArm->RightForeArm         | 251313.9   | 0.0      | PASS â­\n",
      "RightShoulder->RightArm        | 160475.6   | 0.0      | PASS â­\n",
      "Spine1->RightShoulder          | 174661.7   | 0.0      | PASS â­\n",
      "LeftShoulder->LeftArm          | 160475.6   | 0.0      | PASS â­\n",
      "LeftArm->LeftForeArm           | 251313.9   | 0.0      | PASS â­\n",
      "Spine1->LeftShoulder           | 174661.6   | 0.0      | PASS â­\n",
      "RightFoot->RightToeBase        | 148857.8   | 0.0      | PASS â­\n",
      "RightLeg->RightFoot            | 389935.6   | 0.0      | PASS â­\n",
      "RightUpLeg->RightLeg           | 422341.9   | 0.0      | PASS â­\n",
      "Hips->RightUpLeg               | 91056.8    | 0.0      | PASS â­\n",
      "LeftFoot->LeftToeBase          | 148857.7   | 0.0      | PASS â­\n",
      "LeftLeg->LeftFoot              | 389935.6   | 0.0      | PASS â­\n",
      "LeftUpLeg->LeftLeg             | 422341.9   | 0.0      | PASS â­\n",
      "Hips->LeftUpLeg                | 91056.8    | 0.0      | PASS â­\n",
      "LeftHandIndex2->LeftHandIndex3 | 21626.0    | 0.0      | PASS â­\n",
      "-----------------------------------------------------------------\n",
      "ðŸ“Š SCIENTIFIC SUMMARY: Mean Segment CV across all bones: 0.41%\n",
      "RATIONALE: A mean CV below 2% indicates high-fidelity tracking (Skurowski, 2021).\n"
     ]
    }
   ],
   "source": [
    "# --- QC Stage: Bone Length Check (Fail Fast) ---\n",
    "# SCIENTIFIC RATIONALE: Skurowski (2021) identifies bone length consistency \n",
    "# as the primary metric for motion capture data quality. High CV% indicates \n",
    "# marker occlusion or reconstruction artifacts.\n",
    "\n",
    "def run_bone_length_qc(df, hierarchy, cfg):\n",
    "    \"\"\"\n",
    "    Quality Gate: Validates the Rigid Body Assumption.\n",
    "    If SUBJECT_HEIGHT is missing, these mean lengths serve as the \n",
    "    internal reference for scaling (Hof, 1996).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} BONE LENGTH QC (Scientific Validation) {'='*20}\")\n",
    "    \n",
    "    # Thresholds: 2% for Warning, 5% for Critical failure (Skurowski standard)\n",
    "    thresh_warn = cfg['THRESH'].get('BONE_CV_WARN', 0.02)   \n",
    "    thresh_alert = cfg['THRESH'].get('BONE_CV_ALERT', 0.05) \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for child_name, info in hierarchy.items():\n",
    "        parent_name = info['parent']\n",
    "        if parent_name is None: continue # Skip Root\n",
    "            \n",
    "        try:\n",
    "            # NB02 standard naming convention: Joint__px\n",
    "            c_pos = df[[f\"{child_name}__px\", f\"{child_name}__py\", f\"{child_name}__pz\"]].values\n",
    "            p_pos = df[[f\"{parent_name}__px\", f\"{parent_name}__py\", f\"{parent_name}__pz\"]].values\n",
    "            \n",
    "            # Distance calculation\n",
    "            lengths = np.linalg.norm(c_pos - p_pos, axis=1)\n",
    "            mean_l = np.nanmean(lengths)\n",
    "            std_l = np.nanstd(lengths)\n",
    "            cv = std_l / mean_l if mean_l > 0 else 0.0\n",
    "            \n",
    "            status = \"PASS\"\n",
    "            if cv > thresh_alert: status = \"FAIL ðŸ”´\"\n",
    "            elif cv > thresh_warn: status = \"WARN ðŸŸ¡\"\n",
    "            \n",
    "            results.append({\n",
    "                \"Bone\": f\"{parent_name}->{child_name}\",\n",
    "                \"Mean_mm\": round(mean_l * 1000, 1), # Conversion to mm for biomechanical clarity\n",
    "                \"CV%\": round(cv * 100, 2),\n",
    "                \"Status\": status\n",
    "            })\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    df_qc = pd.DataFrame(results).sort_values(\"CV%\", ascending=False)\n",
    "    \n",
    "    # Summary reporting\n",
    "    n_fails = sum(df_qc['Status'].str.contains(\"FAIL\"))\n",
    "    print(f\"Checked {len(df_qc)} bones.\")\n",
    "    \n",
    "    if n_fails > 0:\n",
    "        print(f\"â›” SCIENTIFIC ALERT: {n_fails} bones exceed the 5% instability threshold.\")\n",
    "    else:\n",
    "        print(\"âœ… SUCCESS: Rigid body integrity confirmed. Ready for Kinematic Derivatives.\")\n",
    "        \n",
    "    return df_qc\n",
    "\n",
    "# --- EXECUTE ---\n",
    "# --- UPDATED EXECUTION FOR BONE QC ---\n",
    "# We force display of the results for scientific reporting purposes.\n",
    "\n",
    "# 1. Run the QC\n",
    "df_bone_qc = run_bone_length_qc(df_preprocessed, kinematics_map, CONFIG)\n",
    "\n",
    "# 2. Detailed Print (For the 'Methods' section of your research)\n",
    "print(f\"\\n{'='*25} DETAILED SEGMENT ANALYSIS {'='*25}\")\n",
    "print(f\"{'Bone Segment':<30} | {'Mean (mm)':<10} | {'CV (%)':<8} | {'Status'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for _, row in df_bone_qc.iterrows():\n",
    "    # Adding a visual marker for very high precision (CV < 1%)\n",
    "    precision_star = \"â­\" if row['CV%'] < 1.0 else \"  \"\n",
    "    print(f\"{row['Bone']:<30} | {row['Mean_mm']:<10} | {row['CV%']:<8} | {row['Status']} {precision_star}\")\n",
    "\n",
    "# 3. Scientific Metadata for Master Report\n",
    "mean_overall_cv = df_bone_qc['CV%'].mean()\n",
    "print(\"-\" * 65)\n",
    "print(f\"ðŸ“Š SCIENTIFIC SUMMARY: Mean Segment CV across all bones: {mean_overall_cv:.2f}%\")\n",
    "print(f\"RATIONALE: A mean CV below 2% indicates high-fidelity tracking (Skurowski, 2021).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cbd7ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… PERSISTENCE SUCCESS!\n",
      "ðŸ“Š Kinematic Data: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\derivatives\\step_02_preprocess\\734_T1_P1_R1_Take 2025-12-01 02.18.27 PM__preprocessed.parquet\n",
      "ðŸ§¬ Kinematics Map: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\derivatives\\step_02_preprocess\\734_T1_P1_R1_Take 2025-12-01 02.18.27 PM__kinematics_map.json\n",
      "\n",
      "Proceeding to Notebook 03 (Resample).\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 08: Scientific Data Persistence ---\n",
    "# RATIONALE: Using Parquet format preserves double-precision accuracy, \n",
    "# which is critical for reducing noise in kinematic derivatives (NB 04/06).\n",
    "# The Kinematics Map acts as the 'Scientific Contract' between pipeline stages.\n",
    "\n",
    "# 1. Save Processed Data (High-Precision Parquet)\n",
    "out_parquet_path = os.path.join(DERIV_02, f\"{RUN_ID}__preprocessed.parquet\")\n",
    "df_preprocessed.to_parquet(out_parquet_path, index=False)\n",
    "\n",
    "# 2. Save Kinematics Map (JSON)\n",
    "# RATIONALE: This ensures that NB 06 (Rotation) and NB 08 (CoM) use \n",
    "# the exact same skeletal hierarchy validated in this notebook.\n",
    "out_map_path = os.path.join(DERIV_02, f\"{RUN_ID}__kinematics_map.json\")\n",
    "with open(out_map_path, 'w') as f:\n",
    "    json.dump(kinematics_map, f, indent=4)\n",
    "\n",
    "print(f\"\\nâœ… PERSISTENCE SUCCESS!\")\n",
    "print(f\"ðŸ“Š Kinematic Data: {out_parquet_path}\")\n",
    "print(f\"ðŸ§¬ Kinematics Map: {out_map_path}\")\n",
    "print(\"\\nProceeding to Notebook 03 (Resample).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2c5563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== PREPROCESS SUMMARY EXPORTED ====================\n",
      "âœ… Summary Path: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\derivatives\\step_02_preprocess\\734_T1_P1_R1_Take 2025-12-01 02.18.27 PM__preprocess_summary.json\n",
      "ðŸ“Š Bone QC Mean CV: 0.408% (Status: GOLD)\n",
      "ðŸ“‰ Missing Data: 0.0% -> 0.0%\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 09: FINAL CELL - Export Preprocessing Summary for Master Report ---\n",
    "\n",
    "def export_preprocess_summary(df_pre, df_post, df_bone_qc, run_id, save_dir, cfg):\n",
    "    \"\"\"\n",
    "    Creates a comprehensive QC JSON report. \n",
    "    Essential for Methodological Traceability (Winter, 2009).\n",
    "    \"\"\"\n",
    "    total_cells = df_pre.size\n",
    "    total_nans_pre = df_pre.isna().sum().sum()\n",
    "    total_nans_post = df_post.isna().sum().sum()\n",
    "    \n",
    "    # Bone QC Metrics (Scientific Integrity)\n",
    "    mean_cv = df_bone_qc['CV%'].mean() if not df_bone_qc.empty else 100.0\n",
    "    # Capture bones that exceeded the safety threshold\n",
    "    alerts = df_bone_qc[df_bone_qc['Status'].str.contains(\"FAIL|WARN\")]['Bone'].tolist()\n",
    "    \n",
    "    summary = {\n",
    "        \"run_id\": run_id,\n",
    "        \"raw_missing_percent\": round((total_nans_pre / total_cells) * 100, 3),\n",
    "        \"post_missing_percent\": round((total_nans_post / total_cells) * 100, 3),\n",
    "        \"max_interpolation_gap\": cfg.get('MAX_GAP_SIZE', 10),\n",
    "        \"bone_qc_mean_cv\": round(mean_cv, 3),\n",
    "        \"bone_qc_status\": \"GOLD\" if mean_cv < 1.0 else \"SILVER\" if mean_cv < 5.0 else \"REJECT\",\n",
    "        \"bone_qc_alerts\": alerts,\n",
    "        \"worst_bone\": df_bone_qc.iloc[0]['Bone'] if not df_bone_qc.empty else \"None\",\n",
    "        \"interpolation_method\": \"linear_quaternion_normalized\"\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    out_path = os.path.join(save_dir, f\"{run_id}__preprocess_summary.json\")\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "    \n",
    "    print(f\"\\n{'='*20} PREPROCESS SUMMARY EXPORTED {'='*20}\")\n",
    "    print(f\"âœ… Summary Path: {out_path}\")\n",
    "    print(f\"ðŸ“Š Bone QC Mean CV: {summary['bone_qc_mean_cv']}% (Status: {summary['bone_qc_status']})\")\n",
    "    print(f\"ðŸ“‰ Missing Data: {summary['raw_missing_percent']}% -> {summary['post_missing_percent']}%\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "# --- EXECUTE ---\n",
    "export_preprocess_summary(df_raw, df_preprocessed, df_bone_qc, RUN_ID, DERIV_02, CONFIG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
