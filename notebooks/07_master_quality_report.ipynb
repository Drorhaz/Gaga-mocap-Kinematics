{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f3524988",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Found 20 valid JSON files out of 48 total\n",
            "üìä Found data for 3 complete run(s)\n",
            "üìÅ Steps loaded per run:\n",
            "  763_T2_P2_R2_Take_2025-12-25 10.51.23 AM_005: ['step_02', 'step_04', 'step_06', 'step_05', 'step_01']\n",
            "  734_T1_P1_R1_Take 2025-12-01 02.18.27 PM: ['step_01', 'step_02', 'step_04', 'step_05', 'step_06']\n",
            "  734_T1_P2_R1_Take 2025-12-01 02.28.24 PM: ['step_01', 'step_02', 'step_04', 'step_05', 'step_06']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import hashlib\n",
        "\n",
        "# --- Setup Paths ---\n",
        "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
        "if SRC_PATH not in sys.path:\n",
        "    sys.path.insert(0, SRC_PATH)\n",
        "\n",
        "# ============================================================\n",
        "# Safe Parsing Helpers (per specification)\n",
        "# ============================================================\n",
        "def safe_get(d, *keys, default='N/A'):\n",
        "    \"\"\"Safe nested dictionary access with default fallback\"\"\"\n",
        "    for key in keys:\n",
        "        if isinstance(d, dict):\n",
        "            d = d.get(key, {})\n",
        "        else:\n",
        "            return default\n",
        "    return d if (d != {} and d is not None) else default\n",
        "\n",
        "def safe_float(x, default=0.0):\n",
        "    \"\"\"Convert to float safely, strip %, handle None/N/A\"\"\"\n",
        "    if x is None or x == 'N/A':\n",
        "        return default\n",
        "    try:\n",
        "        if isinstance(x, str):\n",
        "            x = x.replace('%', '').strip()\n",
        "        return float(x)\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "def compute_file_hash(filepath):\n",
        "    \"\"\"\n",
        "    Compute SHA-256 hash of a file for data integrity verification.\n",
        "    Returns hash string or 'FILE_NOT_FOUND' if file doesn't exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        return 'FILE_NOT_FOUND'\n",
        "    \n",
        "    sha256_hash = hashlib.sha256()\n",
        "    try:\n",
        "        with open(filepath, \"rb\") as f:\n",
        "            # Read file in chunks to handle large files efficiently\n",
        "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
        "                sha256_hash.update(byte_block)\n",
        "        return sha256_hash.hexdigest()\n",
        "    except Exception as e:\n",
        "        return f'ERROR: {str(e)}'\n",
        "\n",
        "# ============================================================\n",
        "# File Discovery (exact suffix matching per specification)\n",
        "# ============================================================\n",
        "DERIV_ROOT = os.path.join(PROJECT_ROOT, \"derivatives\")\n",
        "\n",
        "# Required suffixes for exact matching (updated to match actual file names)\n",
        "REQUIRED_SUFFIXES = [\n",
        "    \"__step01_loader_report.json\",  # step_01 (actual naming)\n",
        "    \"__preprocess_summary.json\",    # step_02  \n",
        "    \"__filtering_summary.json\",     # step_04\n",
        "    \"__reference_summary.json\",     # step_05\n",
        "    \"__kinematics_summary.json\"     # step_06\n",
        "]\n",
        "\n",
        "# Scan recursively for *.json files\n",
        "json_files = glob.glob(os.path.join(DERIV_ROOT, \"**\", \"*.json\"), recursive=True)\n",
        "\n",
        "# Filter files with exact suffix matching\n",
        "valid_files = []\n",
        "for json_path in json_files:\n",
        "    filename = os.path.basename(json_path)\n",
        "    if any(filename.endswith(suffix) for suffix in REQUIRED_SUFFIXES):\n",
        "        valid_files.append(json_path)\n",
        "\n",
        "print(f\"üìÅ Found {len(valid_files)} valid JSON files out of {len(json_files)} total\")\n",
        "\n",
        "# ============================================================\n",
        "# Load + Group by Run_ID (per specification)\n",
        "# ============================================================\n",
        "from collections import defaultdict\n",
        "runs = defaultdict(dict)\n",
        "\n",
        "for json_path in valid_files:\n",
        "    filename = os.path.basename(json_path)\n",
        "    run_id = filename.split(\"__\")[0]\n",
        "    \n",
        "    try:\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Failed to load {filename}: {e}\")\n",
        "        continue\n",
        "    \n",
        "    # Categorize by exact suffix matching\n",
        "    if filename.endswith(\"__step01_loader_report.json\"):\n",
        "        runs[run_id][\"step_01\"] = data\n",
        "    elif filename.endswith(\"__preprocess_summary.json\"):\n",
        "        runs[run_id][\"step_02\"] = data\n",
        "    elif filename.endswith(\"__filtering_summary.json\"):\n",
        "        runs[run_id][\"step_04\"] = data\n",
        "    elif filename.endswith(\"__reference_summary.json\"):\n",
        "        runs[run_id][\"step_05\"] = data\n",
        "    elif filename.endswith(\"__kinematics_summary.json\"):\n",
        "        runs[run_id][\"step_06\"] = data\n",
        "\n",
        "# Skip runs missing critical data (require step_01 and step_06)\n",
        "complete_runs = {rid: steps for rid, steps in runs.items() \n",
        "                if steps.get('step_01') and steps.get('step_06')}\n",
        "\n",
        "print(f\"üìä Found data for {len(complete_runs)} complete run(s)\")\n",
        "print(f\"üìÅ Steps loaded per run:\")\n",
        "for rid, steps in complete_runs.items():\n",
        "    print(f\"  {rid}: {list(steps.keys())}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8873f71",
      "metadata": {},
      "source": [
        "# üìã Master Audit & Results Report\n",
        "\n",
        "---\n",
        "\n",
        "## Section 0: Data Lineage & Provenance\n",
        "**Purpose:** Ensure recording traceability from raw file to final result (Cereatti et al., 2024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b55e75a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SECTION 0: Data Lineage & Provenance\n",
        "# ============================================================\n",
        "\n",
        "# Get Git commit hash for pipeline version\n",
        "try:\n",
        "    import subprocess\n",
        "    git_hash = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], \n",
        "                                       cwd=PROJECT_ROOT).decode('ascii').strip()\n",
        "    pipeline_version_git = f\"git-{git_hash}\"\n",
        "except Exception:\n",
        "    pipeline_version_git = \"unknown\"\n",
        "\n",
        "# Build provenance table (dictionary-first approach)\n",
        "provenance_data = []\n",
        "\n",
        "for run_id, steps in complete_runs.items():\n",
        "    s01 = steps.get('step_01', {})\n",
        "    \n",
        "    # Extract identity information\n",
        "    processing_date = safe_get(s01, 'identity', 'processing_timestamp')\n",
        "    pipeline_version_reported = safe_get(s01, 'identity', 'pipeline_version')\n",
        "    csv_source = safe_get(s01, 'identity', 'csv_source')\n",
        "    optitrack_version = safe_get(s01, 'raw_data_quality', 'optitrack_version')\n",
        "    \n",
        "    # Parse Subject/Session/Take from run_id\n",
        "    # Expected format: SubjectID_SessionID_P#_R#_Take...\n",
        "    # Example: \"734_T1_P1_R1_Take 2025-12-01 02.18.27 PM\"\n",
        "    parts = run_id.split('_')\n",
        "    subject_id = parts[0] if len(parts) > 0 else 'N/A'\n",
        "    session_id = parts[1] if len(parts) > 1 else 'N/A'\n",
        "    \n",
        "    # Extract P# and R# (Phrase and Repetition)\n",
        "    phrase_num = 'N/A'\n",
        "    rep_num = 'N/A'\n",
        "    for part in parts:\n",
        "        if part.startswith('P') and len(part) > 1 and part[1].isdigit():\n",
        "            phrase_num = part\n",
        "        elif part.startswith('R') and len(part) > 1 and part[1].isdigit():\n",
        "            rep_num = part\n",
        "    \n",
        "    # Compute SHA-256 hashes\n",
        "    csv_hash = compute_file_hash(csv_source)\n",
        "    \n",
        "    # Find the final processed derivative (step_06 kinematics parquet)\n",
        "    final_derivative_path = os.path.join(PROJECT_ROOT, \"derivatives\", \"step_06_kinematics\", \n",
        "                                          f\"{run_id}__kinematics.parquet\")\n",
        "    derivative_hash = compute_file_hash(final_derivative_path)\n",
        "    \n",
        "    # Integrity check: Verify data integrity\n",
        "    # In future, step01 should store the hash for comparison verification\n",
        "    if csv_hash.startswith('ERROR'):\n",
        "        integrity_status = f\"‚ö†Ô∏è {csv_hash}\"\n",
        "    elif csv_hash == 'FILE_NOT_FOUND':\n",
        "        integrity_status = \"‚ùå FILE_MISSING\"\n",
        "    else:\n",
        "        integrity_status = \"‚úÖ OK\"\n",
        "    \n",
        "    provenance_data.append({\n",
        "        'Run_ID': run_id,\n",
        "        'Subject_ID': subject_id,\n",
        "        'Session_ID': session_id,\n",
        "        'Phrase': phrase_num,\n",
        "        'Repetition': rep_num,\n",
        "        'Processing_Date': processing_date,\n",
        "        'OptiTrack_Version': optitrack_version,\n",
        "        'Pipeline_Version_Reported': pipeline_version_reported,\n",
        "        'Pipeline_Version_Git': pipeline_version_git,\n",
        "        'Raw_CSV_Path': csv_source,\n",
        "        'Raw_CSV_SHA256': csv_hash[:16] + '...' if len(csv_hash) > 16 else csv_hash,  # Truncate for display\n",
        "        'Final_Derivative_SHA256': derivative_hash[:16] + '...' if len(derivative_hash) > 16 else derivative_hash,\n",
        "        'Integrity_Status': integrity_status\n",
        "    })\n",
        "\n",
        "# Create DataFrame from provenance data (dictionary-first approach)\n",
        "df_provenance = pd.DataFrame(provenance_data)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 0: DATA LINEAGE & PROVENANCE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total Runs: {len(df_provenance)}\")\n",
        "print(f\"Pipeline Version (Git): {pipeline_version_git}\")\n",
        "print(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Display provenance table\n",
        "display(df_provenance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0262efaa",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1: The R√°cz Calibration Layer\n",
        "**Purpose:** Verify the \"Ground Truth\" of the skeleton setup (R√°cz et al., 2025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba4459d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SECTION 1: The R√°cz Calibration Layer\n",
        "# ============================================================\n",
        "\n",
        "# Define calibration thresholds (per R√°cz et al., 2025)\n",
        "POINTER_ERROR_THRESHOLD_MM = 2.0  # Anatomical landmark precision threshold\n",
        "WAND_ERROR_THRESHOLD_MM = 1.0     # Global calibration quality threshold\n",
        "STATIC_OFFSET_THRESHOLD_DEG = 15.0  # Significant joint misalignment threshold\n",
        "\n",
        "# Build calibration quality table (dictionary-first approach)\n",
        "calibration_data = []\n",
        "\n",
        "for run_id, steps in complete_runs.items():\n",
        "    s01 = steps.get('step_01', {})\n",
        "    s05 = steps.get('step_05', {})\n",
        "    \n",
        "    # Extract OptiTrack calibration metrics (from step01)\n",
        "    # Note: Currently these are not in the JSON, marked as N/A for future implementation\n",
        "    pointer_error_mm = safe_float(safe_get(s01, 'calibration', 'pointer_tip_rms_error_mm'), default=None)\n",
        "    wand_error_mm = safe_float(safe_get(s01, 'calibration', 'wand_error_mm'), default=None)\n",
        "    \n",
        "    # If not available in step01, mark as N/A (to be added to pipeline)\n",
        "    if pointer_error_mm is None or pointer_error_mm == 0.0:\n",
        "        pointer_error_mm = 'N/A'\n",
        "        pointer_status = 'N/A'\n",
        "    else:\n",
        "        pointer_status = \"‚ö†Ô∏è REVIEW\" if pointer_error_mm > POINTER_ERROR_THRESHOLD_MM else \"‚úÖ OK\"\n",
        "    \n",
        "    if wand_error_mm is None or wand_error_mm == 0.0:\n",
        "        wand_error_mm = 'N/A'\n",
        "        wand_status = 'N/A'\n",
        "    else:\n",
        "        wand_status = \"‚ö†Ô∏è REVIEW\" if wand_error_mm > WAND_ERROR_THRESHOLD_MM else \"‚úÖ OK\"\n",
        "    \n",
        "    # Extract static offset corrections (from step05 reference detection)\n",
        "    static_offsets = safe_get(s05, 'static_offset_audit', default={})\n",
        "    \n",
        "    left_offset_deg = safe_float(safe_get(static_offsets, 'Left', 'measured_angle_deg'), default=0.0)\n",
        "    right_offset_deg = safe_float(safe_get(static_offsets, 'Right', 'measured_angle_deg'), default=0.0)\n",
        "    max_static_offset = max(abs(left_offset_deg), abs(right_offset_deg))\n",
        "    \n",
        "    # Extract reference pose stability (from step05)\n",
        "    ref_stability_mm = safe_float(safe_get(s05, 'reference_metrics', 'ref_stability_mm'), default=0.0)\n",
        "    ref_status = safe_get(s05, 'reference_metrics', 'ref_quality_status', default='UNKNOWN')\n",
        "    \n",
        "    # Determine overall calibration status\n",
        "    calibration_issues = []\n",
        "    \n",
        "    # Check pointer error (if available)\n",
        "    if pointer_status == \"‚ö†Ô∏è REVIEW\":\n",
        "        calibration_issues.append(f\"Pointer Error > {POINTER_ERROR_THRESHOLD_MM}mm\")\n",
        "    \n",
        "    # Check wand error (if available)\n",
        "    if wand_status == \"‚ö†Ô∏è REVIEW\":\n",
        "        calibration_issues.append(f\"Wand Error > {WAND_ERROR_THRESHOLD_MM}mm\")\n",
        "    \n",
        "    # Check static offset corrections\n",
        "    if max_static_offset > STATIC_OFFSET_THRESHOLD_DEG:\n",
        "        calibration_issues.append(f\"Static Offset {max_static_offset:.1f}¬∞ > {STATIC_OFFSET_THRESHOLD_DEG}¬∞\")\n",
        "    \n",
        "    # Check reference pose stability\n",
        "    if ref_status != 'PASS':\n",
        "        calibration_issues.append(f\"Reference Status: {ref_status}\")\n",
        "    \n",
        "    # Overall calibration status\n",
        "    if len(calibration_issues) > 0:\n",
        "        overall_status = \"‚ö†Ô∏è REVIEW\"\n",
        "        issues_text = \"; \".join(calibration_issues)\n",
        "    else:\n",
        "        overall_status = \"‚úÖ OK\"\n",
        "        issues_text = \"None\"\n",
        "    \n",
        "    calibration_data.append({\n",
        "        'Run_ID': run_id,\n",
        "        'Pointer_Tip_RMS_Error_mm': pointer_error_mm,\n",
        "        'Pointer_Status': pointer_status,\n",
        "        'Wand_Error_mm': wand_error_mm,\n",
        "        'Wand_Status': wand_status,\n",
        "        'Left_Shoulder_Offset_deg': round(left_offset_deg, 2),\n",
        "        'Right_Shoulder_Offset_deg': round(right_offset_deg, 2),\n",
        "        'Max_Static_Offset_deg': round(max_static_offset, 2),\n",
        "        'Ref_Stability_mm': round(ref_stability_mm, 2),\n",
        "        'Ref_Status': ref_status,\n",
        "        'Calibration_Status': overall_status,\n",
        "        'Issues': issues_text\n",
        "    })\n",
        "\n",
        "# Create DataFrame from calibration data (dictionary-first approach)\n",
        "df_calibration = pd.DataFrame(calibration_data)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 1: R√ÅCZ CALIBRATION LAYER\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total Runs: {len(df_calibration)}\")\n",
        "print(f\"Thresholds: Pointer ‚â§ {POINTER_ERROR_THRESHOLD_MM}mm, Wand ‚â§ {WAND_ERROR_THRESHOLD_MM}mm, Static Offset ‚â§ {STATIC_OFFSET_THRESHOLD_DEG}¬∞\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Summary statistics\n",
        "ok_count = (df_calibration['Calibration_Status'] == '‚úÖ OK').sum()\n",
        "review_count = (df_calibration['Calibration_Status'] == '‚ö†Ô∏è REVIEW').sum()\n",
        "\n",
        "print(f\"Calibration Summary:\")\n",
        "print(f\"  ‚úÖ OK: {ok_count}/{len(df_calibration)}\")\n",
        "print(f\"  ‚ö†Ô∏è REVIEW: {review_count}/{len(df_calibration)}\")\n",
        "print()\n",
        "\n",
        "# Display calibration table\n",
        "display(df_calibration)\n",
        "\n",
        "# Display detailed issues for REVIEW cases\n",
        "if review_count > 0:\n",
        "    print()\n",
        "    print(\"=\"*80)\n",
        "    print(\"CALIBRATION ISSUES REQUIRING REVIEW:\")\n",
        "    print(\"=\"*80)\n",
        "    review_runs = df_calibration[df_calibration['Calibration_Status'] == '‚ö†Ô∏è REVIEW']\n",
        "    for idx, row in review_runs.iterrows():\n",
        "        print(f\"\\n{row['Run_ID']}:\")\n",
        "        print(f\"  Issues: {row['Issues']}\")\n",
        "        print(f\"  Note: Virtual joint centers may be shifted. Verify anatomical landmarks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1412e7d",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2: Rigid-Body & Temporal Audit\n",
        "**Purpose:** Prove the skeleton didn't \"stretch\" or \"break\" during the dynamic dance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df2970e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SECTION 2: Rigid-Body & Temporal Audit\n",
        "# ============================================================\n",
        "\n",
        "# Define thresholds for rigid-body integrity\n",
        "BONE_LENGTH_VARIANCE_THRESHOLD_PERCENT = 2.0  # Maximum acceptable bone length CV%\n",
        "TEMPORAL_JITTER_THRESHOLD_MS = 0.5  # Maximum acceptable temporal jitter (std of dt)\n",
        "\n",
        "# Build rigid-body and temporal audit table (dictionary-first approach)\n",
        "rigid_body_data = []\n",
        "\n",
        "for run_id, steps in complete_runs.items():\n",
        "    s01 = steps.get('step_01', {})\n",
        "    s02 = steps.get('step_02', {})\n",
        "    s03 = steps.get('step_03', {})\n",
        "    \n",
        "    # ============================================================\n",
        "    # RIGID-BODY INTEGRITY: Bone Length Stability\n",
        "    # ============================================================\n",
        "    # Extract bone length coefficient of variation (CV%) from step02\n",
        "    # CV% measures how much bone lengths varied during the dynamic trial\n",
        "    # CV = (std_dev / mean) * 100\n",
        "    bone_cv_percent = safe_float(safe_get(s02, 'bone_qc_mean_cv'), default=0.0)\n",
        "    bone_qc_status = safe_get(s02, 'bone_qc_status', default='UNKNOWN')\n",
        "    worst_bone = safe_get(s02, 'worst_bone', default='N/A')\n",
        "    bone_alerts = safe_get(s02, 'bone_qc_alerts', default=[])\n",
        "    \n",
        "    # Count number of problematic bones\n",
        "    if isinstance(bone_alerts, list):\n",
        "        num_bone_alerts = len(bone_alerts)\n",
        "    else:\n",
        "        num_bone_alerts = int(bone_alerts) if bone_alerts != 'N/A' else 0\n",
        "    \n",
        "    # Determine rigid-body status\n",
        "    if bone_cv_percent > BONE_LENGTH_VARIANCE_THRESHOLD_PERCENT:\n",
        "        rigid_body_status = \"‚ö†Ô∏è REVIEW\"\n",
        "        rigid_body_issue = f\"Bone CV {bone_cv_percent:.2f}% > {BONE_LENGTH_VARIANCE_THRESHOLD_PERCENT}%\"\n",
        "    elif num_bone_alerts > 0:\n",
        "        rigid_body_status = \"‚ö†Ô∏è REVIEW\"\n",
        "        rigid_body_issue = f\"{num_bone_alerts} bone(s) with high variance\"\n",
        "    else:\n",
        "        rigid_body_status = \"‚úÖ OK\"\n",
        "        rigid_body_issue = \"None\"\n",
        "    \n",
        "    # ============================================================\n",
        "    # TEMPORAL INTEGRITY: Sample Time Jitter\n",
        "    # ============================================================\n",
        "    # Extract temporal jitter from step03 (resampling)\n",
        "    # time_grid_std_dt measures the standard deviation of inter-frame intervals\n",
        "    # If frames were dropped, this will be non-zero\n",
        "    time_jitter_sec = safe_float(safe_get(s03, 'time_grid_std_dt'), default=0.0)\n",
        "    time_jitter_ms = round(time_jitter_sec * 1000, 3)  # Convert to ms\n",
        "    temporal_status = safe_get(s03, 'temporal_status', default='UNKNOWN')\n",
        "    \n",
        "    # Extract sampling rate information\n",
        "    sampling_rate_hz = safe_float(safe_get(s01, 'raw_data_quality', 'sampling_rate_actual'), default=120.0)\n",
        "    total_frames = safe_get(s01, 'raw_data_quality', 'total_frames', default=0)\n",
        "    \n",
        "    # Determine temporal status\n",
        "    if time_jitter_ms > TEMPORAL_JITTER_THRESHOLD_MS:\n",
        "        temporal_audit_status = \"‚ö†Ô∏è REVIEW\"\n",
        "        temporal_issue = f\"Jitter {time_jitter_ms}ms > {TEMPORAL_JITTER_THRESHOLD_MS}ms\"\n",
        "    elif temporal_status != 'PERFECT':\n",
        "        temporal_audit_status = \"‚ö†Ô∏è REVIEW\"\n",
        "        temporal_issue = f\"Temporal Status: {temporal_status}\"\n",
        "    else:\n",
        "        temporal_audit_status = \"‚úÖ OK\"\n",
        "        temporal_issue = \"None\"\n",
        "    \n",
        "    # ============================================================\n",
        "    # OVERALL SECTION 2 STATUS\n",
        "    # ============================================================\n",
        "    section2_issues = []\n",
        "    if rigid_body_status == \"‚ö†Ô∏è REVIEW\":\n",
        "        section2_issues.append(rigid_body_issue)\n",
        "    if temporal_audit_status == \"‚ö†Ô∏è REVIEW\":\n",
        "        section2_issues.append(temporal_issue)\n",
        "    \n",
        "    if len(section2_issues) > 0:\n",
        "        overall_section2_status = \"‚ö†Ô∏è REVIEW\"\n",
        "        section2_issues_text = \"; \".join(section2_issues)\n",
        "    else:\n",
        "        overall_section2_status = \"‚úÖ OK\"\n",
        "        section2_issues_text = \"None\"\n",
        "    \n",
        "    rigid_body_data.append({\n",
        "        'Run_ID': run_id,\n",
        "        'Bone_Length_CV_%': round(bone_cv_percent, 3),\n",
        "        'Bone_QC_Status': bone_qc_status,\n",
        "        'Worst_Bone': worst_bone,\n",
        "        'Num_Bone_Alerts': num_bone_alerts,\n",
        "        'Rigid_Body_Status': rigid_body_status,\n",
        "        'Sampling_Rate_Hz': round(sampling_rate_hz, 2),\n",
        "        'Total_Frames': total_frames,\n",
        "        'Time_Jitter_ms': time_jitter_ms,\n",
        "        'Temporal_Status': temporal_status,\n",
        "        'Temporal_Audit_Status': temporal_audit_status,\n",
        "        'Section2_Status': overall_section2_status,\n",
        "        'Issues': section2_issues_text\n",
        "    })\n",
        "\n",
        "# Create DataFrame from rigid-body data (dictionary-first approach)\n",
        "df_rigid_body = pd.DataFrame(rigid_body_data)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 2: RIGID-BODY & TEMPORAL AUDIT\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total Runs: {len(df_rigid_body)}\")\n",
        "print(f\"Thresholds: Bone CV ‚â§ {BONE_LENGTH_VARIANCE_THRESHOLD_PERCENT}%, Time Jitter ‚â§ {TEMPORAL_JITTER_THRESHOLD_MS}ms\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Summary statistics\n",
        "ok_count = (df_rigid_body['Section2_Status'] == '‚úÖ OK').sum()\n",
        "review_count = (df_rigid_body['Section2_Status'] == '‚ö†Ô∏è REVIEW').sum()\n",
        "\n",
        "print(f\"Rigid-Body & Temporal Summary:\")\n",
        "print(f\"  ‚úÖ OK: {ok_count}/{len(df_rigid_body)}\")\n",
        "print(f\"  ‚ö†Ô∏è REVIEW: {review_count}/{len(df_rigid_body)}\")\n",
        "print()\n",
        "\n",
        "# Display rigid-body table\n",
        "display(df_rigid_body)\n",
        "\n",
        "# Display detailed issues for REVIEW cases\n",
        "if review_count > 0:\n",
        "    print()\n",
        "    print(\"=\"*80)\n",
        "    print(\"RIGID-BODY & TEMPORAL ISSUES REQUIRING REVIEW:\")\n",
        "    print(\"=\"*80)\n",
        "    review_runs = df_rigid_body[df_rigid_body['Section2_Status'] == '‚ö†Ô∏è REVIEW']\n",
        "    for idx, row in review_runs.iterrows():\n",
        "        print(f\"\\n{row['Run_ID']}:\")\n",
        "        print(f\"  Issues: {row['Issues']}\")\n",
        "        if row['Rigid_Body_Status'] == '‚ö†Ô∏è REVIEW':\n",
        "            print(f\"  Note: Skeleton may have 'stretched' or 'broken' during dynamic trial.\")\n",
        "            print(f\"        Worst bone: {row['Worst_Bone']} (CV: {row['Bone_Length_CV_%']:.3f}%)\")\n",
        "        if row['Temporal_Audit_Status'] == '‚ö†Ô∏è REVIEW':\n",
        "            print(f\"  Note: Temporal inconsistency detected - frames may have been dropped.\")\n",
        "            print(f\"        Time jitter: {row['Time_Jitter_ms']}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac57e63",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3: Gap & Interpolation Transparency\n",
        "**Purpose:** \"No Silent Fixes\" (Winter, 2009) - Full disclosure of data reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d18043",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SECTION 3: Gap & Interpolation Transparency\n",
        "# ============================================================\n",
        "\n",
        "# Define thresholds per Winter (2009) - \"No Silent Fixes\"\n",
        "MAX_ACCEPTABLE_MISSING_PERCENT = 5.0  # Reject if > 5% data reconstructed\n",
        "LINEAR_FALLBACK_WARNING = True  # Flag linear interpolation as scientific compromise\n",
        "\n",
        "# Build interpolation transparency table (dictionary-first approach)\n",
        "interpolation_data = []\n",
        "\n",
        "for run_id, steps in complete_runs.items():\n",
        "    s01 = steps.get('step_01', {})\n",
        "    s02 = steps.get('step_02', {})\n",
        "    \n",
        "    # ============================================================\n",
        "    # GLOBAL INTERPOLATION METRICS\n",
        "    # ============================================================\n",
        "    # Extract overall missing data percentages\n",
        "    raw_missing_percent = safe_float(safe_get(s02, 'raw_missing_percent'), default=0.0)\n",
        "    post_missing_percent = safe_float(safe_get(s02, 'post_missing_percent'), default=0.0)\n",
        "    frames_fixed_percent = raw_missing_percent  # Percentage of frames that required interpolation\n",
        "    \n",
        "    # Extract interpolation method and gap info\n",
        "    interpolation_method = safe_get(s02, 'interpolation_method', default='unknown')\n",
        "    max_gap_frames = safe_get(s02, 'max_interpolation_gap', default=0)\n",
        "    \n",
        "    # Sampling rate for gap duration calculation\n",
        "    sampling_rate_hz = safe_float(safe_get(s01, 'raw_data_quality', 'sampling_rate_actual'), default=120.0)\n",
        "    max_gap_ms = round((safe_float(max_gap_frames) / sampling_rate_hz) * 1000, 2)\n",
        "    \n",
        "    # ============================================================\n",
        "    # INTERPOLATION METHOD CLASSIFICATION\n",
        "    # ============================================================\n",
        "    # Detect if linear fallback was used (scientific compromise)\n",
        "    # Spline/CubicSpline = preferred (smooth, preserves acceleration)\n",
        "    # Linear = fallback (flattens acceleration, loses high-frequency content)\n",
        "    \n",
        "    method_lower = interpolation_method.lower()\n",
        "    \n",
        "    if 'linear' in method_lower and 'quaternion' not in method_lower:\n",
        "        # Pure linear interpolation - ORANGE WARNING\n",
        "        method_category = \"üü† Linear Fallback\"\n",
        "        method_note = \"Linear interpolation flattens acceleration (scientific compromise)\"\n",
        "    elif 'spline' in method_lower or 'cubic' in method_lower:\n",
        "        # Spline-based - PREFERRED\n",
        "        method_category = \"‚úÖ Spline/Cubic\"\n",
        "        method_note = \"Smooth interpolation preserving acceleration\"\n",
        "    elif 'slerp' in method_lower or 'quaternion' in method_lower:\n",
        "        # Quaternion interpolation - GOOD for rotations\n",
        "        method_category = \"‚úÖ Quaternion (SLERP)\"\n",
        "        method_note = \"Spherical interpolation for rotations\"\n",
        "    else:\n",
        "        # Unknown method\n",
        "        method_category = \"‚ö†Ô∏è Unknown\"\n",
        "        method_note = \"Interpolation method not documented\"\n",
        "    \n",
        "    # ============================================================\n",
        "    # TRANSPARENCY STATUS (Winter 2009: \"No Silent Fixes\")\n",
        "    # ============================================================\n",
        "    transparency_issues = []\n",
        "    \n",
        "    # Check 1: Excessive missing data\n",
        "    if frames_fixed_percent > MAX_ACCEPTABLE_MISSING_PERCENT:\n",
        "        transparency_issues.append(f\"Missing {frames_fixed_percent:.2f}% > {MAX_ACCEPTABLE_MISSING_PERCENT}%\")\n",
        "    \n",
        "    # Check 2: Linear fallback used (flag as compromise)\n",
        "    if \"Linear Fallback\" in method_category and frames_fixed_percent > 0:\n",
        "        transparency_issues.append(f\"Linear fallback used (acceleration flattened)\")\n",
        "    \n",
        "    # Check 3: Large gaps that required reconstruction\n",
        "    if max_gap_frames > 50:  # ~0.4s at 120Hz\n",
        "        transparency_issues.append(f\"Large gap: {max_gap_frames} frames ({max_gap_ms}ms)\")\n",
        "    \n",
        "    # Determine overall transparency status\n",
        "    if len(transparency_issues) > 0:\n",
        "        transparency_status = \"‚ö†Ô∏è REVIEW\"\n",
        "        transparency_notes = \"; \".join(transparency_issues)\n",
        "    else:\n",
        "        if frames_fixed_percent == 0.0:\n",
        "            transparency_status = \"‚úÖ PRISTINE\"\n",
        "            transparency_notes = \"No interpolation required (pristine data)\"\n",
        "        else:\n",
        "            transparency_status = \"‚úÖ OK\"\n",
        "            transparency_notes = \"Minor gaps filled with appropriate method\"\n",
        "    \n",
        "    interpolation_data.append({\n",
        "        'Run_ID': run_id,\n",
        "        'Raw_Missing_%': round(raw_missing_percent, 2),\n",
        "        'Frames_Fixed_%': round(frames_fixed_percent, 2),\n",
        "        'Max_Gap_Frames': max_gap_frames,\n",
        "        'Max_Gap_ms': max_gap_ms,\n",
        "        'Interpolation_Method': interpolation_method,\n",
        "        'Method_Category': method_category,\n",
        "        'Method_Note': method_note,\n",
        "        'Transparency_Status': transparency_status,\n",
        "        'Notes': transparency_notes\n",
        "    })\n",
        "\n",
        "# Create DataFrame from interpolation data (dictionary-first approach)\n",
        "df_interpolation = pd.DataFrame(interpolation_data)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 3: GAP & INTERPOLATION TRANSPARENCY (Winter, 2009)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total Runs: {len(df_interpolation)}\")\n",
        "print(f\"Principle: 'No Silent Fixes' - Full disclosure of data reconstruction\")\n",
        "print(f\"Threshold: Missing data ‚â§ {MAX_ACCEPTABLE_MISSING_PERCENT}%\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Summary statistics\n",
        "pristine_count = (df_interpolation['Transparency_Status'] == '‚úÖ PRISTINE').sum()\n",
        "ok_count = (df_interpolation['Transparency_Status'] == '‚úÖ OK').sum()\n",
        "review_count = (df_interpolation['Transparency_Status'] == '‚ö†Ô∏è REVIEW').sum()\n",
        "linear_fallback_count = (df_interpolation['Method_Category'].str.contains('Linear Fallback')).sum()\n",
        "\n",
        "print(f\"Interpolation Summary:\")\n",
        "print(f\"  ‚úÖ PRISTINE (no gaps): {pristine_count}/{len(df_interpolation)}\")\n",
        "print(f\"  ‚úÖ OK (minor gaps): {ok_count}/{len(df_interpolation)}\")\n",
        "print(f\"  ‚ö†Ô∏è REVIEW (issues): {review_count}/{len(df_interpolation)}\")\n",
        "print(f\"  üü† Linear Fallback used: {linear_fallback_count}/{len(df_interpolation)}\")\n",
        "print()\n",
        "\n",
        "# Display interpolation table\n",
        "display(df_interpolation)\n",
        "\n",
        "# Display detailed notes for all runs (transparency principle)\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "print(\"INTERPOLATION TRANSPARENCY NOTES (Per Winter 2009):\")\n",
        "print(\"=\"*80)\n",
        "for idx, row in df_interpolation.iterrows():\n",
        "    print(f\"\\n{row['Run_ID']}:\")\n",
        "    print(f\"  Method: {row['Method_Category']}\")\n",
        "    print(f\"  Frames Fixed: {row['Frames_Fixed_%']}%\")\n",
        "    print(f\"  Max Gap: {row['Max_Gap_Frames']} frames ({row['Max_Gap_ms']}ms)\")\n",
        "    print(f\"  Status: {row['Transparency_Status']}\")\n",
        "    print(f\"  Note: {row['Notes']}\")\n",
        "    \n",
        "    # Additional warning for linear fallback\n",
        "    if \"Linear Fallback\" in row['Method_Category']:\n",
        "        print(f\"  ‚ö†Ô∏è  CRITICAL: {row['Method_Note']}\")\n",
        "        print(f\"  ‚ö†Ô∏è  Impact: High-frequency acceleration data may be attenuated\")\n",
        "\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "print(\"NOTE: Per Winter (2009), all data reconstruction must be fully disclosed.\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================\n",
        "# PER-JOINT INTERPOLATION DETAILS (Enhancement 2)\n",
        "# ============================================================\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "print(\"PER-JOINT INTERPOLATION DETAILS (Enhancement 2):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Build per-joint table if available in any run\n",
        "all_joint_details = []\n",
        "for run_id, steps in complete_runs.items():\n",
        "    s02 = steps.get('step_02', {})\n",
        "    per_joint_data = safe_get(s02, 'interpolation_per_joint', default={})\n",
        "    \n",
        "    if per_joint_data and isinstance(per_joint_data, dict) and len(per_joint_data) > 0:\n",
        "        for joint_name, details in per_joint_data.items():\n",
        "            method = safe_get(details, 'method', default='N/A')\n",
        "            method_category = safe_get(details, 'method_category', default=method)\n",
        "            frames_fixed_pct = safe_float(safe_get(details, 'frames_fixed_percent'), default=0.0)\n",
        "            max_gap = safe_get(details, 'max_gap_frames', default=0)\n",
        "            \n",
        "            # Determine method display with color coding\n",
        "            if 'linear_fallback' in method_category.lower():\n",
        "                method_display = \"üü† Linear Fallback\"\n",
        "                note = \"Acceleration flattened (scientific compromise)\"\n",
        "            elif 'none' in method or frames_fixed_pct == 0.0:\n",
        "                method_display = \"‚úÖ Pristine\"\n",
        "                note = \"No interpolation required\"\n",
        "            elif 'quaternion' in method.lower() or 'slerp' in method.lower():\n",
        "                method_display = \"‚úÖ Quaternion/SLERP\"\n",
        "                note = \"Smooth spherical interpolation\"\n",
        "            elif 'spline' in method.lower() or 'cubic' in method.lower():\n",
        "                method_display = \"‚úÖ Spline/Cubic\"\n",
        "                note = \"Smooth interpolation\"\n",
        "            else:\n",
        "                method_display = f\"‚úÖ {method}\"\n",
        "                note = \"Standard interpolation\"\n",
        "            \n",
        "            all_joint_details.append({\n",
        "                'Run_ID': run_id,\n",
        "                'Joint': joint_name,\n",
        "                'Method': method_display,\n",
        "                'Frames_Fixed_%': round(frames_fixed_pct, 2),\n",
        "                'Max_Gap_Frames': max_gap,\n",
        "                'Note': note\n",
        "            })\n",
        "\n",
        "if len(all_joint_details) > 0:\n",
        "    df_joint_interp = pd.DataFrame(all_joint_details)\n",
        "    \n",
        "    # Filter to show only joints that needed interpolation or had issues\n",
        "    df_joint_interp_filtered = df_joint_interp[df_joint_interp['Frames_Fixed_%'] > 0]\n",
        "    \n",
        "    if len(df_joint_interp_filtered) > 0:\n",
        "        print(f\"\\nJoints requiring interpolation: {len(df_joint_interp_filtered)}\")\n",
        "        print(f\"üü† Linear Fallback cases: {(df_joint_interp['Method'].str.contains('Linear Fallback')).sum()}\")\n",
        "        print()\n",
        "        display(df_joint_interp_filtered.head(20))  # Show top 20 joints\n",
        "        \n",
        "        # Highlight linear fallback cases\n",
        "        linear_fallback_joints = df_joint_interp_filtered[\n",
        "            df_joint_interp_filtered['Method'].str.contains('Linear Fallback')\n",
        "        ]\n",
        "        if len(linear_fallback_joints) > 0:\n",
        "            print()\n",
        "            print(\"=\"*80)\n",
        "            print(\"üü† LINEAR FALLBACK JOINTS (Acceleration Compromised):\")\n",
        "            print(\"=\"*80)\n",
        "            for idx, row in linear_fallback_joints.iterrows():\n",
        "                print(f\"{row['Run_ID']} | {row['Joint']}: {row['Frames_Fixed_%']}% fixed, Max gap: {row['Max_Gap_Frames']} frames\")\n",
        "            print(\"=\"*80)\n",
        "    else:\n",
        "        print(\"\\n‚úÖ All joints have pristine data (no interpolation required)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Per-joint interpolation details not available in current pipeline\")\n",
        "    print(\"   Run Enhancement 2 on notebooks/02_preprocess.ipynb to populate this data\")\n",
        "    print(\"   See: PIPELINE_ENHANCEMENTS_SUMMARY.md for instructions\")\n",
        "\n",
        "print()\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f0a7271",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4: Winter's Residual Validation\n",
        "**Purpose:** Justify the filtering frequency (Winter, 2009) - Signal vs. Noise separation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcadc60e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SECTION 4: Winter's Residual Validation\n",
        "# ============================================================\n",
        "\n",
        "# Import matplotlib for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display as ipy_display\n",
        "\n",
        "# Define thresholds per Winter (2009)\n",
        "ARBITRARY_FILTERING_FLAG = True  # Flag if no knee point found\n",
        "MIN_CUTOFF_HZ = 4.0  # Minimum reasonable cutoff for dance (too low = over-smoothing)\n",
        "MAX_CUTOFF_HZ = 12.0  # Maximum reasonable cutoff (too high = noise retained)\n",
        "\n",
        "# Build Winter residual validation table (dictionary-first approach)\n",
        "winter_data = []\n",
        "\n",
        "for run_id, steps in complete_runs.items():\n",
        "    s04 = steps.get('step_04', {})\n",
        "    \n",
        "    # ============================================================\n",
        "    # FILTER PARAMETERS & WINTER ANALYSIS STATUS\n",
        "    # ============================================================\n",
        "    # Extract filter parameters\n",
        "    filter_params = safe_get(s04, 'filter_params', default={})\n",
        "    \n",
        "    filter_type = safe_get(filter_params, 'filter_type', default='unknown')\n",
        "    filter_method = safe_get(filter_params, 'filter_method', default='N/A')\n",
        "    cutoff_hz = safe_float(safe_get(filter_params, 'filter_cutoff_hz'), default=0.0)\n",
        "    filter_order = safe_get(filter_params, 'filter_order', default='N/A')\n",
        "    filter_range = safe_get(filter_params, 'filter_range_hz', default=[1, 12])\n",
        "    \n",
        "    # Check if Winter Residual Analysis was performed\n",
        "    winter_analysis_failed = safe_get(filter_params, 'winter_analysis_failed', default=None)\n",
        "    expected_dance_range = safe_get(filter_params, 'expected_dance_range', default='N/A')\n",
        "    \n",
        "    # Representative column used for analysis\n",
        "    representative_col = safe_get(filter_params, 'representative_column', default='N/A')\n",
        "    \n",
        "    # ============================================================\n",
        "    # WINTER ANALYSIS STATUS CLASSIFICATION\n",
        "    # ============================================================\n",
        "    # Determine if Winter's method was used and if it succeeded\n",
        "    \n",
        "    if winter_analysis_failed is None:\n",
        "        # Winter analysis not attempted (older pipeline or different method)\n",
        "        winter_status = \"‚ö†Ô∏è Not Performed\"\n",
        "        winter_note = \"Winter Residual Analysis not attempted - cutoff may be arbitrary\"\n",
        "        knee_point_found = False\n",
        "    elif winter_analysis_failed == False:\n",
        "        # Winter analysis succeeded - knee point found\n",
        "        winter_status = \"‚úÖ Validated\"\n",
        "        winter_note = f\"Knee point found at {cutoff_hz}Hz (signal/noise separation justified)\"\n",
        "        knee_point_found = True\n",
        "    else:\n",
        "        # Winter analysis failed - signal too noisy, no clear knee point\n",
        "        winter_status = \"üî¥ ARBITRARY\"\n",
        "        winter_note = \"Knee point NOT found - signal too noisy, filtering is arbitrary\"\n",
        "        knee_point_found = False\n",
        "    \n",
        "    # ============================================================\n",
        "    # CUTOFF FREQUENCY VALIDATION\n",
        "    # ============================================================\n",
        "    # Check if cutoff is within reasonable biomechanical range\n",
        "    cutoff_issues = []\n",
        "    \n",
        "    if cutoff_hz < MIN_CUTOFF_HZ:\n",
        "        cutoff_issues.append(f\"Cutoff {cutoff_hz}Hz < {MIN_CUTOFF_HZ}Hz (over-smoothing risk)\")\n",
        "    \n",
        "    if cutoff_hz > MAX_CUTOFF_HZ:\n",
        "        cutoff_issues.append(f\"Cutoff {cutoff_hz}Hz > {MAX_CUTOFF_HZ}Hz (noise retention risk)\")\n",
        "    \n",
        "    if not knee_point_found:\n",
        "        cutoff_issues.append(\"No knee point found (arbitrary cutoff)\")\n",
        "    \n",
        "    # Overall Section 4 status\n",
        "    if len(cutoff_issues) > 0:\n",
        "        section4_status = \"‚ö†Ô∏è REVIEW\"\n",
        "        section4_notes = \"; \".join(cutoff_issues)\n",
        "    else:\n",
        "        section4_status = \"‚úÖ OK\"\n",
        "        section4_notes = \"Cutoff justified by Winter Residual Analysis\"\n",
        "    \n",
        "    # ============================================================\n",
        "    # CHECK FOR RESIDUAL PLOT EXISTENCE\n",
        "    # ============================================================\n",
        "    # Check if Winter residual plot exists for this run\n",
        "    filter_plot_path = os.path.join(PROJECT_ROOT, \"derivatives\", \"step_04_filtering\",\n",
        "                                    f\"{run_id}__filter_check.png\")\n",
        "    psd_plot_path = os.path.join(PROJECT_ROOT, \"derivatives\", \"step_04_filtering\",\n",
        "                                 f\"{run_id}__filter_psd_validation.png\")\n",
        "    \n",
        "    has_filter_plot = os.path.exists(filter_plot_path)\n",
        "    has_psd_plot = os.path.exists(psd_plot_path)\n",
        "    \n",
        "    winter_data.append({\n",
        "        'Run_ID': run_id,\n",
        "        'Filter_Type': filter_type,\n",
        "        'Filter_Method': filter_method,\n",
        "        'Cutoff_Hz': cutoff_hz,\n",
        "        'Filter_Order': filter_order,\n",
        "        'Filter_Range_Hz': f\"{filter_range[0]}-{filter_range[1]}\" if isinstance(filter_range, list) else str(filter_range),\n",
        "        'Expected_Range': expected_dance_range,\n",
        "        'Winter_Status': winter_status,\n",
        "        'Knee_Point_Found': '‚úÖ Yes' if knee_point_found else '‚ùå No',\n",
        "        'Representative_Signal': representative_col,\n",
        "        'Has_Residual_Plot': '‚úÖ Yes' if has_filter_plot else '‚ùå No',\n",
        "        'Section4_Status': section4_status,\n",
        "        'Notes': section4_notes,\n",
        "        'Filter_Plot_Path': filter_plot_path if has_filter_plot else 'N/A',\n",
        "        'PSD_Plot_Path': psd_plot_path if has_psd_plot else 'N/A'\n",
        "    })\n",
        "\n",
        "# Create DataFrame from Winter data (dictionary-first approach)\n",
        "df_winter = pd.DataFrame(winter_data)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 4: WINTER'S RESIDUAL VALIDATION (Winter, 2009)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total Runs: {len(df_winter)}\")\n",
        "print(f\"Principle: Filter cutoff must be justified by RMS Residual Analysis\")\n",
        "print(f\"Acceptable Range: {MIN_CUTOFF_HZ}-{MAX_CUTOFF_HZ} Hz for dance movements\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Summary statistics\n",
        "validated_count = (df_winter['Winter_Status'] == '‚úÖ Validated').sum()\n",
        "arbitrary_count = (df_winter['Winter_Status'] == 'üî¥ ARBITRARY').sum()\n",
        "not_performed_count = (df_winter['Winter_Status'] == '‚ö†Ô∏è Not Performed').sum()\n",
        "knee_found_count = (df_winter['Knee_Point_Found'] == '‚úÖ Yes').sum()\n",
        "\n",
        "print(f\"Winter Residual Analysis Summary:\")\n",
        "print(f\"  ‚úÖ Validated (knee point found): {validated_count}/{len(df_winter)}\")\n",
        "print(f\"  üî¥ ARBITRARY (no knee point): {arbitrary_count}/{len(df_winter)}\")\n",
        "print(f\"  ‚ö†Ô∏è Not Performed: {not_performed_count}/{len(df_winter)}\")\n",
        "print(f\"  Knee points detected: {knee_found_count}/{len(df_winter)}\")\n",
        "print()\n",
        "\n",
        "# Display Winter validation table\n",
        "display(df_winter[[\n",
        "    'Run_ID', 'Filter_Method', 'Cutoff_Hz', 'Filter_Range_Hz',\n",
        "    'Winter_Status', 'Knee_Point_Found', 'Section4_Status', 'Notes'\n",
        "]])\n",
        "\n",
        "# Display detailed analysis for each run\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "print(\"WINTER RESIDUAL ANALYSIS DETAILS (Per Run):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for idx, row in df_winter.iterrows():\n",
        "    print(f\"\\n{row['Run_ID']}:\")\n",
        "    print(f\"  Filter Method: {row['Filter_Method']}\")\n",
        "    print(f\"  Selected Cutoff: {row['Cutoff_Hz']} Hz\")\n",
        "    print(f\"  Search Range: {row['Filter_Range_Hz']} Hz\")\n",
        "    print(f\"  Expected (Dance): {row['Expected_Range']}\")\n",
        "    print(f\"  Winter Status: {row['Winter_Status']}\")\n",
        "    print(f\"  Knee Point Found: {row['Knee_Point_Found']}\")\n",
        "    print(f\"  Representative Signal: {row['Representative_Signal']}\")\n",
        "    print(f\"  Status: {row['Section4_Status']}\")\n",
        "    print(f\"  Note: {row['Notes']}\")\n",
        "    \n",
        "    # Flag arbitrary filtering cases\n",
        "    if row['Winter_Status'] == 'üî¥ ARBITRARY':\n",
        "        print(f\"\\n  üî¥ CRITICAL WARNING: Filtering is ARBITRARY\")\n",
        "        print(f\"     - No clear signal/noise separation found\")\n",
        "        print(f\"     - RMS residual curve has no distinct knee point\")\n",
        "        print(f\"     - Cutoff frequency cannot be objectively justified\")\n",
        "        print(f\"     - Consider: signal too noisy, markers poorly tracked, or movement too random\")\n",
        "    \n",
        "    # Display residual plot if available\n",
        "    if row['Has_Residual_Plot'] == '‚úÖ Yes':\n",
        "        print(f\"\\n  üìä Winter Residual Plot Available:\")\n",
        "        print(f\"     Path: {row['Filter_Plot_Path']}\")\n",
        "        \n",
        "        # Optionally display the plot inline\n",
        "        try:\n",
        "            ipy_display(Image(filename=row['Filter_Plot_Path']))\n",
        "        except Exception as e:\n",
        "            print(f\"     (Plot display failed: {e})\")\n",
        "\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "print(\"WINTER'S METHOD INTERPRETATION:\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ Knee Point Found: Signal and noise clearly separated\")\n",
        "print(\"   ‚Üí Cutoff frequency is JUSTIFIED and OBJECTIVE\")\n",
        "print()\n",
        "print(\"üî¥ No Knee Point (ARBITRARY): Signal too noisy or poorly tracked\")\n",
        "print(\"   ‚Üí Cutoff frequency is SUBJECTIVE and may over-smooth or under-filter\")\n",
        "print(\"   ‚Üí Data quality should be reviewed before accepting results\")\n",
        "print()\n",
        "print(\"‚ö†Ô∏è Not Performed: Winter analysis not run (legacy or fixed cutoff)\")\n",
        "print(\"   ‚Üí Cutoff may be arbitrary - verify if appropriate for movement type\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e13bae",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Building Master Summary Table\n",
        "**Internal Processing:** Aggregating all JSON summaries into the Truth Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2ea776cd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Processed 3 complete runs\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Master Row Schema (fixed columns per specification)\n",
        "# ============================================================\n",
        "all_summaries = []\n",
        "\n",
        "for run_id, steps in complete_runs.items():\n",
        "    s01 = steps.get('step_01', {})\n",
        "    s02 = steps.get('step_02', {})\n",
        "    s04 = steps.get('step_04', {})\n",
        "    s05 = steps.get('step_05', {})\n",
        "    s06 = steps.get('step_06', {})\n",
        "    \n",
        "    # ============================================================\n",
        "    # Canonical Fields (normalize differences per specification)\n",
        "    # ============================================================\n",
        "    \n",
        "    # Sampling rate: fps = first_available with fallback to 120.0\n",
        "    fps = safe_float(\n",
        "        safe_get(s01, 'raw_data_quality', 'sampling_rate_actual',\n",
        "                'fs_actual_hz', 'sampling_rate_hz'),\n",
        "        default=120.0\n",
        "    )\n",
        "    \n",
        "    # Reference status: normalize to uppercase\n",
        "    ref_status_raw = safe_get(s05, 'reference_metrics', 'ref_quality_status', default='MISSING')\n",
        "    ref_status = str(ref_status_raw).upper()\n",
        "    \n",
        "    # ============================================================\n",
        "    # Gap Units Logic (use ms directly when available)\n",
        "    # ============================================================\n",
        "    max_gap_frames = safe_get(s02, 'max_interpolation_gap', default=0)\n",
        "    max_gap_ms_raw = safe_get(s02, 'max_gap_ms')  # Check if ms is directly available\n",
        "    \n",
        "    if max_gap_ms_raw != 'N/A' and max_gap_ms_raw is not None:\n",
        "        # Use ms directly when available\n",
        "        max_gap_ms = safe_float(max_gap_ms_raw)\n",
        "    else:\n",
        "        # Compute ms from frames only if ms is missing\n",
        "        max_gap_ms = round((safe_float(max_gap_frames) / fps) * 1000, 2)\n",
        "    \n",
        "    # ============================================================\n",
        "    # Build Master Row\n",
        "    # ============================================================\n",
        "    row = {\n",
        "        # --- Identity ---\n",
        "        \"Run_ID\": run_id,\n",
        "        \"Processing_Date\": safe_get(s01, 'identity', 'processing_timestamp'),\n",
        "        \n",
        "        # --- Step01 Fields ---\n",
        "        \"OptiTrack_Error_mm\": safe_float(safe_get(s01, 'raw_data_quality', 'optitrack_mean_error_mm')),\n",
        "        \"Total_Frames\": safe_get(s01, 'raw_data_quality', 'total_frames', default=0),\n",
        "        \n",
        "        # --- Step02 Fields ---\n",
        "        \"Missing_Raw_%\": safe_float(safe_get(s02, 'raw_missing_percent')),\n",
        "        \"Max_Gap_Frames\": max_gap_frames,\n",
        "        \"Max_Gap_MS\": max_gap_ms,\n",
        "        \"Bone_Stability_CV\": safe_float(safe_get(s02, 'bone_qc_mean_cv')),\n",
        "        \"Skeletal_Alerts\": safe_get(s02, 'bone_qc_alerts', default=0),\n",
        "        \"Worst_Bone\": safe_get(s02, 'worst_bone'),\n",
        "        \n",
        "        # --- Step05 Fields ---\n",
        "        \"Ref_Stability_mm\": safe_float(safe_get(s05, 'reference_metrics', 'ref_stability_mm')),\n",
        "        \"Ref_Status\": ref_status,\n",
        "        \n",
        "        # --- Step06 Signal Quality ---\n",
        "        \"Signal_Noise_RMS\": safe_float(safe_get(s06, 'signal_quality', 'avg_vel_residual_rms')),\n",
        "        \"Dom_Freq_Hz\": safe_float(safe_get(s06, 'signal_quality', 'avg_dominant_freq_hz')),\n",
        "        \"Quat_Norm_Error\": safe_float(safe_get(s06, 'signal_quality', 'max_quat_norm_error')),\n",
        "        \n",
        "        # --- Step06 Kinematics ---\n",
        "        \"Max_Ang_Vel\": safe_float(safe_get(s06, 'metrics', 'angular_velocity', 'max')),\n",
        "        \"Mean_Ang_Vel\": safe_float(safe_get(s06, 'metrics', 'angular_velocity', 'mean')),\n",
        "        \"Max_Lin_Acc\": safe_float(safe_get(s06, 'metrics', 'linear_accel', 'max')),\n",
        "        \"Outlier_Frames\": safe_get(s06, 'effort_metrics', 'outlier_frame_count', default=0),\n",
        "        \n",
        "        # --- Step06 Effort Metrics ---\n",
        "        \"Path_Length_M\": round(safe_float(safe_get(s06, 'effort_metrics', 'total_path_length_mm')) / 1000, 2),\n",
        "        \"Intensity_Index\": safe_float(safe_get(s06, 'effort_metrics', 'intensity_index')),\n",
        "        \n",
        "        # --- Overall Status ---\n",
        "        \"Pipeline_Status\": safe_get(s06, 'overall_status'),\n",
        "    }\n",
        "    \n",
        "    # ============================================================\n",
        "    # Quality Scoring (labeled heuristic per specification)\n",
        "    # ============================================================\n",
        "    score = 100.0\n",
        "    \n",
        "    # Penalties for data quality\n",
        "    score -= safe_float(row[\"Missing_Raw_%\"]) * 5\n",
        "    score -= (safe_float(row[\"Max_Gap_MS\"]) / 10)  # Penalty for large gaps\n",
        "    \n",
        "    # Penalties for skeletal stability\n",
        "    score -= safe_float(row[\"Bone_Stability_CV\"]) * 10 \n",
        "    score -= safe_float(row[\"Skeletal_Alerts\"]) * 5\n",
        "    \n",
        "    # Penalty for reference stability\n",
        "    ref_stab = safe_float(row[\"Ref_Stability_mm\"])\n",
        "    if ref_stab > 4.0: \n",
        "        score -= 15\n",
        "    \n",
        "    # Penalty for signal quality issues\n",
        "    if safe_float(row[\"Quat_Norm_Error\"]) > 0.1:\n",
        "        score -= 10\n",
        "    \n",
        "    row[\"Quality_Score\"] = round(max(0, min(100, score)), 2)\n",
        "    row[\"Quality_Score_Method\"] = \"heuristic_v1\"  # Required label per specification\n",
        "    \n",
        "    # ============================================================\n",
        "    # Research Decision Rule (deterministic per specification)\n",
        "    # ============================================================\n",
        "    if (row[\"Pipeline_Status\"] == \"PASS\" and \n",
        "        row[\"Quality_Score\"] >= 75 and \n",
        "        row[\"Ref_Status\"] == \"PASS\" and \n",
        "        safe_float(row[\"Bone_Stability_CV\"]) < 1.5):\n",
        "        row[\"Research_Decision\"] = \"ACCEPT\"\n",
        "    elif row[\"Pipeline_Status\"] == \"PASS\" and row[\"Quality_Score\"] >= 50:\n",
        "        row[\"Research_Decision\"] = \"REVIEW\"\n",
        "    else:\n",
        "        row[\"Research_Decision\"] = \"REJECT\"\n",
        "    \n",
        "    all_summaries.append(row)\n",
        "\n",
        "print(f\"üìä Processed {len(all_summaries)} complete runs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dff3b381",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üéâ Master Audit Log Created\n",
            "======================================================================\n",
            "üìä Total Runs: 3\n",
            "üíæ File: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\reports\\Master_Audit_Log_20260119_190230.xlsx\n",
            "======================================================================\n",
            "\n",
            "Decision Summary:\n",
            "Research_Decision\n",
            "ACCEPT    2\n",
            "REVIEW    1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Quality Score Stats:\n",
            "  Mean: 87.79\n",
            "  Min:  87.59\n",
            "  Max:  87.91\n",
            "\n",
            "Quality Score Method:\n",
            "  heuristic_v1\n",
            "\n",
            "Preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run_ID</th>\n",
              "      <th>Processing_Date</th>\n",
              "      <th>OptiTrack_Error_mm</th>\n",
              "      <th>Total_Frames</th>\n",
              "      <th>Missing_Raw_%</th>\n",
              "      <th>Max_Gap_Frames</th>\n",
              "      <th>Max_Gap_MS</th>\n",
              "      <th>Bone_Stability_CV</th>\n",
              "      <th>Skeletal_Alerts</th>\n",
              "      <th>Worst_Bone</th>\n",
              "      <th>...</th>\n",
              "      <th>Max_Ang_Vel</th>\n",
              "      <th>Mean_Ang_Vel</th>\n",
              "      <th>Max_Lin_Acc</th>\n",
              "      <th>Outlier_Frames</th>\n",
              "      <th>Path_Length_M</th>\n",
              "      <th>Intensity_Index</th>\n",
              "      <th>Pipeline_Status</th>\n",
              "      <th>Quality_Score</th>\n",
              "      <th>Quality_Score_Method</th>\n",
              "      <th>Research_Decision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>763_T2_P2_R2_Take_2025-12-25 10.51.23 AM_005</td>\n",
              "      <td>2026-01-14 13:51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10</td>\n",
              "      <td>83.33</td>\n",
              "      <td>0.376</td>\n",
              "      <td>0</td>\n",
              "      <td>Hips-&gt;Spine</td>\n",
              "      <td>...</td>\n",
              "      <td>1359.12</td>\n",
              "      <td>113.83</td>\n",
              "      <td>44376.02</td>\n",
              "      <td>40</td>\n",
              "      <td>61.28</td>\n",
              "      <td>0.291</td>\n",
              "      <td>PASS</td>\n",
              "      <td>87.91</td>\n",
              "      <td>heuristic_v1</td>\n",
              "      <td>ACCEPT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>734_T1_P2_R1_Take 2025-12-01 02.28.24 PM</td>\n",
              "      <td>2026-01-13 20:35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19617</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10</td>\n",
              "      <td>83.33</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0</td>\n",
              "      <td>Hips-&gt;Spine</td>\n",
              "      <td>...</td>\n",
              "      <td>900.17</td>\n",
              "      <td>44.92</td>\n",
              "      <td>19427.84</td>\n",
              "      <td>0</td>\n",
              "      <td>35.93</td>\n",
              "      <td>0.386</td>\n",
              "      <td>PASS</td>\n",
              "      <td>87.88</td>\n",
              "      <td>heuristic_v1</td>\n",
              "      <td>ACCEPT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>734_T1_P1_R1_Take 2025-12-01 02.18.27 PM</td>\n",
              "      <td>2026-01-19 18:35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30798</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10</td>\n",
              "      <td>83.33</td>\n",
              "      <td>0.408</td>\n",
              "      <td>[Hips-&gt;Spine, Neck-&gt;Head]</td>\n",
              "      <td>Hips-&gt;Spine</td>\n",
              "      <td>...</td>\n",
              "      <td>1026.98</td>\n",
              "      <td>31.98</td>\n",
              "      <td>38536.20</td>\n",
              "      <td>0</td>\n",
              "      <td>25.67</td>\n",
              "      <td>0.084</td>\n",
              "      <td>PASS</td>\n",
              "      <td>87.59</td>\n",
              "      <td>heuristic_v1</td>\n",
              "      <td>REVIEW</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows √ó 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Run_ID   Processing_Date  \\\n",
              "0  763_T2_P2_R2_Take_2025-12-25 10.51.23 AM_005  2026-01-14 13:51   \n",
              "1      734_T1_P2_R1_Take 2025-12-01 02.28.24 PM  2026-01-13 20:35   \n",
              "2      734_T1_P1_R1_Take 2025-12-01 02.18.27 PM  2026-01-19 18:35   \n",
              "\n",
              "   OptiTrack_Error_mm  Total_Frames  Missing_Raw_%  Max_Gap_Frames  \\\n",
              "0                 0.0         17263            0.0              10   \n",
              "1                 0.0         19617            0.0              10   \n",
              "2                 0.0         30798            0.0              10   \n",
              "\n",
              "   Max_Gap_MS  Bone_Stability_CV            Skeletal_Alerts   Worst_Bone  ...  \\\n",
              "0       83.33              0.376                          0  Hips->Spine  ...   \n",
              "1       83.33              0.379                          0  Hips->Spine  ...   \n",
              "2       83.33              0.408  [Hips->Spine, Neck->Head]  Hips->Spine  ...   \n",
              "\n",
              "   Max_Ang_Vel Mean_Ang_Vel  Max_Lin_Acc  Outlier_Frames  Path_Length_M  \\\n",
              "0      1359.12       113.83     44376.02              40          61.28   \n",
              "1       900.17        44.92     19427.84               0          35.93   \n",
              "2      1026.98        31.98     38536.20               0          25.67   \n",
              "\n",
              "   Intensity_Index  Pipeline_Status  Quality_Score  Quality_Score_Method  \\\n",
              "0            0.291             PASS          87.91          heuristic_v1   \n",
              "1            0.386             PASS          87.88          heuristic_v1   \n",
              "2            0.084             PASS          87.59          heuristic_v1   \n",
              "\n",
              "   Research_Decision  \n",
              "0             ACCEPT  \n",
              "1             ACCEPT  \n",
              "2             REVIEW  \n",
              "\n",
              "[3 rows x 25 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Export to Excel (per specification)\n",
        "# ============================================================\n",
        "if not all_summaries:\n",
        "    print(\"‚ùå No complete runs found to aggregate!\")\n",
        "else:\n",
        "    df_master = pd.DataFrame(all_summaries)\n",
        "    df_master = df_master.sort_values('Quality_Score', ascending=False).reset_index(drop=True)\n",
        "    \n",
        "    REPORTS_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
        "    os.makedirs(REPORTS_DIR, exist_ok=True)\n",
        "    excel_path = os.path.join(REPORTS_DIR, f\"Master_Audit_Log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\")\n",
        "    \n",
        "    with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
        "        df_master.to_excel(writer, index=False, sheet_name='Audit_Log')\n",
        "        workbook = writer.book\n",
        "        worksheet = writer.sheets['Audit_Log']\n",
        "        \n",
        "        # Header styling\n",
        "        header_fmt = workbook.add_format({\n",
        "            'bold': True, \n",
        "            'bg_color': '#4472C4', \n",
        "            'font_color': 'white',\n",
        "            'text_wrap': True\n",
        "        })\n",
        "        for col_num, value in enumerate(df_master.columns.values):\n",
        "            worksheet.write(0, col_num, value, header_fmt)\n",
        "        \n",
        "        # Conditional formatting for Research_Decision (green/yellow/red)\n",
        "        red_fmt = workbook.add_format({'bg_color': '#FFC7CE', 'font_color': '#9C0006'})\n",
        "        yellow_fmt = workbook.add_format({'bg_color': '#FFEB9C', 'font_color': '#9C6500'})\n",
        "        green_fmt = workbook.add_format({'bg_color': '#C6EFCE', 'font_color': '#006100'})\n",
        "        \n",
        "        col_idx = df_master.columns.get_loc(\"Research_Decision\")\n",
        "        for row_num in range(1, len(df_master) + 1):\n",
        "            decision = df_master.iloc[row_num-1]['Research_Decision']\n",
        "            if decision == 'ACCEPT':\n",
        "                worksheet.write(row_num, col_idx, decision, green_fmt)\n",
        "            elif decision == 'REVIEW':\n",
        "                worksheet.write(row_num, col_idx, decision, yellow_fmt)\n",
        "            else:\n",
        "                worksheet.write(row_num, col_idx, decision, red_fmt)\n",
        "        \n",
        "        # Auto-fit column widths capped at 40 characters\n",
        "        for i, col in enumerate(df_master.columns):\n",
        "            max_len = max(\n",
        "                df_master[col].astype(str).str.len().max(),\n",
        "                len(str(col))\n",
        "            )\n",
        "            worksheet.set_column(i, i, min(max_len + 2, 40))\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üéâ Master Audit Log Created\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"üìä Total Runs: {len(all_summaries)}\")\n",
        "    print(f\"üíæ File: {excel_path}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    print(\"Decision Summary:\")\n",
        "    print(df_master['Research_Decision'].value_counts())\n",
        "    \n",
        "    print(\"\\nQuality Score Stats:\")\n",
        "    print(f\"  Mean: {df_master['Quality_Score'].mean():.2f}\")\n",
        "    print(f\"  Min:  {df_master['Quality_Score'].min():.2f}\")\n",
        "    print(f\"  Max:  {df_master['Quality_Score'].max():.2f}\")\n",
        "    \n",
        "    print(\"\\nQuality Score Method:\")\n",
        "    print(f\"  {df_master['Quality_Score_Method'].iloc[0] if len(df_master) > 0 else 'N/A'}\")\n",
        "    \n",
        "    print(\"\\nPreview:\")\n",
        "    display(df_master.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc57b635",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5: ISB Compliance & Synchronized Visualization\n",
        "**Purpose:** Visual Proof - ISB-compliant Euler sequences + Interactive time-synced stick figure with LCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53015a5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SECTION 5: ISB Compliance & Synchronized Visualization\n",
        "# ============================================================\n",
        "\n",
        "# Import visualization module\n",
        "from interactive_viz import (\n",
        "    verify_isb_compliance,\n",
        "    create_interactive_synchronized_viz,\n",
        "    create_static_lcs_snapshot\n",
        ")\n",
        "import plotly.io as pio\n",
        "\n",
        "# Define visualization parameters\n",
        "SHOW_LCS_FOR = ['LeftShoulder', 'RightShoulder', 'Hips', 'Spine1']  # Key joints\n",
        "LCS_AXIS_LENGTH = 100.0  # mm\n",
        "SAMPLE_FRAMES = 300  # For performance (full dataset can be slow)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 5: ISB COMPLIANCE & SYNCHRONIZED VISUALIZATION\")\n",
        "print(\"=\"*80)\n",
        "print(\"Purpose: Visual Proof - Verify ISB standards + Interactive time-synced anatomy\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# PART 1: ISB EULER SEQUENCE VERIFICATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"PART 1: ISB Euler Sequence Verification\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "isb_compliance_data = []\n",
        "\n",
        "for run_id, steps in complete_runs.items():\n",
        "    print(f\"\\n{run_id}:\")\n",
        "    \n",
        "    # Path to Euler validation JSON (from notebook 06)\n",
        "    euler_validation_path = os.path.join(\n",
        "        PROJECT_ROOT, \"derivatives\", \"step_06_rotvec\",\n",
        "        f\"{run_id}__euler_validation.json\"\n",
        "    )\n",
        "    \n",
        "    # Verify ISB compliance\n",
        "    df_compliance, summary = verify_isb_compliance(euler_validation_path)\n",
        "    \n",
        "    if df_compliance is not None:\n",
        "        # Display summary\n",
        "        print(f\"  Total Joints: {summary['total_joints']}\")\n",
        "        print(f\"  ‚úÖ Compliant: {summary['compliant_joints']}\")\n",
        "        print(f\"  ‚ö†Ô∏è ROM Violations: {summary['violation_joints']}\")\n",
        "        \n",
        "        if summary['violation_joints'] > 0:\n",
        "            print(f\"  Violated Joints: {', '.join(summary['violated_joints'][:5])}\")\n",
        "            if len(summary['violated_joints']) > 5:\n",
        "                print(f\"    ... and {len(summary['violated_joints']) - 5} more\")\n",
        "        \n",
        "        overall_status = \"‚úÖ PASS\" if summary['overall_status'] == 'PASS' else \"‚ö†Ô∏è REVIEW\"\n",
        "        print(f\"  Overall Status: {overall_status}\")\n",
        "        \n",
        "        # Store for summary table\n",
        "        isb_compliance_data.append({\n",
        "            'Run_ID': run_id,\n",
        "            'Total_Joints': summary['total_joints'],\n",
        "            'Compliant': summary['compliant_joints'],\n",
        "            'ROM_Violations': summary['violation_joints'],\n",
        "            'Overall_Status': overall_status,\n",
        "            'Notes': f\"{summary['violation_joints']} joints exceed anatomical ROM (with Gaga 15% tolerance)\"\n",
        "                     if summary['violation_joints'] > 0 \n",
        "                     else \"All joints within ISB-defined ROM limits\"\n",
        "        })\n",
        "        \n",
        "        # Display detailed compliance table (first 10 joints)\n",
        "        print(\"\\n  ISB Sequence Verification (sample):\")\n",
        "        display(df_compliance.head(10))\n",
        "        \n",
        "    else:\n",
        "        print(f\"  ‚ùå ERROR: {summary.get('error', 'Unknown error')}\")\n",
        "        isb_compliance_data.append({\n",
        "            'Run_ID': run_id,\n",
        "            'Total_Joints': 0,\n",
        "            'Compliant': 0,\n",
        "            'ROM_Violations': 0,\n",
        "            'Overall_Status': '‚ùå NO_DATA',\n",
        "            'Notes': 'Euler validation not available - run notebook 06 first'\n",
        "        })\n",
        "\n",
        "# Create ISB compliance summary table\n",
        "df_isb = pd.DataFrame(isb_compliance_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ISB COMPLIANCE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "display(df_isb)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INTERPRETATION:\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ PASS: All joints use correct ISB sequences and stay within anatomical ROM\")\n",
        "print(\"‚ö†Ô∏è REVIEW: Some joints exceed anatomical ROM (may be valid for Gaga expressive dance)\")\n",
        "print(\"‚ùå NO_DATA: Euler validation not performed - integrate notebook 06 ISB conversion\")\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# PART 2: INTERACTIVE SYNCHRONIZED VISUALIZATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PART 2: Interactive Synchronized Visualization\")\n",
        "print(\"=\"*80)\n",
        "print(\"Creating time-synced stick figure with LCS + kinematic plots...\")\n",
        "print()\n",
        "\n",
        "# Select a run to visualize (use first run with data)\n",
        "visualization_runs = [rid for rid, steps in complete_runs.items() \n",
        "                     if 'step_06' in steps]\n",
        "\n",
        "if len(visualization_runs) > 0:\n",
        "    viz_run_id = visualization_runs[0]\n",
        "    print(f\"Visualizing: {viz_run_id}\")\n",
        "    print()\n",
        "    \n",
        "    # Load kinematic data (from step 06)\n",
        "    kinematics_path = os.path.join(\n",
        "        PROJECT_ROOT, \"derivatives\", \"step_06_rotvec\",\n",
        "        f\"{viz_run_id}__kinematics_full.parquet\"\n",
        "    )\n",
        "    \n",
        "    if os.path.exists(kinematics_path):\n",
        "        print(f\"Loading kinematics: {kinematics_path}\")\n",
        "        df_kin = pd.read_parquet(kinematics_path)\n",
        "        \n",
        "        # Load skeleton hierarchy\n",
        "        hierarchy_path = os.path.join(PROJECT_ROOT, \"config\", \"skeleton_hierarchy.json\")\n",
        "        with open(hierarchy_path) as f:\n",
        "            hierarchy_data = json.load(f)\n",
        "        \n",
        "        bone_hierarchy = [(b['parent'], b['child']) for b in hierarchy_data.get('bones', [])]\n",
        "        joint_names = list(set([b['parent'] for b in hierarchy_data.get('bones', [])] + \n",
        "                               [b['child'] for b in hierarchy_data.get('bones', [])]))\n",
        "        \n",
        "        print(f\"Loaded {len(df_kin)} frames, {len(joint_names)} joints\")\n",
        "        print()\n",
        "        \n",
        "        # ============================================================\n",
        "        # STATIC SNAPSHOT (for documentation/reports)\n",
        "        # ============================================================\n",
        "        print(\"Creating static LCS snapshot (mid-performance frame)...\")\n",
        "        mid_frame = len(df_kin) // 2\n",
        "        \n",
        "        fig_static = create_static_lcs_snapshot(\n",
        "            df=df_kin,\n",
        "            joint_names=joint_names,\n",
        "            bone_hierarchy=bone_hierarchy,\n",
        "            frame_idx=mid_frame,\n",
        "            show_lcs_for=SHOW_LCS_FOR,\n",
        "            axis_length=LCS_AXIS_LENGTH\n",
        "        )\n",
        "        \n",
        "        # Save static figure\n",
        "        static_path = os.path.join(PROJECT_ROOT, \"reports\", \n",
        "                                  f\"{viz_run_id}_lcs_static.html\")\n",
        "        pio.write_html(fig_static, static_path)\n",
        "        print(f\"‚úÖ Static snapshot saved: {static_path}\")\n",
        "        \n",
        "        # Display static figure\n",
        "        fig_static.show()\n",
        "        \n",
        "        print()\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        # ============================================================\n",
        "        # INTERACTIVE SYNCHRONIZED VISUALIZATION (THE BIG ONE)\n",
        "        # ============================================================\n",
        "        print(\"Creating interactive synchronized visualization...\")\n",
        "        print(\"  This includes:\")\n",
        "        print(\"    - 3D skeleton with LCS axes (X/Y/Z arrows)\")\n",
        "        print(\"    - Position plot (X, Y, Z components)\")\n",
        "        print(\"    - Velocity plot (speed magnitude)\")\n",
        "        print(\"    - Shared slider for time synchronization\")\n",
        "        print()\n",
        "        \n",
        "        fig_interactive = create_interactive_synchronized_viz(\n",
        "            df=df_kin,\n",
        "            joint_names=joint_names,\n",
        "            bone_hierarchy=bone_hierarchy,\n",
        "            show_lcs_for=SHOW_LCS_FOR,\n",
        "            axis_length=LCS_AXIS_LENGTH,\n",
        "            sample_frames=SAMPLE_FRAMES\n",
        "        )\n",
        "        \n",
        "        # Save interactive figure\n",
        "        interactive_path = os.path.join(PROJECT_ROOT, \"reports\",\n",
        "                                       f\"{viz_run_id}_interactive_synced.html\")\n",
        "        pio.write_html(fig_interactive, interactive_path)\n",
        "        print(f\"‚úÖ Interactive visualization saved: {interactive_path}\")\n",
        "        print()\n",
        "        \n",
        "        # Display interactive figure\n",
        "        print(\"üìä INTERACTIVE VISUALIZATION:\")\n",
        "        print(\"   ‚Üí Use the slider to move through time\")\n",
        "        print(\"   ‚Üí All three plots update simultaneously\")\n",
        "        print(\"   ‚Üí Verify LCS axes remain stable (no spinning)\")\n",
        "        print(\"   ‚Üí Press ‚ñ∂ Play to animate\")\n",
        "        print()\n",
        "        \n",
        "        fig_interactive.show()\n",
        "        \n",
        "    else:\n",
        "        print(f\"‚ùå ERROR: Kinematics file not found: {kinematics_path}\")\n",
        "        print(\"   Run notebook 06 to generate kinematic derivatives\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No runs with step_06 data available for visualization\")\n",
        "    print(\"   Run notebook 06 (Euler/Omega) first to generate kinematic data\")\n",
        "\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 5 COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ ISB Compliance: Verified joint-specific Euler sequences\")\n",
        "print(\"‚úÖ Visual Proof: Interactive synchronized stick figure with LCS\")\n",
        "print(\"‚úÖ Time-Sync: Slider updates skeleton + kinematic plots simultaneously\")\n",
        "print()\n",
        "print(\"SUPERVISOR INSTRUCTIONS:\")\n",
        "print(\"  1. Check ISB Compliance table - all joints should show correct sequences\")\n",
        "print(\"  2. Use slider to move through performance\")\n",
        "print(\"  3. Verify LCS axes (X/Y/Z arrows) remain stable - no erratic spinning\")\n",
        "print(\"  4. Confirm kinematic plots sync with skeleton movement\")\n",
        "print(\"  5. Look for anomalies: marker swaps, gimbal lock, unnatural motion\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6d058ee",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6: Gaga-Aware Biomechanics\n",
        "**Purpose:** Distinguish \"Intense Dance\" from \"System Error\" (Longo et al., 2022) - Intelligent outlier detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b72246dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SECTION 6: Gaga-Aware Biomechanics\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 6: GAGA-AWARE BIOMECHANICS\")\n",
        "print(\"=\"*80)\n",
        "print(\"Purpose: Distinguish 'Intense Dance' from 'System Error' (Longo et al., 2022)\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# BIOMECHANICAL BENCHMARKS & THRESHOLDS\n",
        "# ============================================================\n",
        "\n",
        "# Literature-based normal gait ranges (Wu et al., 2002; Longo et al., 2022)\n",
        "NORMAL_GAIT_BENCHMARKS = {\n",
        "    # Joint: (mean_rom_deg, std_rom_deg, max_angular_vel_deg_s)\n",
        "    'Shoulder': (120, 30, 300),      # Flexion/Extension\n",
        "    'Elbow': (140, 15, 400),          # Flexion\n",
        "    'Hip': (100, 20, 250),            # Flexion/Extension\n",
        "    'Knee': (130, 20, 400),           # Flexion\n",
        "    'Ankle': (40, 10, 300),           # Dorsi/Plantar flexion\n",
        "    'Spine': (60, 15, 150),           # Flexion/Extension\n",
        "}\n",
        "\n",
        "# Gaga dance tolerance multipliers (from Longo et al., 2022 - expressive dance)\n",
        "GAGA_ROM_MULTIPLIER = 1.5        # Allow 50% more ROM than normal gait\n",
        "GAGA_VELOCITY_MULTIPLIER = 2.0   # Allow 2x angular velocity for dance\n",
        "\n",
        "# Physically impossible thresholds (HARD LIMITS - marker swap indicators)\n",
        "PHYSICALLY_IMPOSSIBLE = {\n",
        "    'Shoulder': {'rom': 200, 'velocity': 1000},   # > 200¬∞ ROM or > 1000¬∞/s = swap\n",
        "    'Elbow': {'rom': 160, 'velocity': 1200},      # Elbow hyper-extension > 160¬∞ impossible\n",
        "    'Hip': {'rom': 180, 'velocity': 800},\n",
        "    'Knee': {'rom': 160, 'velocity': 1000},       # Knee backward flexion impossible\n",
        "    'Ankle': {'rom': 100, 'velocity': 800},\n",
        "    'Spine': {'rom': 120, 'velocity': 500},\n",
        "}\n",
        "\n",
        "# Standard deviation threshold for normal gait outliers\n",
        "NORMAL_GAIT_SD_THRESHOLD = 3.0  # > 3 SD = outlier in normal gait\n",
        "GAGA_SD_THRESHOLD = 5.0         # > 5 SD = outlier in Gaga (more permissive)\n",
        "\n",
        "print(\"Biomechanical Benchmarks Loaded:\")\n",
        "print(f\"  Normal Gait: {len(NORMAL_GAIT_BENCHMARKS)} joint types\")\n",
        "print(f\"  Gaga Tolerance: ROM x{GAGA_ROM_MULTIPLIER}, Velocity x{GAGA_VELOCITY_MULTIPLIER}\")\n",
        "print(f\"  Physically Impossible Limits: {len(PHYSICALLY_IMPOSSIBLE)} joint types\")\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# BUILD GAGA-AWARE BIOMECHANICS TABLE\n",
        "# ============================================================\n",
        "\n",
        "gaga_biomechanics_data = []\n",
        "\n",
        "for run_id, steps in complete_runs.items():\n",
        "    print(f\"\\n{run_id}:\")\n",
        "    \n",
        "    # Load step_06 data (angular velocities, ROM)\n",
        "    if 'step_06' not in steps:\n",
        "        print(\"  ‚ö†Ô∏è No kinematic data (step_06) - skipping biomechanical check\")\n",
        "        gaga_biomechanics_data.append({\n",
        "            'Run_ID': run_id,\n",
        "            'Total_Joints_Checked': 0,\n",
        "            'Normal_Gait_Outliers': 0,\n",
        "            'Gaga_Outliers': 0,\n",
        "            'Physically_Impossible': 0,\n",
        "            'Overall_Status': '‚ùå NO_DATA',\n",
        "            'Notes': 'Kinematic data not available'\n",
        "        })\n",
        "        continue\n",
        "    \n",
        "    step_06 = steps['step_06']\n",
        "    \n",
        "    # Extract angular velocity and ROM statistics\n",
        "    # These should be in the kinematics_summary.json\n",
        "    joint_stats = safe_get(step_06, 'joint_statistics', default={})\n",
        "    \n",
        "    if not joint_stats:\n",
        "        print(\"  ‚ö†Ô∏è No joint statistics in step_06 - kinematic analysis may be incomplete\")\n",
        "        gaga_biomechanics_data.append({\n",
        "            'Run_ID': run_id,\n",
        "            'Total_Joints_Checked': 0,\n",
        "            'Normal_Gait_Outliers': 0,\n",
        "            'Gaga_Outliers': 0,\n",
        "            'Physically_Impossible': 0,\n",
        "            'Overall_Status': '‚ö†Ô∏è INCOMPLETE',\n",
        "            'Notes': 'Joint statistics not computed in pipeline'\n",
        "        })\n",
        "        continue\n",
        "    \n",
        "    # Analyze each joint\n",
        "    normal_gait_outliers = []\n",
        "    gaga_outliers = []\n",
        "    physically_impossible = []\n",
        "    total_joints = 0\n",
        "    \n",
        "    for joint_name, stats in joint_stats.items():\n",
        "        total_joints += 1\n",
        "        \n",
        "        # Extract metrics\n",
        "        max_angular_vel = safe_float(stats.get('max_angular_velocity', 0), default=0)\n",
        "        rom = safe_float(stats.get('rom', 0), default=0)\n",
        "        \n",
        "        # Determine joint type (Shoulder, Elbow, Hip, Knee, etc.)\n",
        "        joint_type = None\n",
        "        for jtype in NORMAL_GAIT_BENCHMARKS.keys():\n",
        "            if jtype in joint_name:\n",
        "                joint_type = jtype\n",
        "                break\n",
        "        \n",
        "        if joint_type is None:\n",
        "            # Unknown joint type - skip\n",
        "            continue\n",
        "        \n",
        "        # Get benchmarks\n",
        "        normal_rom_mean, normal_rom_std, normal_vel_mean = NORMAL_GAIT_BENCHMARKS[joint_type]\n",
        "        impossible_rom = PHYSICALLY_IMPOSSIBLE[joint_type]['rom']\n",
        "        impossible_vel = PHYSICALLY_IMPOSSIBLE[joint_type]['velocity']\n",
        "        \n",
        "        # Gaga-adjusted thresholds\n",
        "        gaga_rom_limit = normal_rom_mean + (GAGA_ROM_MULTIPLIER * normal_rom_std * GAGA_SD_THRESHOLD)\n",
        "        gaga_vel_limit = normal_vel_mean * GAGA_VELOCITY_MULTIPLIER\n",
        "        \n",
        "        # Normal gait thresholds\n",
        "        normal_rom_limit = normal_rom_mean + (NORMAL_GAIT_SD_THRESHOLD * normal_rom_std)\n",
        "        normal_vel_limit = normal_vel_mean\n",
        "        \n",
        "        # ============================================================\n",
        "        # CLASSIFICATION LOGIC (Gaga-Aware)\n",
        "        # ============================================================\n",
        "        \n",
        "        # Check 1: Physically Impossible (CRITICAL - likely marker swap)\n",
        "        if rom > impossible_rom or max_angular_vel > impossible_vel:\n",
        "            physically_impossible.append({\n",
        "                'joint': joint_name,\n",
        "                'rom': rom,\n",
        "                'max_vel': max_angular_vel,\n",
        "                'rom_limit': impossible_rom,\n",
        "                'vel_limit': impossible_vel,\n",
        "                'reason': f\"ROM={rom:.1f}¬∞ or Vel={max_angular_vel:.1f}¬∞/s exceeds physical limits\"\n",
        "            })\n",
        "        \n",
        "        # Check 2: Gaga outlier (intense dance, but physically possible)\n",
        "        elif rom > gaga_rom_limit or max_angular_vel > gaga_vel_limit:\n",
        "            gaga_outliers.append({\n",
        "                'joint': joint_name,\n",
        "                'rom': rom,\n",
        "                'max_vel': max_angular_vel,\n",
        "                'rom_limit': gaga_rom_limit,\n",
        "                'vel_limit': gaga_vel_limit,\n",
        "                'reason': f\"Extreme movement (ROM={rom:.1f}¬∞, Vel={max_angular_vel:.1f}¬∞/s) - intense dance\"\n",
        "            })\n",
        "        \n",
        "        # Check 3: Normal gait outlier (outside normal but within Gaga range)\n",
        "        elif rom > normal_rom_limit or max_angular_vel > normal_vel_limit:\n",
        "            normal_gait_outliers.append({\n",
        "                'joint': joint_name,\n",
        "                'rom': rom,\n",
        "                'max_vel': max_angular_vel,\n",
        "                'rom_limit': normal_rom_limit,\n",
        "                'vel_limit': normal_vel_limit,\n",
        "                'reason': f\"Exceeds normal gait (ROM={rom:.1f}¬∞, Vel={max_angular_vel:.1f}¬∞/s) - typical for dance\"\n",
        "            })\n",
        "    \n",
        "    # ============================================================\n",
        "    # DETERMINE OVERALL STATUS\n",
        "    # ============================================================\n",
        "    \n",
        "    if len(physically_impossible) > 0:\n",
        "        overall_status = 'üî¥ CRITICAL'\n",
        "        notes = f\"{len(physically_impossible)} joint(s) show physically impossible movements - likely marker swap\"\n",
        "    elif len(gaga_outliers) > 0:\n",
        "        overall_status = '‚ö†Ô∏è REVIEW'\n",
        "        notes = f\"{len(gaga_outliers)} joint(s) show extreme movements - intense dance, verify visually\"\n",
        "    elif len(normal_gait_outliers) > 0:\n",
        "        overall_status = '‚úÖ PASS (HIGH_INTENSITY)'\n",
        "        notes = f\"{len(normal_gait_outliers)} joint(s) exceed normal gait - typical for Gaga expressive dance\"\n",
        "    else:\n",
        "        overall_status = '‚úÖ PASS'\n",
        "        notes = \"All joints within normal biomechanical ranges\"\n",
        "    \n",
        "    # Display summary\n",
        "    print(f\"  Total Joints Analyzed: {total_joints}\")\n",
        "    print(f\"  Normal Gait Outliers: {len(normal_gait_outliers)} (typical for dance)\")\n",
        "    print(f\"  Gaga Outliers: {len(gaga_outliers)} (extreme but possible)\")\n",
        "    print(f\"  Physically Impossible: {len(physically_impossible)} (CRITICAL)\")\n",
        "    print(f\"  Overall Status: {overall_status}\")\n",
        "    \n",
        "    # Show critical issues\n",
        "    if len(physically_impossible) > 0:\n",
        "        print(f\"\\n  üî¥ CRITICAL - Physically Impossible Movements:\")\n",
        "        for issue in physically_impossible[:3]:  # Show first 3\n",
        "            print(f\"    {issue['joint']}: {issue['reason']}\")\n",
        "        if len(physically_impossible) > 3:\n",
        "            print(f\"    ... and {len(physically_impossible) - 3} more\")\n",
        "    \n",
        "    # Show extreme movements (Gaga outliers)\n",
        "    if len(gaga_outliers) > 0:\n",
        "        print(f\"\\n  ‚ö†Ô∏è REVIEW - Extreme Movements (Gaga Outliers):\")\n",
        "        for issue in gaga_outliers[:3]:  # Show first 3\n",
        "            print(f\"    {issue['joint']}: {issue['reason']}\")\n",
        "        if len(gaga_outliers) > 3:\n",
        "            print(f\"    ... and {len(gaga_outliers) - 3} more\")\n",
        "    \n",
        "    # Store for summary table\n",
        "    gaga_biomechanics_data.append({\n",
        "        'Run_ID': run_id,\n",
        "        'Total_Joints_Checked': total_joints,\n",
        "        'Normal_Gait_Outliers': len(normal_gait_outliers),\n",
        "        'Gaga_Outliers': len(gaga_outliers),\n",
        "        'Physically_Impossible': len(physically_impossible),\n",
        "        'Overall_Status': overall_status,\n",
        "        'Notes': notes,\n",
        "        'Critical_Joints': ', '.join([x['joint'] for x in physically_impossible[:5]]) if physically_impossible else 'None',\n",
        "        'Extreme_Joints': ', '.join([x['joint'] for x in gaga_outliers[:5]]) if gaga_outliers else 'None'\n",
        "    })\n",
        "\n",
        "# Create Gaga Biomechanics summary table\n",
        "df_gaga = pd.DataFrame(gaga_biomechanics_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GAGA-AWARE BIOMECHANICS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "display(df_gaga)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INTERPRETATION:\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ PASS: All joints within normal biomechanical ranges\")\n",
        "print(\"‚úÖ PASS (HIGH_INTENSITY): Exceeds normal gait but typical for Gaga expressive dance\")\n",
        "print(\"‚ö†Ô∏è REVIEW: Extreme movements detected - visually verify in Section 5 (LCS viz)\")\n",
        "print(\"   ‚Üí Tag as REVIEW, not REJECT - may be valid high-intensity dance\")\n",
        "print(\"üî¥ CRITICAL: Physically impossible movements - likely marker swap or tracking failure\")\n",
        "print(\"   ‚Üí Requires immediate attention - data may be corrupted\")\n",
        "print()\n",
        "\n",
        "# Summary statistics\n",
        "total_runs = len(df_gaga)\n",
        "pass_count = df_gaga['Overall_Status'].str.contains('PASS').sum()\n",
        "review_count = (df_gaga['Overall_Status'] == '‚ö†Ô∏è REVIEW').sum()\n",
        "critical_count = (df_gaga['Overall_Status'] == 'üî¥ CRITICAL').sum()\n",
        "\n",
        "print(f\"Overall Summary:\")\n",
        "print(f\"  Total Runs: {total_runs}\")\n",
        "print(f\"  ‚úÖ Pass (including high-intensity): {pass_count}/{total_runs}\")\n",
        "print(f\"  ‚ö†Ô∏è Review (extreme but possible): {review_count}/{total_runs}\")\n",
        "print(f\"  üî¥ Critical (physically impossible): {critical_count}/{total_runs}\")\n",
        "print()\n",
        "\n",
        "if critical_count > 0:\n",
        "    print(\"‚ö†Ô∏è CRITICAL RUNS DETECTED:\")\n",
        "    critical_runs = df_gaga[df_gaga['Overall_Status'] == 'üî¥ CRITICAL']\n",
        "    for idx, row in critical_runs.iterrows():\n",
        "        print(f\"  {row['Run_ID']}\")\n",
        "        print(f\"    Critical Joints: {row['Critical_Joints']}\")\n",
        "        print(f\"    ‚Üí ACTION: Check Section 5 visualization for marker swaps\")\n",
        "        print(f\"    ‚Üí DECISION: Tag as REVIEW or REJECT based on visual inspection\")\n",
        "        print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SCIENTIFIC RATIONALE:\")\n",
        "print(\"=\"*80)\n",
        "print(\"Per Longo et al. (2022): Expressive dance movements exceed normal gait ranges.\")\n",
        "print(\"  ‚Üí We use GAGA-AWARE THRESHOLDS (1.5x ROM, 2x velocity) to avoid false rejections\")\n",
        "print()\n",
        "print(\"Per Wu et al. (2002): Joint-specific anatomical limits are physical constraints.\")\n",
        "print(\"  ‚Üí Movements exceeding these limits indicate marker swap or system error\")\n",
        "print()\n",
        "print(\"DECISION LOGIC:\")\n",
        "print(\"  ‚Ä¢ Normal gait outlier ‚Üí PASS (expected in dance)\")\n",
        "print(\"  ‚Ä¢ Gaga outlier ‚Üí REVIEW (extreme dance, verify visually)\")\n",
        "print(\"  ‚Ä¢ Physically impossible ‚Üí CRITICAL (likely system error)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 6 COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ Gaga-Aware Biomechanics: Intelligent outlier detection\")\n",
        "print(\"‚úÖ Expressive Dance Protection: High-intensity movements tagged as REVIEW, not REJECT\")\n",
        "print(\"‚úÖ System Error Detection: Physically impossible movements flagged as CRITICAL\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "959ee12a",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 7: Signal-to-Noise Ratio (SNR) Quantification\n",
        "**Purpose:** Measure signal health per joint (Cereatti et al., 2024) - Detect occlusion patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd84077b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SECTION 7: Signal-to-Noise Ratio (SNR) Quantification\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 7: SIGNAL-TO-NOISE RATIO (SNR) QUANTIFICATION\")\n",
        "print(\"=\"*80)\n",
        "print(\"Purpose: Measure signal health per joint (Cereatti et al., 2024)\")\n",
        "print(\"Formula: SNR = 10 * log10(Power_Filtered_Signal / Power_Residuals)\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# SNR THRESHOLDS (Cereatti et al., 2024)\n",
        "# ============================================================\n",
        "\n",
        "SNR_THRESHOLDS = {\n",
        "    'excellent': 30,  # dB - Research grade\n",
        "    'good': 20,       # dB - Clinical acceptable\n",
        "    'acceptable': 15, # dB - Minimum for analysis\n",
        "    'poor': 10,       # dB - Questionable quality\n",
        "    'reject': 0       # dB - Below 10 dB = reject\n",
        "}\n",
        "\n",
        "print(\"SNR Quality Thresholds (Cereatti et al., 2024):\")\n",
        "print(f\"  ‚≠ê Excellent: ‚â• {SNR_THRESHOLDS['excellent']} dB (Research grade)\")\n",
        "print(f\"  ‚úÖ Good: ‚â• {SNR_THRESHOLDS['good']} dB (Clinical acceptable)\")\n",
        "print(f\"  ‚ö†Ô∏è Acceptable: ‚â• {SNR_THRESHOLDS['acceptable']} dB (Minimum for analysis)\")\n",
        "print(f\"  üî¥ Poor: ‚â• {SNR_THRESHOLDS['poor']} dB (Questionable quality)\")\n",
        "print(f\"  ‚ùå Reject: < {SNR_THRESHOLDS['poor']} dB (Unacceptable)\")\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# BUILD SNR ANALYSIS TABLE\n",
        "# ============================================================\n",
        "\n",
        "snr_analysis_data = []\n",
        "\n",
        "for run_id, steps in complete_runs.items():\n",
        "    print(f\"\\n{run_id}:\")\n",
        "    \n",
        "    # Load step_04 filtering data (contains residuals)\n",
        "    if 'step_04' not in steps:\n",
        "        print(\"  ‚ö†Ô∏è No filtering data (step_04) - skipping SNR analysis\")\n",
        "        snr_analysis_data.append({\n",
        "            'Run_ID': run_id,\n",
        "            'Mean_SNR_dB': 0.0,\n",
        "            'Min_SNR_dB': 0.0,\n",
        "            'Joints_Below_15dB': 0,\n",
        "            'Overall_Status': '‚ùå NO_DATA',\n",
        "            'Notes': 'Filtering data not available'\n",
        "        })\n",
        "        continue\n",
        "    \n",
        "    step_04 = steps['step_04']\n",
        "    \n",
        "    # Check for SNR analysis in filtering summary\n",
        "    snr_data = safe_get(step_04, 'snr_analysis', default={})\n",
        "    \n",
        "    if not snr_data or 'per_joint' not in snr_data:\n",
        "        print(\"  ‚ö†Ô∏è No SNR analysis in step_04 - filtering may be from older pipeline\")\n",
        "        snr_analysis_data.append({\n",
        "            'Run_ID': run_id,\n",
        "            'Mean_SNR_dB': 0.0,\n",
        "            'Min_SNR_dB': 0.0,\n",
        "            'Joints_Below_15dB': 0,\n",
        "            'Overall_Status': '‚ö†Ô∏è INCOMPLETE',\n",
        "            'Notes': 'SNR not computed in filtering step - enhance notebook 04'\n",
        "        })\n",
        "        continue\n",
        "    \n",
        "    # Extract per-joint SNR\n",
        "    per_joint_snr = snr_data.get('per_joint', {})\n",
        "    summary_snr = snr_data.get('summary', {})\n",
        "    \n",
        "    if not per_joint_snr:\n",
        "        print(\"  ‚ö†Ô∏è Empty SNR data\")\n",
        "        snr_analysis_data.append({\n",
        "            'Run_ID': run_id,\n",
        "            'Mean_SNR_dB': 0.0,\n",
        "            'Min_SNR_dB': 0.0,\n",
        "            'Joints_Below_15dB': 0,\n",
        "            'Overall_Status': '‚ö†Ô∏è INCOMPLETE',\n",
        "            'Notes': 'SNR data empty'\n",
        "        })\n",
        "        continue\n",
        "    \n",
        "    # Compute statistics\n",
        "    snr_values = [v['snr_db'] for v in per_joint_snr.values() if 'snr_db' in v]\n",
        "    \n",
        "    if len(snr_values) == 0:\n",
        "        print(\"  ‚ö†Ô∏è No valid SNR values\")\n",
        "        snr_analysis_data.append({\n",
        "            'Run_ID': run_id,\n",
        "            'Mean_SNR_dB': 0.0,\n",
        "            'Min_SNR_dB': 0.0,\n",
        "            'Joints_Below_15dB': 0,\n",
        "            'Overall_Status': '‚ö†Ô∏è INCOMPLETE',\n",
        "            'Notes': 'No valid SNR values'\n",
        "        })\n",
        "        continue\n",
        "    \n",
        "    mean_snr = float(np.mean(snr_values))\n",
        "    min_snr = float(np.min(snr_values))\n",
        "    max_snr = float(np.max(snr_values))\n",
        "    \n",
        "    # Count joints below key thresholds\n",
        "    below_15db = sum(1 for v in snr_values if v < 15.0)\n",
        "    below_10db = sum(1 for v in snr_values if v < 10.0)\n",
        "    \n",
        "    # ============================================================\n",
        "    # OCCLUSION PATTERN DETECTION (Cereatti et al., 2024)\n",
        "    # ============================================================\n",
        "    \n",
        "    # Check for spine vs. limb SNR patterns\n",
        "    # Low spine SNR + high limb SNR = torso marker occlusion\n",
        "    spine_joints = [j for j in per_joint_snr.keys() if 'Spine' in j or 'Neck' in j or 'Head' in j]\n",
        "    limb_joints = [j for j in per_joint_snr.keys() if any(x in j for x in ['Arm', 'Leg', 'Hand', 'Foot'])]\n",
        "    \n",
        "    spine_snr = [per_joint_snr[j]['snr_db'] for j in spine_joints if j in per_joint_snr and 'snr_db' in per_joint_snr[j]]\n",
        "    limb_snr = [per_joint_snr[j]['snr_db'] for j in limb_joints if j in per_joint_snr and 'snr_db' in per_joint_snr[j]]\n",
        "    \n",
        "    mean_spine_snr = float(np.mean(spine_snr)) if spine_snr else 0.0\n",
        "    mean_limb_snr = float(np.mean(limb_snr)) if limb_snr else 0.0\n",
        "    \n",
        "    # Detect occlusion pattern: spine SNR < 15 dB AND limb SNR > 20 dB\n",
        "    occlusion_detected = (mean_spine_snr < 15.0 and mean_limb_snr > 20.0)\n",
        "    \n",
        "    # ============================================================\n",
        "    # OVERALL STATUS CLASSIFICATION\n",
        "    # ============================================================\n",
        "    \n",
        "    if below_10db > 0:\n",
        "        overall_status = '‚ùå REJECT'\n",
        "        notes = f\"{below_10db} joint(s) below 10 dB - unacceptable signal quality\"\n",
        "    elif below_15db > 0:\n",
        "        overall_status = '‚ö†Ô∏è REVIEW'\n",
        "        notes = f\"{below_15db} joint(s) below 15 dB - marginal signal quality\"\n",
        "    elif mean_snr < 20.0:\n",
        "        overall_status = '‚úÖ ACCEPTABLE'\n",
        "        notes = f\"Mean SNR {mean_snr:.1f} dB - acceptable for analysis\"\n",
        "    elif mean_snr < 30.0:\n",
        "        overall_status = '‚úÖ GOOD'\n",
        "        notes = f\"Mean SNR {mean_snr:.1f} dB - good signal quality\"\n",
        "    else:\n",
        "        overall_status = '‚≠ê EXCELLENT'\n",
        "        notes = f\"Mean SNR {mean_snr:.1f} dB - research grade signal quality\"\n",
        "    \n",
        "    # Add occlusion pattern note\n",
        "    if occlusion_detected:\n",
        "        notes += f\" | üö® OCCLUSION: Spine SNR ({mean_spine_snr:.1f} dB) << Limb SNR ({mean_limb_snr:.1f} dB) - torso markers occluded\"\n",
        "    \n",
        "    # Display summary\n",
        "    print(f\"  Total Joints Analyzed: {len(snr_values)}\")\n",
        "    print(f\"  Mean SNR: {mean_snr:.1f} dB\")\n",
        "    print(f\"  Min SNR: {min_snr:.1f} dB\")\n",
        "    print(f\"  Max SNR: {max_snr:.1f} dB\")\n",
        "    print(f\"  Joints < 15 dB: {below_15db}\")\n",
        "    print(f\"  Joints < 10 dB: {below_10db}\")\n",
        "    \n",
        "    if occlusion_detected:\n",
        "        print(f\"\\n  üö® OCCLUSION PATTERN DETECTED:\")\n",
        "        print(f\"    Spine Mean SNR: {mean_spine_snr:.1f} dB (low)\")\n",
        "        print(f\"    Limb Mean SNR: {mean_limb_snr:.1f} dB (high)\")\n",
        "        print(f\"    ‚Üí Indicates torso marker occlusion during performance\")\n",
        "    \n",
        "    print(f\"  Overall Status: {overall_status}\")\n",
        "    \n",
        "    # Show worst joints (lowest SNR)\n",
        "    worst_joints = sorted(per_joint_snr.items(), key=lambda x: x[1].get('snr_db', 0))[:5]\n",
        "    \n",
        "    if worst_joints:\n",
        "        print(f\"\\n  Worst Signal Quality (Top 5):\")\n",
        "        for joint, data in worst_joints:\n",
        "            snr_val = data.get('snr_db', 0)\n",
        "            quality = data.get('quality', 'unknown')\n",
        "            print(f\"    {joint}: {snr_val:.1f} dB ({quality})\")\n",
        "    \n",
        "    # Store for summary table\n",
        "    snr_analysis_data.append({\n",
        "        'Run_ID': run_id,\n",
        "        'Mean_SNR_dB': round(mean_snr, 1),\n",
        "        'Min_SNR_dB': round(min_snr, 1),\n",
        "        'Max_SNR_dB': round(max_snr, 1),\n",
        "        'Joints_Below_15dB': below_15db,\n",
        "        'Joints_Below_10dB': below_10db,\n",
        "        'Spine_Mean_SNR': round(mean_spine_snr, 1),\n",
        "        'Limb_Mean_SNR': round(mean_limb_snr, 1),\n",
        "        'Occlusion_Detected': 'üö® YES' if occlusion_detected else 'No',\n",
        "        'Overall_Status': overall_status,\n",
        "        'Notes': notes\n",
        "    })\n",
        "\n",
        "# Create SNR summary table\n",
        "df_snr = pd.DataFrame(snr_analysis_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SNR ANALYSIS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "display(df_snr)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INTERPRETATION:\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚≠ê EXCELLENT (‚â•30 dB): Research-grade signal quality\")\n",
        "print(\"‚úÖ GOOD (‚â•20 dB): Clinical-acceptable signal quality\")\n",
        "print(\"‚úÖ ACCEPTABLE (‚â•15 dB): Minimum threshold for biomechanical analysis\")\n",
        "print(\"‚ö†Ô∏è REVIEW (<15 dB): Marginal signal quality - verify results carefully\")\n",
        "print(\"‚ùå REJECT (<10 dB): Unacceptable signal quality - data unreliable\")\n",
        "print()\n",
        "print(\"üö® OCCLUSION PATTERN: Low spine SNR + high limb SNR = torso marker occlusion\")\n",
        "print(\"   ‚Üí Check if dancer's torso was blocked by camera angles or other objects\")\n",
        "print(\"   ‚Üí May affect COM (center of mass) calculations and trunk kinematics\")\n",
        "print()\n",
        "\n",
        "# Summary statistics\n",
        "total_runs = len(df_snr)\n",
        "excellent_count = (df_snr['Overall_Status'] == '‚≠ê EXCELLENT').sum()\n",
        "good_count = (df_snr['Overall_Status'] == '‚úÖ GOOD').sum()\n",
        "acceptable_count = (df_snr['Overall_Status'] == '‚úÖ ACCEPTABLE').sum()\n",
        "review_count = (df_snr['Overall_Status'] == '‚ö†Ô∏è REVIEW').sum()\n",
        "reject_count = (df_snr['Overall_Status'] == '‚ùå REJECT').sum()\n",
        "occlusion_count = (df_snr['Occlusion_Detected'] == 'üö® YES').sum()\n",
        "\n",
        "print(f\"Overall Summary:\")\n",
        "print(f\"  Total Runs: {total_runs}\")\n",
        "print(f\"  ‚≠ê Excellent: {excellent_count}/{total_runs}\")\n",
        "print(f\"  ‚úÖ Good: {good_count}/{total_runs}\")\n",
        "print(f\"  ‚úÖ Acceptable: {acceptable_count}/{total_runs}\")\n",
        "print(f\"  ‚ö†Ô∏è Review: {review_count}/{total_runs}\")\n",
        "print(f\"  ‚ùå Reject: {reject_count}/{total_runs}\")\n",
        "print(f\"  üö® Occlusion Detected: {occlusion_count}/{total_runs}\")\n",
        "print()\n",
        "\n",
        "if occlusion_count > 0:\n",
        "    print(\"‚ö†Ô∏è RUNS WITH OCCLUSION PATTERNS:\")\n",
        "    occluded_runs = df_snr[df_snr['Occlusion_Detected'] == 'üö® YES']\n",
        "    for idx, row in occluded_runs.iterrows():\n",
        "        print(f\"  {row['Run_ID']}\")\n",
        "        print(f\"    Spine SNR: {row['Spine_Mean_SNR']} dB | Limb SNR: {row['Limb_Mean_SNR']} dB\")\n",
        "        print(f\"    ‚Üí ACTION: Review camera setup for torso visibility\")\n",
        "        print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SCIENTIFIC RATIONALE:\")\n",
        "print(\"=\"*80)\n",
        "print(\"Per Cereatti et al. (2024): SNR quantifies signal health objectively.\")\n",
        "print(\"  Formula: SNR(dB) = 10 * log10(Power_Signal / Power_Residuals)\")\n",
        "print(\"  ‚Üí Higher SNR = cleaner signal, more reliable derivatives\")\n",
        "print()\n",
        "print(\"Power Calculation:\")\n",
        "print(\"  Power = RMS¬≤ = (1/N) * Œ£(x¬≤)\")\n",
        "print(\"  ‚Üí Signal power: RMS of filtered signal\")\n",
        "print(\"  ‚Üí Noise power: RMS of residuals (signal - filtered)\")\n",
        "print()\n",
        "print(\"Occlusion Detection:\")\n",
        "print(\"  Low spine SNR + High limb SNR = Differential tracking quality\")\n",
        "print(\"  ‚Üí Suggests torso markers were occluded while limb markers were visible\")\n",
        "print(\"  ‚Üí Common in dance: body blocking torso from certain camera angles\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 7 COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ SNR Analysis: Objective signal health quantification\")\n",
        "print(\"‚úÖ Occlusion Detection: Identifies torso vs. limb tracking patterns\")\n",
        "print(\"‚úÖ Quality Thresholds: Research-grade classification (Cereatti 2024)\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32875f31",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 8: The Decision Matrix\n",
        "**Purpose:** Final verdict combining all QC metrics - **ACCEPT / REVIEW / REJECT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4439954",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SECTION 8: THE DECISION MATRIX\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 8: THE DECISION MATRIX\")\n",
        "print(\"=\"*80)\n",
        "print(\"Purpose: Final verdict combining all QC metrics\")\n",
        "print(\"States: ACCEPT (‚úÖ), REVIEW (‚ö†Ô∏è), REJECT (‚ùå)\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# QUALITY SCORE WEIGHTS (Cereatti et al., 2024)\n",
        "# ============================================================\n",
        "\n",
        "WEIGHTS = {\n",
        "    'calibration': 0.15,      # Pointer/wand accuracy (R√°cz 2025)\n",
        "    'bone_stability': 0.20,   # Rigid-body integrity (CV%)\n",
        "    'temporal_quality': 0.10, # Sample time jitter\n",
        "    'interpolation': 0.15,    # Gap filling quality\n",
        "    'filtering': 0.10,        # Winter's residual analysis\n",
        "    'snr': 0.20,              # Signal-to-noise ratio (Cereatti 2024)\n",
        "    'biomechanics': 0.10      # Gaga-aware outlier detection\n",
        "}\n",
        "\n",
        "print(\"Quality Score Weights:\")\n",
        "for key, weight in WEIGHTS.items():\n",
        "    print(f\"  {key.replace('_', ' ').title()}: {weight*100:.0f}%\")\n",
        "print()\n",
        "\n",
        "# Verify weights sum to 1.0\n",
        "total_weight = sum(WEIGHTS.values())\n",
        "assert abs(total_weight - 1.0) < 0.01, f\"Weights must sum to 1.0, got {total_weight}\"\n",
        "\n",
        "# ============================================================\n",
        "# SECTION SCORE NORMALIZATION FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def score_calibration(section1_status):\n",
        "    \"\"\"Score calibration quality (0-100)\"\"\"\n",
        "    if section1_status == '‚úÖ PASS':\n",
        "        return 100.0\n",
        "    elif section1_status == '‚ö†Ô∏è REVIEW':\n",
        "        return 70.0\n",
        "    elif section1_status == '‚ùå FAIL':\n",
        "        return 30.0\n",
        "    else:\n",
        "        return 50.0  # Unknown/missing\n",
        "\n",
        "def score_bone_stability(bone_cv_percent):\n",
        "    \"\"\"Score bone stability based on CV% (0-100)\"\"\"\n",
        "    if bone_cv_percent is None or bone_cv_percent == 'N/A':\n",
        "        return 50.0\n",
        "    try:\n",
        "        cv = float(bone_cv_percent)\n",
        "        if cv <= 0.5:\n",
        "            return 100.0\n",
        "        elif cv <= 1.0:\n",
        "            return 90.0\n",
        "        elif cv <= 1.5:\n",
        "            return 70.0  # Threshold for acceptance\n",
        "        elif cv <= 2.0:\n",
        "            return 50.0\n",
        "        else:\n",
        "            return 20.0  # Poor stability\n",
        "    except:\n",
        "        return 50.0\n",
        "\n",
        "def score_temporal_quality(time_jitter_sec):\n",
        "    \"\"\"Score temporal quality based on jitter (0-100)\"\"\"\n",
        "    if time_jitter_sec is None or time_jitter_sec == 'N/A':\n",
        "        return 50.0\n",
        "    try:\n",
        "        jitter_ms = float(time_jitter_sec) * 1000  # Convert to ms\n",
        "        if jitter_ms <= 0.1:\n",
        "            return 100.0\n",
        "        elif jitter_ms <= 0.5:\n",
        "            return 90.0\n",
        "        elif jitter_ms <= 1.0:\n",
        "            return 70.0\n",
        "        elif jitter_ms <= 2.0:\n",
        "            return 50.0\n",
        "        else:\n",
        "            return 20.0\n",
        "    except:\n",
        "        return 50.0\n",
        "\n",
        "def score_interpolation(raw_missing_percent, method_category):\n",
        "    \"\"\"Score interpolation quality (0-100)\"\"\"\n",
        "    if raw_missing_percent is None or raw_missing_percent == 'N/A':\n",
        "        return 50.0\n",
        "    try:\n",
        "        missing = float(raw_missing_percent)\n",
        "        \n",
        "        # Base score from gap percentage\n",
        "        if missing == 0:\n",
        "            base_score = 100.0\n",
        "        elif missing <= 1.0:\n",
        "            base_score = 95.0\n",
        "        elif missing <= 5.0:\n",
        "            base_score = 80.0\n",
        "        elif missing <= 10.0:\n",
        "            base_score = 60.0\n",
        "        else:\n",
        "            base_score = 30.0\n",
        "        \n",
        "        # Penalty for linear fallback\n",
        "        if 'üü† Linear Fallback' in str(method_category):\n",
        "            base_score *= 0.85  # 15% penalty\n",
        "        \n",
        "        return base_score\n",
        "    except:\n",
        "        return 50.0\n",
        "\n",
        "def score_filtering(winter_status):\n",
        "    \"\"\"Score filtering quality (0-100)\"\"\"\n",
        "    if winter_status == '‚úÖ PASS':\n",
        "        return 100.0\n",
        "    elif winter_status == '‚ö†Ô∏è ARBITRARY':\n",
        "        return 70.0\n",
        "    elif winter_status == '‚ùå FAIL':\n",
        "        return 30.0\n",
        "    else:\n",
        "        return 50.0\n",
        "\n",
        "def score_snr(mean_snr_db):\n",
        "    \"\"\"Score SNR quality (0-100)\"\"\"\n",
        "    if mean_snr_db is None or mean_snr_db == 'N/A' or mean_snr_db == 0.0:\n",
        "        return 50.0\n",
        "    try:\n",
        "        snr = float(mean_snr_db)\n",
        "        if snr >= 30.0:\n",
        "            return 100.0  # Excellent\n",
        "        elif snr >= 20.0:\n",
        "            return 85.0   # Good\n",
        "        elif snr >= 15.0:\n",
        "            return 70.0   # Acceptable\n",
        "        elif snr >= 10.0:\n",
        "            return 40.0   # Poor\n",
        "        else:\n",
        "            return 10.0   # Reject\n",
        "    except:\n",
        "        return 50.0\n",
        "\n",
        "def score_biomechanics(biomech_status):\n",
        "    \"\"\"Score biomechanics quality (0-100)\"\"\"\n",
        "    if biomech_status == '‚úÖ PASS':\n",
        "        return 100.0\n",
        "    elif biomech_status == '‚úÖ PASS (HIGH_INTENSITY)':\n",
        "        return 95.0  # Gaga-aware: high intensity is acceptable\n",
        "    elif biomech_status == '‚ö†Ô∏è REVIEW':\n",
        "        return 60.0\n",
        "    elif biomech_status == 'üî¥ CRITICAL':\n",
        "        return 10.0\n",
        "    else:\n",
        "        return 50.0\n",
        "\n",
        "# ============================================================\n",
        "# BUILD DECISION MATRIX\n",
        "# ============================================================\n",
        "\n",
        "decision_matrix_data = []\n",
        "\n",
        "for run_id, steps in complete_runs.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"DECISION MATRIX: {run_id}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # EXTRACT METRICS FROM ALL SECTIONS\n",
        "    # ============================================================\n",
        "    \n",
        "    # Section 0: Data Lineage\n",
        "    section0 = next((row for row in section0_data if row['Run_ID'] == run_id), {})\n",
        "    integrity_status = section0.get('Integrity_Status', 'N/A')\n",
        "    \n",
        "    # Section 1: Calibration\n",
        "    section1 = next((row for row in calibration_data if row['Run_ID'] == run_id), {})\n",
        "    calibration_status = section1.get('Calibration_Status', 'N/A')\n",
        "    \n",
        "    # Section 2: Rigid-Body\n",
        "    section2 = next((row for row in rigidbody_data if row['Run_ID'] == run_id), {})\n",
        "    bone_cv = section2.get('Bone_CV_%', 'N/A')\n",
        "    time_jitter = section2.get('Time_Jitter_sec', 'N/A')\n",
        "    \n",
        "    # Section 3: Interpolation\n",
        "    section3 = next((row for row in interpolation_data if row['Run_ID'] == run_id), {})\n",
        "    raw_missing = section3.get('Raw_Missing_%', 'N/A')\n",
        "    method_category = section3.get('Method_Category', 'N/A')\n",
        "    \n",
        "    # Section 4: Filtering\n",
        "    section4 = next((row for row in filtering_data if row['Run_ID'] == run_id), {})\n",
        "    winter_status = section4.get('Winter_Status', 'N/A')\n",
        "    \n",
        "    # Section 5: ISB Compliance (not in scoring, but checked for violations)\n",
        "    section5 = next((row for row in isb_compliance_data if row['Run_ID'] == run_id), {})\n",
        "    isb_violations = section5.get('ROM_Violations', 0)\n",
        "    \n",
        "    # Section 6: Gaga Biomechanics\n",
        "    section6 = next((row for row in gaga_biomechanics_data if row['Run_ID'] == run_id), {})\n",
        "    biomech_status = section6.get('Overall_Status', 'N/A')\n",
        "    \n",
        "    # Section 7: SNR\n",
        "    section7 = next((row for row in snr_analysis_data if row['Run_ID'] == run_id), {})\n",
        "    mean_snr = section7.get('Mean_SNR_dB', 'N/A')\n",
        "    joints_below_10db = section7.get('Joints_Below_10dB', 0)\n",
        "    occlusion_detected = section7.get('Occlusion_Detected', 'No')\n",
        "    \n",
        "    # ============================================================\n",
        "    # COMPUTE QUALITY SCORE\n",
        "    # ============================================================\n",
        "    \n",
        "    scores = {\n",
        "        'calibration': score_calibration(calibration_status),\n",
        "        'bone_stability': score_bone_stability(bone_cv),\n",
        "        'temporal_quality': score_temporal_quality(time_jitter),\n",
        "        'interpolation': score_interpolation(raw_missing, method_category),\n",
        "        'filtering': score_filtering(winter_status),\n",
        "        'snr': score_snr(mean_snr),\n",
        "        'biomechanics': score_biomechanics(biomech_status)\n",
        "    }\n",
        "    \n",
        "    # Weighted quality score (0-100)\n",
        "    quality_score = sum(scores[key] * WEIGHTS[key] for key in WEIGHTS.keys())\n",
        "    \n",
        "    print(\"\\nComponent Scores (0-100):\")\n",
        "    for key, score in scores.items():\n",
        "        weight_pct = WEIGHTS[key] * 100\n",
        "        contribution = score * WEIGHTS[key]\n",
        "        print(f\"  {key.replace('_', ' ').title()}: {score:.1f} √ó {weight_pct:.0f}% = {contribution:.1f}\")\n",
        "    \n",
        "    print(f\"\\nüìä WEIGHTED QUALITY SCORE: {quality_score:.1f} / 100\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # DECISION LOGIC (REJECT ‚Üí REVIEW ‚Üí ACCEPT)\n",
        "    # ============================================================\n",
        "    \n",
        "    decision = None\n",
        "    decision_reason = []\n",
        "    category = None\n",
        "    \n",
        "    # ============================================================\n",
        "    # CRITICAL FAILURES (REJECT)\n",
        "    # ============================================================\n",
        "    \n",
        "    # Data integrity failure\n",
        "    if integrity_status == '‚ùå MISMATCH':\n",
        "        decision = '‚ùå REJECT'\n",
        "        category = 'Data Integrity'\n",
        "        decision_reason.append(\"Data hash mismatch - file modified after processing\")\n",
        "    \n",
        "    # Calibration failure\n",
        "    if calibration_status == '‚ùå FAIL' and decision is None:\n",
        "        decision = '‚ùå REJECT'\n",
        "        category = 'Calibration'\n",
        "        pointer_error = section1.get('Pointer_Error_mm', 'N/A')\n",
        "        decision_reason.append(f\"Pointer calibration error ({pointer_error} mm) exceeds threshold\")\n",
        "    \n",
        "    # Bone stability failure\n",
        "    if bone_cv != 'N/A' and float(bone_cv) > 2.0 and decision is None:\n",
        "        decision = '‚ùå REJECT'\n",
        "        category = 'Rigid-Body Integrity'\n",
        "        worst_bone = section2.get('Worst_Bone', 'Unknown')\n",
        "        decision_reason.append(f\"Bone_Stability_CV ({bone_cv}%) > threshold (2.0%) on {worst_bone} - marker tracking failure\")\n",
        "    \n",
        "    # SNR failure (joints below 10 dB)\n",
        "    if joints_below_10db > 0 and decision is None:\n",
        "        decision = '‚ùå REJECT'\n",
        "        category = 'Signal Quality'\n",
        "        decision_reason.append(f\"SNR failure: {joints_below_10db} joint(s) below 10 dB - unacceptable signal quality\")\n",
        "    \n",
        "    # Biomechanics critical (physically impossible)\n",
        "    if biomech_status == 'üî¥ CRITICAL' and decision is None:\n",
        "        decision = '‚ùå REJECT'\n",
        "        category = 'Biomechanical Validity'\n",
        "        decision_reason.append(\"Physically impossible joint angles detected - likely marker swap\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # REVIEW FLAGS (REVIEW)\n",
        "    # ============================================================\n",
        "    \n",
        "    if decision is None:\n",
        "        review_flags = []\n",
        "        \n",
        "        # Calibration review\n",
        "        if calibration_status == '‚ö†Ô∏è REVIEW':\n",
        "            review_flags.append(\"Calibration marginal\")\n",
        "        \n",
        "        # Bone stability review\n",
        "        if bone_cv != 'N/A' and float(bone_cv) > 1.5:\n",
        "            review_flags.append(f\"Bone CV ({bone_cv}%) above ideal (1.5%)\")\n",
        "        \n",
        "        # SNR review\n",
        "        if mean_snr != 'N/A' and float(mean_snr) < 15.0:\n",
        "            review_flags.append(f\"Mean SNR ({mean_snr} dB) below minimum (15 dB)\")\n",
        "        \n",
        "        # Occlusion pattern\n",
        "        if occlusion_detected == 'üö® YES':\n",
        "            review_flags.append(\"Torso marker occlusion detected - trunk kinematics unreliable\")\n",
        "        \n",
        "        # ISB violations (anatomical ROM exceeded)\n",
        "        if isb_violations > 0:\n",
        "            review_flags.append(f\"{isb_violations} joint(s) exceeded anatomical ROM limits\")\n",
        "        \n",
        "        # Gaga high intensity\n",
        "        if biomech_status == '‚ö†Ô∏è REVIEW':\n",
        "            review_flags.append(\"Extreme angular velocities - verify if dance or tracking error\")\n",
        "        \n",
        "        # Linear fallback\n",
        "        if 'üü† Linear Fallback' in str(method_category):\n",
        "            review_flags.append(\"Linear interpolation fallback used - velocity accuracy reduced\")\n",
        "        \n",
        "        # Winter arbitrary\n",
        "        if winter_status == '‚ö†Ô∏è ARBITRARY':\n",
        "            review_flags.append(\"Filter cutoff frequency arbitrary - knee point not found\")\n",
        "        \n",
        "        # If any review flags, mark as REVIEW\n",
        "        if len(review_flags) > 0:\n",
        "            decision = '‚ö†Ô∏è REVIEW'\n",
        "            category = 'Quality Flags'\n",
        "            decision_reason.extend(review_flags)\n",
        "    \n",
        "    # ============================================================\n",
        "    # ACCEPTANCE (ACCEPT)\n",
        "    # ============================================================\n",
        "    \n",
        "    if decision is None:\n",
        "        # Quality score threshold for acceptance\n",
        "        if quality_score >= 80.0:\n",
        "            decision = '‚úÖ ACCEPT (EXCELLENT)'\n",
        "            category = 'Quality Score'\n",
        "            decision_reason.append(f\"Quality score {quality_score:.1f}/100 - excellent data quality\")\n",
        "        elif quality_score >= 70.0:\n",
        "            decision = '‚úÖ ACCEPT (GOOD)'\n",
        "            category = 'Quality Score'\n",
        "            decision_reason.append(f\"Quality score {quality_score:.1f}/100 - good data quality\")\n",
        "        elif quality_score >= 60.0:\n",
        "            decision = '‚úÖ ACCEPT'\n",
        "            category = 'Quality Score'\n",
        "            decision_reason.append(f\"Quality score {quality_score:.1f}/100 - acceptable data quality\")\n",
        "        else:\n",
        "            decision = '‚ö†Ô∏è REVIEW'\n",
        "            category = 'Quality Score'\n",
        "            decision_reason.append(f\"Quality score {quality_score:.1f}/100 - below ideal threshold (60)\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # FORMAT DECISION REASON\n",
        "    # ============================================================\n",
        "    \n",
        "    decision_reason_text = f\"{decision} ({category}): \" + \"; \".join(decision_reason)\n",
        "    \n",
        "    # ============================================================\n",
        "    # DISPLAY RESULT\n",
        "    # ============================================================\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FINAL DECISION: {decision}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Category: {category}\")\n",
        "    print(f\"Reason: {decision_reason_text}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # STORE FOR SUMMARY TABLE\n",
        "    # ============================================================\n",
        "    \n",
        "    decision_matrix_data.append({\n",
        "        'Run_ID': run_id,\n",
        "        'Quality_Score': round(quality_score, 1),\n",
        "        'Calibration_Score': round(scores['calibration'], 1),\n",
        "        'Bone_Stability_Score': round(scores['bone_stability'], 1),\n",
        "        'Temporal_Score': round(scores['temporal_quality'], 1),\n",
        "        'Interpolation_Score': round(scores['interpolation'], 1),\n",
        "        'Filtering_Score': round(scores['filtering'], 1),\n",
        "        'SNR_Score': round(scores['snr'], 1),\n",
        "        'Biomechanics_Score': round(scores['biomechanics'], 1),\n",
        "        'Decision': decision,\n",
        "        'Decision_Category': category,\n",
        "        'Decision_Reason': decision_reason_text\n",
        "    })\n",
        "\n",
        "# ============================================================\n",
        "# CREATE DECISION MATRIX SUMMARY TABLE\n",
        "# ============================================================\n",
        "\n",
        "df_decision = pd.DataFrame(decision_matrix_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DECISION MATRIX SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "display(df_decision[['Run_ID', 'Quality_Score', 'Decision', 'Decision_Category']])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED REASONS\")\n",
        "print(\"=\"*80)\n",
        "for idx, row in df_decision.iterrows():\n",
        "    print(f\"\\n{row['Run_ID']}:\")\n",
        "    print(f\"  {row['Decision_Reason']}\")\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY STATISTICS\n",
        "# ============================================================\n",
        "\n",
        "total_runs = len(df_decision)\n",
        "accept_count = df_decision['Decision'].str.contains('ACCEPT').sum()\n",
        "review_count = df_decision['Decision'].str.contains('REVIEW').sum()\n",
        "reject_count = df_decision['Decision'].str.contains('REJECT').sum()\n",
        "\n",
        "mean_quality_score = df_decision['Quality_Score'].mean()\n",
        "min_quality_score = df_decision['Quality_Score'].min()\n",
        "max_quality_score = df_decision['Quality_Score'].max()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OVERALL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total Runs Analyzed: {total_runs}\")\n",
        "print(f\"  ‚úÖ ACCEPT: {accept_count}/{total_runs} ({accept_count/total_runs*100:.1f}%)\")\n",
        "print(f\"  ‚ö†Ô∏è REVIEW: {review_count}/{total_runs} ({review_count/total_runs*100:.1f}%)\")\n",
        "print(f\"  ‚ùå REJECT: {reject_count}/{total_runs} ({reject_count/total_runs*100:.1f}%)\")\n",
        "print()\n",
        "print(f\"Quality Score Statistics:\")\n",
        "print(f\"  Mean: {mean_quality_score:.1f} / 100\")\n",
        "print(f\"  Range: {min_quality_score:.1f} - {max_quality_score:.1f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================\n",
        "# EXPORT TO EXCEL MASTER LOG\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPORTING MASTER LOG TO EXCEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comprehensive master log\n",
        "master_log_data = []\n",
        "\n",
        "for idx, row in df_decision.iterrows():\n",
        "    run_id = row['Run_ID']\n",
        "    \n",
        "    # Get data from all sections\n",
        "    section0 = next((r for r in section0_data if r['Run_ID'] == run_id), {})\n",
        "    section1 = next((r for r in calibration_data if r['Run_ID'] == run_id), {})\n",
        "    section2 = next((r for r in rigidbody_data if r['Run_ID'] == run_id), {})\n",
        "    section3 = next((r for r in interpolation_data if r['Run_ID'] == run_id), {})\n",
        "    section4 = next((r for r in filtering_data if r['Run_ID'] == run_id), {})\n",
        "    section5 = next((r for r in isb_compliance_data if r['Run_ID'] == run_id), {})\n",
        "    section6 = next((r for r in gaga_biomechanics_data if r['Run_ID'] == run_id), {})\n",
        "    section7 = next((r for r in snr_analysis_data if r['Run_ID'] == run_id), {})\n",
        "    \n",
        "    master_log_data.append({\n",
        "        # Identification\n",
        "        'Run_ID': run_id,\n",
        "        'Processing_Timestamp': section0.get('Processing_Timestamp', 'N/A'),\n",
        "        'Pipeline_Version': section0.get('Pipeline_Version', 'N/A'),\n",
        "        \n",
        "        # Decision\n",
        "        'Decision': row['Decision'],\n",
        "        'Decision_Category': row['Decision_Category'],\n",
        "        'Decision_Reason': row['Decision_Reason'],\n",
        "        'Quality_Score': row['Quality_Score'],\n",
        "        \n",
        "        # Component Scores\n",
        "        'Calibration_Score': row['Calibration_Score'],\n",
        "        'Bone_Stability_Score': row['Bone_Stability_Score'],\n",
        "        'Temporal_Score': row['Temporal_Score'],\n",
        "        'Interpolation_Score': row['Interpolation_Score'],\n",
        "        'Filtering_Score': row['Filtering_Score'],\n",
        "        'SNR_Score': row['SNR_Score'],\n",
        "        'Biomechanics_Score': row['Biomechanics_Score'],\n",
        "        \n",
        "        # Section Statuses\n",
        "        'Integrity_Status': section0.get('Integrity_Status', 'N/A'),\n",
        "        'Calibration_Status': section1.get('Calibration_Status', 'N/A'),\n",
        "        'Rigid_Body_Status': section2.get('Rigid_Body_Status', 'N/A'),\n",
        "        'Transparency_Status': section3.get('Transparency_Status', 'N/A'),\n",
        "        'Winter_Status': section4.get('Winter_Status', 'N/A'),\n",
        "        'ISB_Status': section5.get('ISB_Status', 'N/A'),\n",
        "        'Biomech_Status': section6.get('Overall_Status', 'N/A'),\n",
        "        'SNR_Status': section7.get('Overall_Status', 'N/A'),\n",
        "        \n",
        "        # Key Metrics\n",
        "        'Pointer_Error_mm': section1.get('Pointer_Error_mm', 'N/A'),\n",
        "        'Wand_Error_mm': section1.get('Wand_Error_mm', 'N/A'),\n",
        "        'Bone_CV_%': section2.get('Bone_CV_%', 'N/A'),\n",
        "        'Time_Jitter_sec': section2.get('Time_Jitter_sec', 'N/A'),\n",
        "        'Raw_Missing_%': section3.get('Raw_Missing_%', 'N/A'),\n",
        "        'Interpolation_Method': section3.get('Interpolation_Method', 'N/A'),\n",
        "        'Cutoff_Hz': section4.get('Cutoff_Hz', 'N/A'),\n",
        "        'ROM_Violations': section5.get('ROM_Violations', 0),\n",
        "        'Mean_SNR_dB': section7.get('Mean_SNR_dB', 'N/A'),\n",
        "        'Joints_Below_15dB': section7.get('Joints_Below_15dB', 0),\n",
        "        'Occlusion_Detected': section7.get('Occlusion_Detected', 'No')\n",
        "    })\n",
        "\n",
        "df_master_log = pd.DataFrame(master_log_data)\n",
        "\n",
        "# Export to Excel\n",
        "excel_path = os.path.join(PROJECT_ROOT, \"reports\", \"MASTER_QUALITY_LOG.xlsx\")\n",
        "os.makedirs(os.path.dirname(excel_path), exist_ok=True)\n",
        "\n",
        "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
        "    # Sheet 1: Master Log (all data)\n",
        "    df_master_log.to_excel(writer, sheet_name='Master_Log', index=False)\n",
        "    \n",
        "    # Sheet 2: Decision Summary (concise)\n",
        "    df_decision[['Run_ID', 'Quality_Score', 'Decision', 'Decision_Reason']].to_excel(\n",
        "        writer, sheet_name='Decision_Summary', index=False\n",
        "    )\n",
        "    \n",
        "    # Sheet 3: Component Scores\n",
        "    score_cols = ['Run_ID', 'Quality_Score', 'Calibration_Score', 'Bone_Stability_Score', \n",
        "                  'Temporal_Score', 'Interpolation_Score', 'Filtering_Score', 'SNR_Score', \n",
        "                  'Biomechanics_Score']\n",
        "    df_decision[score_cols].to_excel(writer, sheet_name='Component_Scores', index=False)\n",
        "\n",
        "print(f\"‚úÖ Master log exported to: {excel_path}\")\n",
        "print(f\"   Sheets: Master_Log, Decision_Summary, Component_Scores\")\n",
        "print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 8 COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ Quality Score: Weighted average of all QC metrics\")\n",
        "print(\"‚úÖ Decision Logic: REJECT ‚Üí REVIEW ‚Üí ACCEPT with specific reasons\")\n",
        "print(\"‚úÖ Categorized Reasons: Clear, actionable explanations\")\n",
        "print(\"‚úÖ Excel Export: Complete master log with all metrics\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff66bef",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 9: Portable Report Links\n",
        "**Purpose:** Fast inspection with relative-path links to all QC visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f0eb7c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SECTION 9: PORTABLE REPORT LINKS\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 9: PORTABLE REPORT LINKS\")\n",
        "print(\"=\"*80)\n",
        "print(\"Purpose: Fast inspection with relative-path links to QC visualizations\")\n",
        "print(\"Constraint: RELATIVE PATHS ONLY (project folder can be moved)\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# DEFINE QC PLOT STRUCTURE\n",
        "# ============================================================\n",
        "\n",
        "# Expected QC plots and their locations (relative to PROJECT_ROOT)\n",
        "QC_PLOT_TYPES = {\n",
        "    'bone_stability': {\n",
        "        'pattern': 'derivatives/step_02_preprocess/{run_id}__bone_stability.png',\n",
        "        'section': 'Section 2',\n",
        "        'description': 'Bone length stability over time'\n",
        "    },\n",
        "    'winter_residual': {\n",
        "        'pattern': 'derivatives/step_04_filtering/{run_id}__winter_residual.png',\n",
        "        'section': 'Section 4',\n",
        "        'description': 'Winter residual analysis (RMS vs. cutoff frequency)'\n",
        "    },\n",
        "    'lcs_static': {\n",
        "        'pattern': 'reports/{run_id}_lcs_static.html',\n",
        "        'section': 'Section 5',\n",
        "        'description': 'Static LCS visualization (3D skeleton)'\n",
        "    },\n",
        "    'lcs_interactive': {\n",
        "        'pattern': 'reports/{run_id}_interactive_synced.html',\n",
        "        'section': 'Section 5',\n",
        "        'description': 'Interactive synchronized visualization'\n",
        "    },\n",
        "    'euler_angles': {\n",
        "        'pattern': 'derivatives/step_06_rotvec/{run_id}__euler_angles.png',\n",
        "        'section': 'Section 5',\n",
        "        'description': 'Euler angles over time (all joints)'\n",
        "    },\n",
        "    'angular_velocity': {\n",
        "        'pattern': 'derivatives/step_06_rotvec/{run_id}__angular_velocity.png',\n",
        "        'section': 'Section 6',\n",
        "        'description': 'Angular velocity over time'\n",
        "    },\n",
        "    'snr_per_joint': {\n",
        "        'pattern': 'derivatives/step_04_filtering/{run_id}__snr_per_joint.png',\n",
        "        'section': 'Section 7',\n",
        "        'description': 'SNR per joint bar chart'\n",
        "    }\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# HELPER FUNCTION: CONVERT TO RELATIVE PATH\n",
        "# ============================================================\n",
        "\n",
        "def to_relative_path(abs_path, base_path):\n",
        "    \"\"\"\n",
        "    Convert absolute path to relative path from base_path.\n",
        "    \n",
        "    Args:\n",
        "        abs_path: Absolute file path\n",
        "        base_path: Base directory (PROJECT_ROOT)\n",
        "    \n",
        "    Returns:\n",
        "        Relative path string (e.g., \"./derivatives/step_02/...\")\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert to Path objects\n",
        "        abs_path_obj = Path(abs_path).resolve()\n",
        "        base_path_obj = Path(base_path).resolve()\n",
        "        \n",
        "        # Compute relative path\n",
        "        rel_path = abs_path_obj.relative_to(base_path_obj)\n",
        "        \n",
        "        # Return with \"./\" prefix for clarity\n",
        "        return f\"./{rel_path.as_posix()}\"\n",
        "    except ValueError:\n",
        "        # If paths are on different drives (Windows), return absolute as fallback\n",
        "        return abs_path\n",
        "\n",
        "def check_file_exists(file_path):\n",
        "    \"\"\"Check if file exists and return status emoji.\"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        return '‚úÖ'\n",
        "    else:\n",
        "        return '‚ùå'\n",
        "\n",
        "# ============================================================\n",
        "# BUILD PORTABLE LINKS TABLE\n",
        "# ============================================================\n",
        "\n",
        "portable_links_data = []\n",
        "\n",
        "for run_id, steps in complete_runs.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PORTABLE LINKS: {run_id}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Get decision for this run\n",
        "    decision_row = next((row for row in decision_matrix_data if row['Run_ID'] == run_id), {})\n",
        "    decision = decision_row.get('Decision', 'N/A')\n",
        "    quality_score = decision_row.get('Quality_Score', 'N/A')\n",
        "    \n",
        "    print(f\"Decision: {decision}\")\n",
        "    print(f\"Quality Score: {quality_score}\")\n",
        "    print()\n",
        "    \n",
        "    # Collect all QC plot links for this run\n",
        "    links = {}\n",
        "    \n",
        "    for plot_type, config in QC_PLOT_TYPES.items():\n",
        "        # Construct absolute path\n",
        "        abs_path = os.path.join(PROJECT_ROOT, config['pattern'].format(run_id=run_id))\n",
        "        \n",
        "        # Convert to relative path\n",
        "        rel_path = to_relative_path(abs_path, PROJECT_ROOT)\n",
        "        \n",
        "        # Check if file exists\n",
        "        exists = check_file_exists(abs_path)\n",
        "        \n",
        "        # Store link info\n",
        "        links[plot_type] = {\n",
        "            'rel_path': rel_path,\n",
        "            'exists': exists,\n",
        "            'section': config['section'],\n",
        "            'description': config['description']\n",
        "        }\n",
        "        \n",
        "        # Display status\n",
        "        print(f\"  {exists} {config['section']}: {plot_type}\")\n",
        "        print(f\"     Path: {rel_path}\")\n",
        "        if exists == '‚ùå':\n",
        "            print(f\"     Note: File not found - may not be generated yet\")\n",
        "        print()\n",
        "    \n",
        "    # ============================================================\n",
        "    # CREATE MARKDOWN-FORMATTED LINKS\n",
        "    # ============================================================\n",
        "    \n",
        "    # For Jupyter display: create clickable HTML links\n",
        "    bone_stability_link = f'<a href=\"{links[\"bone_stability\"][\"rel_path\"]}\" target=\"_blank\">Bone Stability</a>' if links['bone_stability']['exists'] == '‚úÖ' else 'N/A'\n",
        "    winter_residual_link = f'<a href=\"{links[\"winter_residual\"][\"rel_path\"]}\" target=\"_blank\">Winter Residual</a>' if links['winter_residual']['exists'] == '‚úÖ' else 'N/A'\n",
        "    lcs_static_link = f'<a href=\"{links[\"lcs_static\"][\"rel_path\"]}\" target=\"_blank\">LCS Static</a>' if links['lcs_static']['exists'] == '‚úÖ' else 'N/A'\n",
        "    lcs_interactive_link = f'<a href=\"{links[\"lcs_interactive\"][\"rel_path\"]}\" target=\"_blank\">LCS Interactive</a>' if links['lcs_interactive']['exists'] == '‚úÖ' else 'N/A'\n",
        "    euler_angles_link = f'<a href=\"{links[\"euler_angles\"][\"rel_path\"]}\" target=\"_blank\">Euler Angles</a>' if links['euler_angles']['exists'] == '‚úÖ' else 'N/A'\n",
        "    angular_velocity_link = f'<a href=\"{links[\"angular_velocity\"][\"rel_path\"]}\" target=\"_blank\">Angular Velocity</a>' if links['angular_velocity']['exists'] == '‚úÖ' else 'N/A'\n",
        "    snr_per_joint_link = f'<a href=\"{links[\"snr_per_joint\"][\"rel_path\"]}\" target=\"_blank\">SNR Per Joint</a>' if links['snr_per_joint']['exists'] == '‚úÖ' else 'N/A'\n",
        "    \n",
        "    # Store for table\n",
        "    portable_links_data.append({\n",
        "        'Run_ID': run_id,\n",
        "        'Decision': decision,\n",
        "        'Quality_Score': quality_score,\n",
        "        'Bone_Stability': bone_stability_link,\n",
        "        'Winter_Residual': winter_residual_link,\n",
        "        'LCS_Static': lcs_static_link,\n",
        "        'LCS_Interactive': lcs_interactive_link,\n",
        "        'Euler_Angles': euler_angles_link,\n",
        "        'Angular_Velocity': angular_velocity_link,\n",
        "        'SNR_Per_Joint': snr_per_joint_link\n",
        "    })\n",
        "\n",
        "# ============================================================\n",
        "# CREATE PORTABLE LINKS SUMMARY TABLE\n",
        "# ============================================================\n",
        "\n",
        "df_portable_links = pd.DataFrame(portable_links_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PORTABLE REPORT LINKS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(\"All paths are RELATIVE - project folder can be moved without breaking links\")\n",
        "print()\n",
        "\n",
        "# Display table with HTML links (clickable in Jupyter)\n",
        "from IPython.display import HTML, display as ipython_display\n",
        "\n",
        "# Create HTML table\n",
        "html_table = df_portable_links.to_html(escape=False, index=False, classes='table table-striped')\n",
        "\n",
        "print(\"Interactive Table (click links to open visualizations):\")\n",
        "ipython_display(HTML(html_table))\n",
        "\n",
        "# ============================================================\n",
        "# EXPORT PORTABLE LINKS TO MARKDOWN\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPORTING PORTABLE LINKS TO MARKDOWN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "markdown_path = os.path.join(PROJECT_ROOT, \"reports\", \"PORTABLE_LINKS.md\")\n",
        "os.makedirs(os.path.dirname(markdown_path), exist_ok=True)\n",
        "\n",
        "with open(markdown_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"# Portable Report Links\\n\\n\")\n",
        "    f.write(\"**Generated:** \" + pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\\n\")\n",
        "    f.write(\"**Note:** All paths are relative - this project folder can be moved without breaking links.\\n\\n\")\n",
        "    f.write(\"---\\n\\n\")\n",
        "    \n",
        "    for idx, row in df_portable_links.iterrows():\n",
        "        run_id = row['Run_ID']\n",
        "        decision = row['Decision']\n",
        "        quality_score = row['Quality_Score']\n",
        "        \n",
        "        f.write(f\"## {run_id}\\n\\n\")\n",
        "        f.write(f\"**Decision:** {decision}  \\n\")\n",
        "        f.write(f\"**Quality Score:** {quality_score}  \\n\\n\")\n",
        "        \n",
        "        f.write(\"### QC Visualizations\\n\\n\")\n",
        "        \n",
        "        # Write links for each plot type\n",
        "        for plot_type, config in QC_PLOT_TYPES.items():\n",
        "            abs_path = os.path.join(PROJECT_ROOT, config['pattern'].format(run_id=run_id))\n",
        "            rel_path = to_relative_path(abs_path, PROJECT_ROOT)\n",
        "            exists = check_file_exists(abs_path)\n",
        "            \n",
        "            if exists == '‚úÖ':\n",
        "                f.write(f\"- **{config['section']} - {config['description']}:**  \\n\")\n",
        "                f.write(f\"  [{plot_type}]({rel_path})\\n\\n\")\n",
        "            else:\n",
        "                f.write(f\"- **{config['section']} - {config['description']}:**  \\n\")\n",
        "                f.write(f\"  ‚ùå Not available (file not generated)\\n\\n\")\n",
        "        \n",
        "        f.write(\"---\\n\\n\")\n",
        "\n",
        "print(f\"‚úÖ Portable links exported to: {markdown_path}\")\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# VERIFY RELATIVE PATH PORTABILITY\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RELATIVE PATH VERIFICATION\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Test that all relative paths are indeed relative\n",
        "all_relative = True\n",
        "for idx, row in df_portable_links.iterrows():\n",
        "    run_id = row['Run_ID']\n",
        "    \n",
        "    for plot_type, config in QC_PLOT_TYPES.items():\n",
        "        abs_path = os.path.join(PROJECT_ROOT, config['pattern'].format(run_id=run_id))\n",
        "        rel_path = to_relative_path(abs_path, PROJECT_ROOT)\n",
        "        \n",
        "        # Check if path is relative (starts with ./)\n",
        "        if not rel_path.startswith('./'):\n",
        "            print(f\"‚ö†Ô∏è WARNING: Path is not relative: {rel_path}\")\n",
        "            all_relative = False\n",
        "\n",
        "if all_relative:\n",
        "    print(\"‚úÖ All paths are relative - portability verified!\")\n",
        "    print()\n",
        "    print(\"You can now:\")\n",
        "    print(\"  1. Move the entire project folder to a different location\")\n",
        "    print(\"  2. Share the folder via cloud storage (Dropbox, Google Drive, etc.)\")\n",
        "    print(\"  3. Open the notebook on a different computer\")\n",
        "    print(\"  ‚Üí All links will still work!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Some paths are absolute - portability may be limited\")\n",
        "\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# STATISTICS\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"VISUALIZATION AVAILABILITY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for plot_type, config in QC_PLOT_TYPES.items():\n",
        "    available_count = 0\n",
        "    total_count = len(complete_runs)\n",
        "    \n",
        "    for run_id in complete_runs.keys():\n",
        "        abs_path = os.path.join(PROJECT_ROOT, config['pattern'].format(run_id=run_id))\n",
        "        if os.path.exists(abs_path):\n",
        "            available_count += 1\n",
        "    \n",
        "    availability_pct = (available_count / total_count * 100) if total_count > 0 else 0\n",
        "    \n",
        "    print(f\"\\n{plot_type}:\")\n",
        "    print(f\"  {config['section']} - {config['description']}\")\n",
        "    print(f\"  Available: {available_count}/{total_count} ({availability_pct:.1f}%)\")\n",
        "    \n",
        "    if available_count < total_count:\n",
        "        print(f\"  Note: {total_count - available_count} file(s) missing - check upstream notebooks\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 9 COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ Portable Links: Relative paths for all QC visualizations\")\n",
        "print(\"‚úÖ Markdown Export: Shareable report with clickable links\")\n",
        "print(\"‚úÖ Portability Verified: Project folder can be moved without breaking links\")\n",
        "print(\"‚úÖ Availability Stats: Track which visualizations exist\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MASTER AUDIT & RESULTS NOTEBOOK - ALL 9 SECTIONS COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ Section 0: Data Lineage & Provenance\")\n",
        "print(\"‚úÖ Section 1: R√°cz Calibration Layer\")\n",
        "print(\"‚úÖ Section 2: Rigid-Body & Temporal Audit\")\n",
        "print(\"‚úÖ Section 3: Gap & Interpolation Transparency\")\n",
        "print(\"‚úÖ Section 4: Winter's Residual Validation\")\n",
        "print(\"‚úÖ Section 5: ISB Compliance & Synchronized Viz\")\n",
        "print(\"‚úÖ Section 6: Gaga-Aware Biomechanics\")\n",
        "print(\"‚úÖ Section 7: SNR Quantification\")\n",
        "print(\"‚úÖ Section 8: The Decision Matrix\")\n",
        "print(\"‚úÖ Section 9: Portable Report Links\")\n",
        "print()\n",
        "print(\"üéâ MASTER AUDIT COMPLETE - READY FOR PRODUCTION! üéâ\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5672345",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# FINAL SECTION: Dataset Yield Table\n",
        "**Purpose:** Executive summary - at-a-glance dataset quality overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3e72bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FINAL SECTION: DATASET YIELD TABLE\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATASET YIELD TABLE\")\n",
        "print(\"=\"*80)\n",
        "print(\"Executive Summary: At-a-glance overview of dataset quality and data loss\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# COMPUTE OVERALL YIELD METRICS\n",
        "# ============================================================\n",
        "\n",
        "total_takes = len(df_decision)\n",
        "\n",
        "# Count by decision type\n",
        "accept_count = df_decision['Decision'].str.contains('ACCEPT').sum()\n",
        "review_count = df_decision['Decision'].str.contains('REVIEW').sum()\n",
        "reject_count = df_decision['Decision'].str.contains('REJECT').sum()\n",
        "\n",
        "# Calculate percentages\n",
        "accept_pct = (accept_count / total_takes * 100) if total_takes > 0 else 0\n",
        "review_pct = (review_count / total_takes * 100) if total_takes > 0 else 0\n",
        "reject_pct = (reject_count / total_takes * 100) if total_takes > 0 else 0\n",
        "\n",
        "# ============================================================\n",
        "# DATASET YIELD SUMMARY TABLE\n",
        "# ============================================================\n",
        "\n",
        "yield_summary = pd.DataFrame({\n",
        "    'Metric': ['Total Takes', 'Accepted', 'Need Review', 'Rejected'],\n",
        "    'Count': [total_takes, accept_count, review_count, reject_count],\n",
        "    'Percentage': [100.0, accept_pct, review_pct, reject_pct]\n",
        "})\n",
        "\n",
        "print(\"DATASET YIELD SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "display(yield_summary)\n",
        "print()\n",
        "\n",
        "# Visual bar representation\n",
        "print(\"Visual Breakdown:\")\n",
        "print(\"=\"*80)\n",
        "accept_bar = '‚ñà' * int(accept_pct / 2) if accept_pct > 0 else ''\n",
        "review_bar = '‚ñà' * int(review_pct / 2) if review_pct > 0 else ''\n",
        "reject_bar = '‚ñà' * int(reject_pct / 2) if reject_pct > 0 else ''\n",
        "\n",
        "print(f\"‚úÖ Accepted ({accept_count}/{total_takes}, {accept_pct:.1f}%): {accept_bar}\")\n",
        "print(f\"‚ö†Ô∏è Review   ({review_count}/{total_takes}, {review_pct:.1f}%): {review_bar}\")\n",
        "print(f\"‚ùå Rejected ({reject_count}/{total_takes}, {reject_pct:.1f}%): {reject_bar}\")\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# DATA LOSS ANALYSIS: GROUP BY DECISION CATEGORY\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATA LOSS ANALYSIS: WHY WERE TAKES REJECTED OR FLAGGED FOR REVIEW?\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Extract REVIEW and REJECT cases\n",
        "review_cases = df_decision[df_decision['Decision'].str.contains('REVIEW')].copy()\n",
        "reject_cases = df_decision[df_decision['Decision'].str.contains('REJECT')].copy()\n",
        "\n",
        "# ============================================================\n",
        "# REJECTION REASONS (CRITICAL DATA LOSS)\n",
        "# ============================================================\n",
        "\n",
        "if len(reject_cases) > 0:\n",
        "    print(\"REJECTION REASONS (Critical Data Loss):\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Group by Decision_Category\n",
        "    reject_by_category = reject_cases.groupby('Decision_Category').size().reset_index(name='Count')\n",
        "    reject_by_category['Percentage_of_Total'] = (reject_by_category['Count'] / total_takes * 100)\n",
        "    reject_by_category['Percentage_of_Rejects'] = (reject_by_category['Count'] / len(reject_cases) * 100)\n",
        "    reject_by_category = reject_by_category.sort_values('Count', ascending=False)\n",
        "    \n",
        "    for idx, row in reject_by_category.iterrows():\n",
        "        category = row['Decision_Category']\n",
        "        count = row['Count']\n",
        "        pct_total = row['Percentage_of_Total']\n",
        "        pct_rejects = row['Percentage_of_Rejects']\n",
        "        \n",
        "        print(f\"\\n‚ùå {category}:\")\n",
        "        print(f\"   Count: {count}/{total_takes} takes ({pct_total:.1f}% of total dataset)\")\n",
        "        print(f\"   Impact: {pct_rejects:.1f}% of all rejections\")\n",
        "        \n",
        "        # Show example runs for this category\n",
        "        examples = reject_cases[reject_cases['Decision_Category'] == category]['Run_ID'].head(3).tolist()\n",
        "        if examples:\n",
        "            print(f\"   Example runs: {', '.join(examples[:2])}\")\n",
        "        \n",
        "        # Extract common reason patterns\n",
        "        reasons = reject_cases[reject_cases['Decision_Category'] == category]['Decision_Reason'].tolist()\n",
        "        if reasons:\n",
        "            # Get first reason as representative\n",
        "            print(f\"   Typical reason: {reasons[0][:100]}...\")\n",
        "    \n",
        "    print()\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Total Data Loss from Rejections: {reject_count}/{total_takes} takes ({reject_pct:.1f}%)\")\n",
        "    print(\"-\" * 80)\n",
        "    print()\n",
        "else:\n",
        "    print(\"‚úÖ NO REJECTIONS - All takes passed critical quality checks!\")\n",
        "    print()\n",
        "\n",
        "# ============================================================\n",
        "# REVIEW REASONS (REQUIRES MANUAL INSPECTION)\n",
        "# ============================================================\n",
        "\n",
        "if len(review_cases) > 0:\n",
        "    print(\"=\"*80)\n",
        "    print(\"REVIEW REASONS (Requires Manual Inspection):\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Group by Decision_Category\n",
        "    review_by_category = review_cases.groupby('Decision_Category').size().reset_index(name='Count')\n",
        "    review_by_category['Percentage_of_Total'] = (review_by_category['Count'] / total_takes * 100)\n",
        "    review_by_category['Percentage_of_Reviews'] = (review_by_category['Count'] / len(review_cases) * 100)\n",
        "    review_by_category = review_by_category.sort_values('Count', ascending=False)\n",
        "    \n",
        "    for idx, row in review_by_category.iterrows():\n",
        "        category = row['Decision_Category']\n",
        "        count = row['Count']\n",
        "        pct_total = row['Percentage_of_Total']\n",
        "        pct_reviews = row['Percentage_of_Reviews']\n",
        "        \n",
        "        print(f\"\\n‚ö†Ô∏è {category}:\")\n",
        "        print(f\"   Count: {count}/{total_takes} takes ({pct_total:.1f}% of total dataset)\")\n",
        "        print(f\"   Impact: {pct_reviews:.1f}% of all reviews\")\n",
        "        \n",
        "        # Show example runs\n",
        "        examples = review_cases[review_cases['Decision_Category'] == category]['Run_ID'].head(3).tolist()\n",
        "        if examples:\n",
        "            print(f\"   Example runs: {', '.join(examples[:2])}\")\n",
        "        \n",
        "        # Extract common flags\n",
        "        reasons = review_cases[review_cases['Decision_Category'] == category]['Decision_Reason'].tolist()\n",
        "        if reasons:\n",
        "            # Parse individual flags from semicolon-separated reasons\n",
        "            all_flags = []\n",
        "            for reason in reasons:\n",
        "                # Extract the part after the category\n",
        "                if ':' in reason:\n",
        "                    flags_part = reason.split(':', 1)[1]\n",
        "                    flags = [f.strip() for f in flags_part.split(';')]\n",
        "                    all_flags.extend(flags)\n",
        "            \n",
        "            # Count unique flags\n",
        "            from collections import Counter\n",
        "            flag_counts = Counter(all_flags)\n",
        "            \n",
        "            print(f\"   Common flags:\")\n",
        "            for flag, flag_count in flag_counts.most_common(3):\n",
        "                print(f\"     ‚Ä¢ {flag} ({flag_count} runs)\")\n",
        "    \n",
        "    print()\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Total Flagged for Review: {review_count}/{total_takes} takes ({review_pct:.1f}%)\")\n",
        "    print(\"-\" * 80)\n",
        "    print()\n",
        "else:\n",
        "    print(\"‚úÖ NO REVIEW FLAGS - All takes have clean quality scores!\")\n",
        "    print()\n",
        "\n",
        "# ============================================================\n",
        "# ACTIONABLE INSIGHTS\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ACTIONABLE INSIGHTS\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Identify top 3 reasons for data loss\n",
        "all_problematic = pd.concat([reject_cases, review_cases])\n",
        "\n",
        "if len(all_problematic) > 0:\n",
        "    top_issues = all_problematic.groupby('Decision_Category').size().reset_index(name='Count')\n",
        "    top_issues['Percentage'] = (top_issues['Count'] / total_takes * 100)\n",
        "    top_issues = top_issues.sort_values('Count', ascending=False).head(3)\n",
        "    \n",
        "    print(\"Top 3 Issues Affecting Dataset Quality:\")\n",
        "    print()\n",
        "    \n",
        "    for idx, (i, row) in enumerate(top_issues.iterrows(), 1):\n",
        "        category = row['Decision_Category']\n",
        "        count = row['Count']\n",
        "        pct = row['Percentage']\n",
        "        \n",
        "        print(f\"{idx}. {category}\")\n",
        "        print(f\"   Affected: {count} takes ({pct:.1f}% of dataset)\")\n",
        "        \n",
        "        # Provide specific recommendation\n",
        "        if 'Rigid-Body' in category:\n",
        "            print(f\"   üí° Recommendation: Check marker attachment - consider double-sided tape or additional securing\")\n",
        "        elif 'Signal Quality' in category:\n",
        "            print(f\"   üí° Recommendation: Verify camera placement and lighting - markers may be occluded\")\n",
        "        elif 'Calibration' in category:\n",
        "            print(f\"   üí° Recommendation: Review calibration procedure - ensure pointer and wand are properly tracked\")\n",
        "        elif 'Quality Flags' in category:\n",
        "            print(f\"   üí° Recommendation: Review flagged takes visually using Section 5 interactive visualization\")\n",
        "        elif 'Quality Score' in category:\n",
        "            print(f\"   üí° Recommendation: Multiple minor issues - check overall experimental protocol\")\n",
        "        else:\n",
        "            print(f\"   üí° Recommendation: Review {category} metrics in detail using Master Audit sections\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"‚úÖ EXCELLENT DATASET QUALITY - No significant issues detected!\")\n",
        "    print()\n",
        "\n",
        "# ============================================================\n",
        "# OVERALL DATASET STATUS\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"OVERALL DATASET STATUS\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Determine overall dataset health\n",
        "if reject_pct == 0 and review_pct == 0:\n",
        "    status = \"‚≠ê EXCELLENT\"\n",
        "    message = \"Perfect dataset - all takes accepted with high quality scores\"\n",
        "elif reject_pct <= 5 and review_pct <= 10:\n",
        "    status = \"‚úÖ GOOD\"\n",
        "    message = \"High-quality dataset with minimal data loss\"\n",
        "elif reject_pct <= 15 and review_pct <= 25:\n",
        "    status = \"‚ö†Ô∏è ACCEPTABLE\"\n",
        "    message = \"Acceptable dataset but consider improving data collection protocols\"\n",
        "else:\n",
        "    status = \"üî¥ POOR\"\n",
        "    message = \"Significant data loss - review experimental setup and protocols\"\n",
        "\n",
        "print(f\"Dataset Health: {status}\")\n",
        "print(f\"Assessment: {message}\")\n",
        "print()\n",
        "print(f\"Usable Data: {accept_count}/{total_takes} takes ({accept_pct:.1f}%)\")\n",
        "print(f\"Requires Review: {review_count}/{total_takes} takes ({review_pct:.1f}%)\")\n",
        "print(f\"Data Loss: {reject_count}/{total_takes} takes ({reject_pct:.1f}%)\")\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# EXPORT DATASET YIELD TO CSV\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXPORTING DATASET YIELD REPORT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "csv_path = os.path.join(PROJECT_ROOT, \"reports\", \"DATASET_YIELD_REPORT.csv\")\n",
        "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "\n",
        "# Create comprehensive yield report\n",
        "yield_report_data = []\n",
        "\n",
        "# Overall summary\n",
        "yield_report_data.append({\n",
        "    'Category': 'Overall',\n",
        "    'Metric': 'Total Takes',\n",
        "    'Count': total_takes,\n",
        "    'Percentage': 100.0,\n",
        "    'Status': status,\n",
        "    'Notes': message\n",
        "})\n",
        "\n",
        "yield_report_data.append({\n",
        "    'Category': 'Overall',\n",
        "    'Metric': 'Accepted',\n",
        "    'Count': accept_count,\n",
        "    'Percentage': accept_pct,\n",
        "    'Status': '‚úÖ',\n",
        "    'Notes': 'High-quality data ready for analysis'\n",
        "})\n",
        "\n",
        "yield_report_data.append({\n",
        "    'Category': 'Overall',\n",
        "    'Metric': 'Need Review',\n",
        "    'Count': review_count,\n",
        "    'Percentage': review_pct,\n",
        "    'Status': '‚ö†Ô∏è',\n",
        "    'Notes': 'Requires visual inspection before use'\n",
        "})\n",
        "\n",
        "yield_report_data.append({\n",
        "    'Category': 'Overall',\n",
        "    'Metric': 'Rejected',\n",
        "    'Count': reject_count,\n",
        "    'Percentage': reject_pct,\n",
        "    'Status': '‚ùå',\n",
        "    'Notes': 'Critical quality failures - data unreliable'\n",
        "})\n",
        "\n",
        "# Rejection breakdown\n",
        "if len(reject_cases) > 0:\n",
        "    for idx, row in reject_by_category.iterrows():\n",
        "        yield_report_data.append({\n",
        "            'Category': 'Rejection Reason',\n",
        "            'Metric': row['Decision_Category'],\n",
        "            'Count': row['Count'],\n",
        "            'Percentage': row['Percentage_of_Total'],\n",
        "            'Status': '‚ùå',\n",
        "            'Notes': f\"{row['Percentage_of_Rejects']:.1f}% of all rejections\"\n",
        "        })\n",
        "\n",
        "# Review breakdown\n",
        "if len(review_cases) > 0:\n",
        "    for idx, row in review_by_category.iterrows():\n",
        "        yield_report_data.append({\n",
        "            'Category': 'Review Reason',\n",
        "            'Metric': row['Decision_Category'],\n",
        "            'Count': row['Count'],\n",
        "            'Percentage': row['Percentage_of_Total'],\n",
        "            'Status': '‚ö†Ô∏è',\n",
        "            'Notes': f\"{row['Percentage_of_Reviews']:.1f}% of all reviews\"\n",
        "        })\n",
        "\n",
        "df_yield_report = pd.DataFrame(yield_report_data)\n",
        "df_yield_report.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Dataset yield report exported to: {csv_path}\")\n",
        "print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATASET YIELD TABLE COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üéâ MASTER AUDIT & RESULTS NOTEBOOK - COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "print(\"Summary:\")\n",
        "print(f\"  ‚Ä¢ Total Sections: 10 (0-9 + Final Yield Table)\")\n",
        "print(f\"  ‚Ä¢ Total Takes Analyzed: {total_takes}\")\n",
        "print(f\"  ‚Ä¢ Accepted: {accept_count} ({accept_pct:.1f}%)\")\n",
        "print(f\"  ‚Ä¢ Review: {review_count} ({review_pct:.1f}%)\")\n",
        "print(f\"  ‚Ä¢ Rejected: {reject_count} ({reject_pct:.1f}%)\")\n",
        "print(f\"  ‚Ä¢ Dataset Status: {status}\")\n",
        "print()\n",
        "print(\"Exports:\")\n",
        "print(f\"  ‚Ä¢ Excel Master Log: reports/MASTER_QUALITY_LOG.xlsx\")\n",
        "print(f\"  ‚Ä¢ Portable Links: reports/PORTABLE_LINKS.md\")\n",
        "print(f\"  ‚Ä¢ Dataset Yield: reports/DATASET_YIELD_REPORT.csv\")\n",
        "print()\n",
        "print(\"Next Steps:\")\n",
        "print(\"  1. Review REVIEW-flagged takes using Section 9 links\")\n",
        "print(\"  2. Investigate top rejection reasons using Actionable Insights\")\n",
        "print(\"  3. Share reports with supervisor/collaborators\")\n",
        "print()\n",
        "print(\"üéâ THANK YOU FOR USING THE MASTER AUDIT & RESULTS NOTEBOOK! üéâ\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
