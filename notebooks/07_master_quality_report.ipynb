{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3524988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Found data for 3 run(s)\n",
      "ðŸ“ Steps loaded per run:\n",
      "  734_T1_P1_R1_Take 2025-12-01 02.18.27 PM: ['step_06', 'step_04', 'step_02', 'step_01', 'step_05']\n",
      "  734_T1_P2_R1_Take 2025-12-01 02.28.24 PM: ['step_06', 'step_04', 'step_02', 'step_01', 'step_05']\n",
      "  763_T2_P2_R2_Take_2025-12-25 10.51.23 AM_005: ['step_06', 'step_04', 'step_02', 'step_01', 'step_05']\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xlsxwriter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 171\u001b[0m\n\u001b[1;32m    168\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(REPORTS_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    169\u001b[0m excel_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(REPORTS_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaster_Audit_Log_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexcel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxlsxwriter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m    172\u001b[0m     df_master\u001b[38;5;241m.\u001b[39mto_excel(writer, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAudit_Log\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    173\u001b[0m     workbook \u001b[38;5;241m=\u001b[39m writer\u001b[38;5;241m.\u001b[39mbook\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/excel/_xlsxwriter.py:197\u001b[0m, in \u001b[0;36mXlsxWriter.__init__\u001b[0;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    186\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m WriteExcelBuffer \u001b[38;5;241m|\u001b[39m ExcelWriter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# Use the xlsxwriter module as the Excel writer.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxlsxwriter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[1;32m    199\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xlsxwriter'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# --- Setup Paths ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "\n",
    "# ============================================================\n",
    "# Safe Parsing Helpers (per specification)\n",
    "# ============================================================\n",
    "def safe_get(d, *keys, default='N/A'):\n",
    "    \"\"\"Safe nested dictionary access with default fallback\"\"\"\n",
    "    for key in keys:\n",
    "        if isinstance(d, dict):\n",
    "            d = d.get(key, {})\n",
    "        else:\n",
    "            return default\n",
    "    return d if (d != {} and d is not None) else default\n",
    "\n",
    "def safe_float(x, default=0.0):\n",
    "    \"\"\"Convert to float safely, strip %, handle None/N/A\"\"\"\n",
    "    if x is None or x == 'N/A':\n",
    "        return default\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            x = x.replace('%', '').strip()\n",
    "        return float(x)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "# ============================================================\n",
    "# File Discovery (exact suffix matching per specification)\n",
    "# ============================================================\n",
    "DERIV_ROOT = os.path.join(PROJECT_ROOT, \"derivatives\")\n",
    "\n",
    "# Required suffixes for exact matching (updated to match actual file names)\n",
    "REQUIRED_SUFFIXES = [\n",
    "    \"__step01_loader_report.json\",  # step_01 (actual naming)\n",
    "    \"__preprocess_summary.json\",    # step_02  \n",
    "    \"__filtering_summary.json\",     # step_04\n",
    "    \"__reference_summary.json\",     # step_05\n",
    "    \"__kinematics_summary.json\"     # step_06\n",
    "]\n",
    "\n",
    "# Scan recursively for *.json files\n",
    "json_files = glob.glob(os.path.join(DERIV_ROOT, \"**\", \"*.json\"), recursive=True)\n",
    "\n",
    "# Filter files with exact suffix matching\n",
    "valid_files = []\n",
    "for json_path in json_files:\n",
    "    filename = os.path.basename(json_path)\n",
    "    if any(filename.endswith(suffix) for suffix in REQUIRED_SUFFIXES):\n",
    "        valid_files.append(json_path)\n",
    "\n",
    "print(f\"ðŸ“ Found {len(valid_files)} valid JSON files out of {len(json_files)} total\")\n",
    "\n",
    "# ============================================================\n",
    "# Load + Group by Run_ID (per specification)\n",
    "# ============================================================\n",
    "from collections import defaultdict\n",
    "runs = defaultdict(dict)\n",
    "\n",
    "for json_path in valid_files:\n",
    "    filename = os.path.basename(json_path)\n",
    "    run_id = filename.split(\"__\")[0]\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Failed to load {filename}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Categorize by exact suffix matching\n",
    "    if filename.endswith(\"__step01_loader_report.json\"):\n",
    "        runs[run_id][\"step_01\"] = data\n",
    "    elif filename.endswith(\"__preprocess_summary.json\"):\n",
    "        runs[run_id][\"step_02\"] = data\n",
    "    elif filename.endswith(\"__filtering_summary.json\"):\n",
    "        runs[run_id][\"step_04\"] = data\n",
    "    elif filename.endswith(\"__reference_summary.json\"):\n",
    "        runs[run_id][\"step_05\"] = data\n",
    "    elif filename.endswith(\"__kinematics_summary.json\"):\n",
    "        runs[run_id][\"step_06\"] = data\n",
    "\n",
    "# Skip runs missing critical data (require step_01 and step_06)\n",
    "complete_runs = {rid: steps for rid, steps in runs.items() \n",
    "                if steps.get('step_01') and steps.get('step_06')}\n",
    "\n",
    "print(f\"ðŸ“Š Found data for {len(complete_runs)} complete run(s)\")\n",
    "print(f\"ðŸ“ Steps loaded per run:\")\n",
    "for rid, steps in complete_runs.items():\n",
    "    print(f\"  {rid}: {list(steps.keys())}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea776cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Master Row Schema (fixed columns per specification)\n",
    "# ============================================================\n",
    "all_summaries = []\n",
    "\n",
    "for run_id, steps in complete_runs.items():\n",
    "    s01 = steps.get('step_01', {})\n",
    "    s02 = steps.get('step_02', {})\n",
    "    s04 = steps.get('step_04', {})\n",
    "    s05 = steps.get('step_05', {})\n",
    "    s06 = steps.get('step_06', {})\n",
    "    \n",
    "    # ============================================================\n",
    "    # Canonical Fields (normalize differences per specification)\n",
    "    # ============================================================\n",
    "    \n",
    "    # Sampling rate: fps = first_available with fallback to 120.0\n",
    "    fps = safe_float(\n",
    "        safe_get(s01, 'raw_data_quality', 'sampling_rate_actual',\n",
    "                'fs_actual_hz', 'sampling_rate_hz'),\n",
    "        default=120.0\n",
    "    )\n",
    "    \n",
    "    # Reference status: normalize to uppercase\n",
    "    ref_status_raw = safe_get(s05, 'reference_metrics', 'ref_quality_status', default='MISSING')\n",
    "    ref_status = str(ref_status_raw).upper()\n",
    "    \n",
    "    # ============================================================\n",
    "    # Gap Units Logic (use ms directly when available)\n",
    "    # ============================================================\n",
    "    max_gap_frames = safe_get(s02, 'max_interpolation_gap', default=0)\n",
    "    max_gap_ms_raw = safe_get(s02, 'max_gap_ms')  # Check if ms is directly available\n",
    "    \n",
    "    if max_gap_ms_raw != 'N/A' and max_gap_ms_raw is not None:\n",
    "        # Use ms directly when available\n",
    "        max_gap_ms = safe_float(max_gap_ms_raw)\n",
    "    else:\n",
    "        # Compute ms from frames only if ms is missing\n",
    "        max_gap_ms = round((safe_float(max_gap_frames) / fps) * 1000, 2)\n",
    "    \n",
    "    # ============================================================\n",
    "    # Build Master Row\n",
    "    # ============================================================\n",
    "    row = {\n",
    "        # --- Identity ---\n",
    "        \"Run_ID\": run_id,\n",
    "        \"Processing_Date\": safe_get(s01, 'identity', 'processing_timestamp'),\n",
    "        \n",
    "        # --- Step01 Fields ---\n",
    "        \"OptiTrack_Error_mm\": safe_float(safe_get(s01, 'raw_data_quality', 'optitrack_mean_error_mm')),\n",
    "        \"Total_Frames\": safe_get(s01, 'raw_data_quality', 'total_frames', default=0),\n",
    "        \n",
    "        # --- Step02 Fields ---\n",
    "        \"Missing_Raw_%\": safe_float(safe_get(s02, 'raw_missing_percent')),\n",
    "        \"Max_Gap_Frames\": max_gap_frames,\n",
    "        \"Max_Gap_MS\": max_gap_ms,\n",
    "        \"Bone_Stability_CV\": safe_float(safe_get(s02, 'bone_qc_mean_cv')),\n",
    "        \"Skeletal_Alerts\": safe_get(s02, 'bone_qc_alerts', default=0),\n",
    "        \"Worst_Bone\": safe_get(s02, 'worst_bone'),\n",
    "        \n",
    "        # --- Step05 Fields ---\n",
    "        \"Ref_Stability_mm\": safe_float(safe_get(s05, 'reference_metrics', 'ref_stability_mm')),\n",
    "        \"Ref_Status\": ref_status,\n",
    "        \n",
    "        # --- Step06 Signal Quality ---\n",
    "        \"Signal_Noise_RMS\": safe_float(safe_get(s06, 'signal_quality', 'avg_vel_residual_rms')),\n",
    "        \"Dom_Freq_Hz\": safe_float(safe_get(s06, 'signal_quality', 'avg_dominant_freq_hz')),\n",
    "        \"Quat_Norm_Error\": safe_float(safe_get(s06, 'signal_quality', 'max_quat_norm_error')),\n",
    "        \n",
    "        # --- Step06 Kinematics ---\n",
    "        \"Max_Ang_Vel\": safe_float(safe_get(s06, 'metrics', 'angular_velocity', 'max')),\n",
    "        \"Mean_Ang_Vel\": safe_float(safe_get(s06, 'metrics', 'angular_velocity', 'mean')),\n",
    "        \"Max_Lin_Acc\": safe_float(safe_get(s06, 'metrics', 'linear_accel', 'max')),\n",
    "        \"Outlier_Frames\": safe_get(s06, 'effort_metrics', 'outlier_frame_count', default=0),\n",
    "        \n",
    "        # --- Step06 Effort Metrics ---\n",
    "        \"Path_Length_M\": round(safe_float(safe_get(s06, 'effort_metrics', 'total_path_length_mm')) / 1000, 2),\n",
    "        \"Intensity_Index\": safe_float(safe_get(s06, 'effort_metrics', 'intensity_index')),\n",
    "        \n",
    "        # --- Overall Status ---\n",
    "        \"Pipeline_Status\": safe_get(s06, 'overall_status'),\n",
    "    }\n",
    "    \n",
    "    # ============================================================\n",
    "    # Quality Scoring (labeled heuristic per specification)\n",
    "    # ============================================================\n",
    "    score = 100.0\n",
    "    \n",
    "    # Penalties for data quality\n",
    "    score -= safe_float(row[\"Missing_Raw_%\"]) * 5\n",
    "    score -= (safe_float(row[\"Max_Gap_MS\"]) / 10)  # Penalty for large gaps\n",
    "    \n",
    "    # Penalties for skeletal stability\n",
    "    score -= safe_float(row[\"Bone_Stability_CV\"]) * 10 \n",
    "    score -= safe_float(row[\"Skeletal_Alerts\"]) * 5\n",
    "    \n",
    "    # Penalty for reference stability\n",
    "    ref_stab = safe_float(row[\"Ref_Stability_mm\"])\n",
    "    if ref_stab > 4.0: \n",
    "        score -= 15\n",
    "    \n",
    "    # Penalty for signal quality issues\n",
    "    if safe_float(row[\"Quat_Norm_Error\"]) > 0.1:\n",
    "        score -= 10\n",
    "    \n",
    "    row[\"Quality_Score\"] = round(max(0, min(100, score)), 2)\n",
    "    row[\"Quality_Score_Method\"] = \"heuristic_v1\"  # Required label per specification\n",
    "    \n",
    "    # ============================================================\n",
    "    # Research Decision Rule (deterministic per specification)\n",
    "    # ============================================================\n",
    "    if (row[\"Pipeline_Status\"] == \"PASS\" and \n",
    "        row[\"Quality_Score\"] >= 75 and \n",
    "        row[\"Ref_Status\"] == \"PASS\" and \n",
    "        safe_float(row[\"Bone_Stability_CV\"]) < 1.5):\n",
    "        row[\"Research_Decision\"] = \"ACCEPT\"\n",
    "    elif row[\"Pipeline_Status\"] == \"PASS\" and row[\"Quality_Score\"] >= 50:\n",
    "        row[\"Research_Decision\"] = \"REVIEW\"\n",
    "    else:\n",
    "        row[\"Research_Decision\"] = \"REJECT\"\n",
    "    \n",
    "    all_summaries.append(row)\n",
    "\n",
    "print(f\"ðŸ“Š Processed {len(all_summaries)} complete runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Export to Excel (per specification)\n",
    "# ============================================================\n",
    "if not all_summaries:\n",
    "    print(\"âŒ No complete runs found to aggregate!\")\n",
    "else:\n",
    "    df_master = pd.DataFrame(all_summaries)\n",
    "    df_master = df_master.sort_values('Quality_Score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    REPORTS_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
    "    os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "    excel_path = os.path.join(REPORTS_DIR, f\"Master_Audit_Log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\")\n",
    "    \n",
    "    with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
    "        df_master.to_excel(writer, index=False, sheet_name='Audit_Log')\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets['Audit_Log']\n",
    "        \n",
    "        # Header styling\n",
    "        header_fmt = workbook.add_format({\n",
    "            'bold': True, \n",
    "            'bg_color': '#4472C4', \n",
    "            'font_color': 'white',\n",
    "            'text_wrap': True\n",
    "        })\n",
    "        for col_num, value in enumerate(df_master.columns.values):\n",
    "            worksheet.write(0, col_num, value, header_fmt)\n",
    "        \n",
    "        # Conditional formatting for Research_Decision (green/yellow/red)\n",
    "        red_fmt = workbook.add_format({'bg_color': '#FFC7CE', 'font_color': '#9C0006'})\n",
    "        yellow_fmt = workbook.add_format({'bg_color': '#FFEB9C', 'font_color': '#9C6500'})\n",
    "        green_fmt = workbook.add_format({'bg_color': '#C6EFCE', 'font_color': '#006100'})\n",
    "        \n",
    "        col_idx = df_master.columns.get_loc(\"Research_Decision\")\n",
    "        for row_num in range(1, len(df_master) + 1):\n",
    "            decision = df_master.iloc[row_num-1]['Research_Decision']\n",
    "            if decision == 'ACCEPT':\n",
    "                worksheet.write(row_num, col_idx, decision, green_fmt)\n",
    "            elif decision == 'REVIEW':\n",
    "                worksheet.write(row_num, col_idx, decision, yellow_fmt)\n",
    "            else:\n",
    "                worksheet.write(row_num, col_idx, decision, red_fmt)\n",
    "        \n",
    "        # Auto-fit column widths capped at 40 characters\n",
    "        for i, col in enumerate(df_master.columns):\n",
    "            max_len = max(\n",
    "                df_master[col].astype(str).str.len().max(),\n",
    "                len(str(col))\n",
    "            )\n",
    "            worksheet.set_column(i, i, min(max_len + 2, 40))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸŽ‰ Master Audit Log Created\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"ðŸ“Š Total Runs: {len(all_summaries)}\")\n",
    "    print(f\"ðŸ’¾ File: {excel_path}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    print(\"Decision Summary:\")\n",
    "    print(df_master['Research_Decision'].value_counts())\n",
    "    \n",
    "    print(\"\\nQuality Score Stats:\")\n",
    "    print(f\"  Mean: {df_master['Quality_Score'].mean():.2f}\")\n",
    "    print(f\"  Min:  {df_master['Quality_Score'].min():.2f}\")\n",
    "    print(f\"  Max:  {df_master['Quality_Score'].max():.2f}\")\n",
    "    \n",
    "    print(\"\\nQuality Score Method:\")\n",
    "    print(f\"  {df_master['Quality_Score_Method'].iloc[0] if len(df_master) > 0 else 'N/A'}\")\n",
    "    \n",
    "    print(\"\\nPreview:\")\n",
    "    display(df_master.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
