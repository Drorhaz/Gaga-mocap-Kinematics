{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3524988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Found 20 valid JSON files out of 48 total\n",
      "ðŸ“Š Found data for 3 complete run(s)\n",
      "ðŸ“ Steps loaded per run:\n",
      "  763_T2_P2_R2_Take_2025-12-25 10.51.23 AM_005: ['step_02', 'step_04', 'step_06', 'step_05', 'step_01']\n",
      "  734_T1_P1_R1_Take 2025-12-01 02.18.27 PM: ['step_01', 'step_02', 'step_04', 'step_05', 'step_06']\n",
      "  734_T1_P2_R1_Take 2025-12-01 02.28.24 PM: ['step_01', 'step_02', 'step_04', 'step_05', 'step_06']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# --- Setup Paths ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "\n",
    "# ============================================================\n",
    "# Safe Parsing Helpers (per specification)\n",
    "# ============================================================\n",
    "def safe_get(d, *keys, default='N/A'):\n",
    "    \"\"\"Safe nested dictionary access with default fallback\"\"\"\n",
    "    for key in keys:\n",
    "        if isinstance(d, dict):\n",
    "            d = d.get(key, {})\n",
    "        else:\n",
    "            return default\n",
    "    return d if (d != {} and d is not None) else default\n",
    "\n",
    "def safe_float(x, default=0.0):\n",
    "    \"\"\"Convert to float safely, strip %, handle None/N/A\"\"\"\n",
    "    if x is None or x == 'N/A':\n",
    "        return default\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            x = x.replace('%', '').strip()\n",
    "        return float(x)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "# ============================================================\n",
    "# File Discovery (exact suffix matching per specification)\n",
    "# ============================================================\n",
    "DERIV_ROOT = os.path.join(PROJECT_ROOT, \"derivatives\")\n",
    "\n",
    "# Required suffixes for exact matching (updated to match actual file names)\n",
    "REQUIRED_SUFFIXES = [\n",
    "    \"__step01_loader_report.json\",  # step_01 (actual naming)\n",
    "    \"__preprocess_summary.json\",    # step_02  \n",
    "    \"__filtering_summary.json\",     # step_04\n",
    "    \"__reference_summary.json\",     # step_05\n",
    "    \"__kinematics_summary.json\"     # step_06\n",
    "]\n",
    "\n",
    "# Scan recursively for *.json files\n",
    "json_files = glob.glob(os.path.join(DERIV_ROOT, \"**\", \"*.json\"), recursive=True)\n",
    "\n",
    "# Filter files with exact suffix matching\n",
    "valid_files = []\n",
    "for json_path in json_files:\n",
    "    filename = os.path.basename(json_path)\n",
    "    if any(filename.endswith(suffix) for suffix in REQUIRED_SUFFIXES):\n",
    "        valid_files.append(json_path)\n",
    "\n",
    "print(f\"ðŸ“ Found {len(valid_files)} valid JSON files out of {len(json_files)} total\")\n",
    "\n",
    "# ============================================================\n",
    "# Load + Group by Run_ID (per specification)\n",
    "# ============================================================\n",
    "from collections import defaultdict\n",
    "runs = defaultdict(dict)\n",
    "\n",
    "for json_path in valid_files:\n",
    "    filename = os.path.basename(json_path)\n",
    "    run_id = filename.split(\"__\")[0]\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Failed to load {filename}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Categorize by exact suffix matching\n",
    "    if filename.endswith(\"__step01_loader_report.json\"):\n",
    "        runs[run_id][\"step_01\"] = data\n",
    "    elif filename.endswith(\"__preprocess_summary.json\"):\n",
    "        runs[run_id][\"step_02\"] = data\n",
    "    elif filename.endswith(\"__filtering_summary.json\"):\n",
    "        runs[run_id][\"step_04\"] = data\n",
    "    elif filename.endswith(\"__reference_summary.json\"):\n",
    "        runs[run_id][\"step_05\"] = data\n",
    "    elif filename.endswith(\"__kinematics_summary.json\"):\n",
    "        runs[run_id][\"step_06\"] = data\n",
    "\n",
    "# Skip runs missing critical data (require step_01 and step_06)\n",
    "complete_runs = {rid: steps for rid, steps in runs.items() \n",
    "                if steps.get('step_01') and steps.get('step_06')}\n",
    "\n",
    "print(f\"ðŸ“Š Found data for {len(complete_runs)} complete run(s)\")\n",
    "print(f\"ðŸ“ Steps loaded per run:\")\n",
    "for rid, steps in complete_runs.items():\n",
    "    print(f\"  {rid}: {list(steps.keys())}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea776cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Processed 3 complete runs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Master Row Schema (fixed columns per specification)\n",
    "# ============================================================\n",
    "all_summaries = []\n",
    "\n",
    "for run_id, steps in complete_runs.items():\n",
    "    s01 = steps.get('step_01', {})\n",
    "    s02 = steps.get('step_02', {})\n",
    "    s04 = steps.get('step_04', {})\n",
    "    s05 = steps.get('step_05', {})\n",
    "    s06 = steps.get('step_06', {})\n",
    "    \n",
    "    # ============================================================\n",
    "    # Canonical Fields (normalize differences per specification)\n",
    "    # ============================================================\n",
    "    \n",
    "    # Sampling rate: fps = first_available with fallback to 120.0\n",
    "    fps = safe_float(\n",
    "        safe_get(s01, 'raw_data_quality', 'sampling_rate_actual',\n",
    "                'fs_actual_hz', 'sampling_rate_hz'),\n",
    "        default=120.0\n",
    "    )\n",
    "    \n",
    "    # Reference status: normalize to uppercase\n",
    "    ref_status_raw = safe_get(s05, 'reference_metrics', 'ref_quality_status', default='MISSING')\n",
    "    ref_status = str(ref_status_raw).upper()\n",
    "    \n",
    "    # ============================================================\n",
    "    # Gap Units Logic (use ms directly when available)\n",
    "    # ============================================================\n",
    "    max_gap_frames = safe_get(s02, 'max_interpolation_gap', default=0)\n",
    "    max_gap_ms_raw = safe_get(s02, 'max_gap_ms')  # Check if ms is directly available\n",
    "    \n",
    "    if max_gap_ms_raw != 'N/A' and max_gap_ms_raw is not None:\n",
    "        # Use ms directly when available\n",
    "        max_gap_ms = safe_float(max_gap_ms_raw)\n",
    "    else:\n",
    "        # Compute ms from frames only if ms is missing\n",
    "        max_gap_ms = round((safe_float(max_gap_frames) / fps) * 1000, 2)\n",
    "    \n",
    "    # ============================================================\n",
    "    # Build Master Row\n",
    "    # ============================================================\n",
    "    row = {\n",
    "        # --- Identity ---\n",
    "        \"Run_ID\": run_id,\n",
    "        \"Processing_Date\": safe_get(s01, 'identity', 'processing_timestamp'),\n",
    "        \n",
    "        # --- Step01 Fields ---\n",
    "        \"OptiTrack_Error_mm\": safe_float(safe_get(s01, 'raw_data_quality', 'optitrack_mean_error_mm')),\n",
    "        \"Total_Frames\": safe_get(s01, 'raw_data_quality', 'total_frames', default=0),\n",
    "        \n",
    "        # --- Step02 Fields ---\n",
    "        \"Missing_Raw_%\": safe_float(safe_get(s02, 'raw_missing_percent')),\n",
    "        \"Max_Gap_Frames\": max_gap_frames,\n",
    "        \"Max_Gap_MS\": max_gap_ms,\n",
    "        \"Bone_Stability_CV\": safe_float(safe_get(s02, 'bone_qc_mean_cv')),\n",
    "        \"Skeletal_Alerts\": safe_get(s02, 'bone_qc_alerts', default=0),\n",
    "        \"Worst_Bone\": safe_get(s02, 'worst_bone'),\n",
    "        \n",
    "        # --- Step05 Fields ---\n",
    "        \"Ref_Stability_mm\": safe_float(safe_get(s05, 'reference_metrics', 'ref_stability_mm')),\n",
    "        \"Ref_Status\": ref_status,\n",
    "        \n",
    "        # --- Step06 Signal Quality ---\n",
    "        \"Signal_Noise_RMS\": safe_float(safe_get(s06, 'signal_quality', 'avg_vel_residual_rms')),\n",
    "        \"Dom_Freq_Hz\": safe_float(safe_get(s06, 'signal_quality', 'avg_dominant_freq_hz')),\n",
    "        \"Quat_Norm_Error\": safe_float(safe_get(s06, 'signal_quality', 'max_quat_norm_error')),\n",
    "        \n",
    "        # --- Step06 Kinematics ---\n",
    "        \"Max_Ang_Vel\": safe_float(safe_get(s06, 'metrics', 'angular_velocity', 'max')),\n",
    "        \"Mean_Ang_Vel\": safe_float(safe_get(s06, 'metrics', 'angular_velocity', 'mean')),\n",
    "        \"Max_Lin_Acc\": safe_float(safe_get(s06, 'metrics', 'linear_accel', 'max')),\n",
    "        \"Outlier_Frames\": safe_get(s06, 'effort_metrics', 'outlier_frame_count', default=0),\n",
    "        \n",
    "        # --- Step06 Effort Metrics ---\n",
    "        \"Path_Length_M\": round(safe_float(safe_get(s06, 'effort_metrics', 'total_path_length_mm')) / 1000, 2),\n",
    "        \"Intensity_Index\": safe_float(safe_get(s06, 'effort_metrics', 'intensity_index')),\n",
    "        \n",
    "        # --- Overall Status ---\n",
    "        \"Pipeline_Status\": safe_get(s06, 'overall_status'),\n",
    "    }\n",
    "    \n",
    "    # ============================================================\n",
    "    # Quality Scoring (labeled heuristic per specification)\n",
    "    # ============================================================\n",
    "    score = 100.0\n",
    "    \n",
    "    # Penalties for data quality\n",
    "    score -= safe_float(row[\"Missing_Raw_%\"]) * 5\n",
    "    score -= (safe_float(row[\"Max_Gap_MS\"]) / 10)  # Penalty for large gaps\n",
    "    \n",
    "    # Penalties for skeletal stability\n",
    "    score -= safe_float(row[\"Bone_Stability_CV\"]) * 10 \n",
    "    score -= safe_float(row[\"Skeletal_Alerts\"]) * 5\n",
    "    \n",
    "    # Penalty for reference stability\n",
    "    ref_stab = safe_float(row[\"Ref_Stability_mm\"])\n",
    "    if ref_stab > 4.0: \n",
    "        score -= 15\n",
    "    \n",
    "    # Penalty for signal quality issues\n",
    "    if safe_float(row[\"Quat_Norm_Error\"]) > 0.1:\n",
    "        score -= 10\n",
    "    \n",
    "    row[\"Quality_Score\"] = round(max(0, min(100, score)), 2)\n",
    "    row[\"Quality_Score_Method\"] = \"heuristic_v1\"  # Required label per specification\n",
    "    \n",
    "    # ============================================================\n",
    "    # Research Decision Rule (deterministic per specification)\n",
    "    # ============================================================\n",
    "    if (row[\"Pipeline_Status\"] == \"PASS\" and \n",
    "        row[\"Quality_Score\"] >= 75 and \n",
    "        row[\"Ref_Status\"] == \"PASS\" and \n",
    "        safe_float(row[\"Bone_Stability_CV\"]) < 1.5):\n",
    "        row[\"Research_Decision\"] = \"ACCEPT\"\n",
    "    elif row[\"Pipeline_Status\"] == \"PASS\" and row[\"Quality_Score\"] >= 50:\n",
    "        row[\"Research_Decision\"] = \"REVIEW\"\n",
    "    else:\n",
    "        row[\"Research_Decision\"] = \"REJECT\"\n",
    "    \n",
    "    all_summaries.append(row)\n",
    "\n",
    "print(f\"ðŸ“Š Processed {len(all_summaries)} complete runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff3b381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸŽ‰ Master Audit Log Created\n",
      "======================================================================\n",
      "ðŸ“Š Total Runs: 3\n",
      "ðŸ’¾ File: c:\\Users\\drorh\\OneDrive - Mobileye\\Desktop\\gaga\\reports\\Master_Audit_Log_20260119_190230.xlsx\n",
      "======================================================================\n",
      "\n",
      "Decision Summary:\n",
      "Research_Decision\n",
      "ACCEPT    2\n",
      "REVIEW    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Quality Score Stats:\n",
      "  Mean: 87.79\n",
      "  Min:  87.59\n",
      "  Max:  87.91\n",
      "\n",
      "Quality Score Method:\n",
      "  heuristic_v1\n",
      "\n",
      "Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run_ID</th>\n",
       "      <th>Processing_Date</th>\n",
       "      <th>OptiTrack_Error_mm</th>\n",
       "      <th>Total_Frames</th>\n",
       "      <th>Missing_Raw_%</th>\n",
       "      <th>Max_Gap_Frames</th>\n",
       "      <th>Max_Gap_MS</th>\n",
       "      <th>Bone_Stability_CV</th>\n",
       "      <th>Skeletal_Alerts</th>\n",
       "      <th>Worst_Bone</th>\n",
       "      <th>...</th>\n",
       "      <th>Max_Ang_Vel</th>\n",
       "      <th>Mean_Ang_Vel</th>\n",
       "      <th>Max_Lin_Acc</th>\n",
       "      <th>Outlier_Frames</th>\n",
       "      <th>Path_Length_M</th>\n",
       "      <th>Intensity_Index</th>\n",
       "      <th>Pipeline_Status</th>\n",
       "      <th>Quality_Score</th>\n",
       "      <th>Quality_Score_Method</th>\n",
       "      <th>Research_Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>763_T2_P2_R2_Take_2025-12-25 10.51.23 AM_005</td>\n",
       "      <td>2026-01-14 13:51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>83.33</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0</td>\n",
       "      <td>Hips-&gt;Spine</td>\n",
       "      <td>...</td>\n",
       "      <td>1359.12</td>\n",
       "      <td>113.83</td>\n",
       "      <td>44376.02</td>\n",
       "      <td>40</td>\n",
       "      <td>61.28</td>\n",
       "      <td>0.291</td>\n",
       "      <td>PASS</td>\n",
       "      <td>87.91</td>\n",
       "      <td>heuristic_v1</td>\n",
       "      <td>ACCEPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>734_T1_P2_R1_Take 2025-12-01 02.28.24 PM</td>\n",
       "      <td>2026-01-13 20:35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>83.33</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0</td>\n",
       "      <td>Hips-&gt;Spine</td>\n",
       "      <td>...</td>\n",
       "      <td>900.17</td>\n",
       "      <td>44.92</td>\n",
       "      <td>19427.84</td>\n",
       "      <td>0</td>\n",
       "      <td>35.93</td>\n",
       "      <td>0.386</td>\n",
       "      <td>PASS</td>\n",
       "      <td>87.88</td>\n",
       "      <td>heuristic_v1</td>\n",
       "      <td>ACCEPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>734_T1_P1_R1_Take 2025-12-01 02.18.27 PM</td>\n",
       "      <td>2026-01-19 18:35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>83.33</td>\n",
       "      <td>0.408</td>\n",
       "      <td>[Hips-&gt;Spine, Neck-&gt;Head]</td>\n",
       "      <td>Hips-&gt;Spine</td>\n",
       "      <td>...</td>\n",
       "      <td>1026.98</td>\n",
       "      <td>31.98</td>\n",
       "      <td>38536.20</td>\n",
       "      <td>0</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.084</td>\n",
       "      <td>PASS</td>\n",
       "      <td>87.59</td>\n",
       "      <td>heuristic_v1</td>\n",
       "      <td>REVIEW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Run_ID   Processing_Date  \\\n",
       "0  763_T2_P2_R2_Take_2025-12-25 10.51.23 AM_005  2026-01-14 13:51   \n",
       "1      734_T1_P2_R1_Take 2025-12-01 02.28.24 PM  2026-01-13 20:35   \n",
       "2      734_T1_P1_R1_Take 2025-12-01 02.18.27 PM  2026-01-19 18:35   \n",
       "\n",
       "   OptiTrack_Error_mm  Total_Frames  Missing_Raw_%  Max_Gap_Frames  \\\n",
       "0                 0.0         17263            0.0              10   \n",
       "1                 0.0         19617            0.0              10   \n",
       "2                 0.0         30798            0.0              10   \n",
       "\n",
       "   Max_Gap_MS  Bone_Stability_CV            Skeletal_Alerts   Worst_Bone  ...  \\\n",
       "0       83.33              0.376                          0  Hips->Spine  ...   \n",
       "1       83.33              0.379                          0  Hips->Spine  ...   \n",
       "2       83.33              0.408  [Hips->Spine, Neck->Head]  Hips->Spine  ...   \n",
       "\n",
       "   Max_Ang_Vel Mean_Ang_Vel  Max_Lin_Acc  Outlier_Frames  Path_Length_M  \\\n",
       "0      1359.12       113.83     44376.02              40          61.28   \n",
       "1       900.17        44.92     19427.84               0          35.93   \n",
       "2      1026.98        31.98     38536.20               0          25.67   \n",
       "\n",
       "   Intensity_Index  Pipeline_Status  Quality_Score  Quality_Score_Method  \\\n",
       "0            0.291             PASS          87.91          heuristic_v1   \n",
       "1            0.386             PASS          87.88          heuristic_v1   \n",
       "2            0.084             PASS          87.59          heuristic_v1   \n",
       "\n",
       "   Research_Decision  \n",
       "0             ACCEPT  \n",
       "1             ACCEPT  \n",
       "2             REVIEW  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Export to Excel (per specification)\n",
    "# ============================================================\n",
    "if not all_summaries:\n",
    "    print(\"âŒ No complete runs found to aggregate!\")\n",
    "else:\n",
    "    df_master = pd.DataFrame(all_summaries)\n",
    "    df_master = df_master.sort_values('Quality_Score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    REPORTS_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
    "    os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "    excel_path = os.path.join(REPORTS_DIR, f\"Master_Audit_Log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\")\n",
    "    \n",
    "    with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
    "        df_master.to_excel(writer, index=False, sheet_name='Audit_Log')\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets['Audit_Log']\n",
    "        \n",
    "        # Header styling\n",
    "        header_fmt = workbook.add_format({\n",
    "            'bold': True, \n",
    "            'bg_color': '#4472C4', \n",
    "            'font_color': 'white',\n",
    "            'text_wrap': True\n",
    "        })\n",
    "        for col_num, value in enumerate(df_master.columns.values):\n",
    "            worksheet.write(0, col_num, value, header_fmt)\n",
    "        \n",
    "        # Conditional formatting for Research_Decision (green/yellow/red)\n",
    "        red_fmt = workbook.add_format({'bg_color': '#FFC7CE', 'font_color': '#9C0006'})\n",
    "        yellow_fmt = workbook.add_format({'bg_color': '#FFEB9C', 'font_color': '#9C6500'})\n",
    "        green_fmt = workbook.add_format({'bg_color': '#C6EFCE', 'font_color': '#006100'})\n",
    "        \n",
    "        col_idx = df_master.columns.get_loc(\"Research_Decision\")\n",
    "        for row_num in range(1, len(df_master) + 1):\n",
    "            decision = df_master.iloc[row_num-1]['Research_Decision']\n",
    "            if decision == 'ACCEPT':\n",
    "                worksheet.write(row_num, col_idx, decision, green_fmt)\n",
    "            elif decision == 'REVIEW':\n",
    "                worksheet.write(row_num, col_idx, decision, yellow_fmt)\n",
    "            else:\n",
    "                worksheet.write(row_num, col_idx, decision, red_fmt)\n",
    "        \n",
    "        # Auto-fit column widths capped at 40 characters\n",
    "        for i, col in enumerate(df_master.columns):\n",
    "            max_len = max(\n",
    "                df_master[col].astype(str).str.len().max(),\n",
    "                len(str(col))\n",
    "            )\n",
    "            worksheet.set_column(i, i, min(max_len + 2, 40))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸŽ‰ Master Audit Log Created\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"ðŸ“Š Total Runs: {len(all_summaries)}\")\n",
    "    print(f\"ðŸ’¾ File: {excel_path}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    print(\"Decision Summary:\")\n",
    "    print(df_master['Research_Decision'].value_counts())\n",
    "    \n",
    "    print(\"\\nQuality Score Stats:\")\n",
    "    print(f\"  Mean: {df_master['Quality_Score'].mean():.2f}\")\n",
    "    print(f\"  Min:  {df_master['Quality_Score'].min():.2f}\")\n",
    "    print(f\"  Max:  {df_master['Quality_Score'].max():.2f}\")\n",
    "    \n",
    "    print(\"\\nQuality Score Method:\")\n",
    "    print(f\"  {df_master['Quality_Score_Method'].iloc[0] if len(df_master) > 0 else 'N/A'}\")\n",
    "    \n",
    "    print(\"\\nPreview:\")\n",
    "    display(df_master.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
