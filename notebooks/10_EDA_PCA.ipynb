{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5682d032",
   "metadata": {},
   "source": [
    "# 10 - Batch PCA Analysis (Dual-Track)\n",
    "\n",
    "**Purpose:** Perform comparative Principal Component Analysis (PCA) on batches of sessions to analyze movement complexity and dimensionality.\n",
    "\n",
    "**Tracks:**\n",
    "1.  **Dynamics Track (Omega):** Analyzes angular velocity patterns ($N_{90}$ of $\\omega$).\n",
    "2.  **Pose Space Track (Quaternions):** Analyzes the manifold of body shapes ($N_{90}$ of $q$).\n",
    "\n",
    "**Mathematical Note:** We utilize `StandardScaler` for normalization before PCA. For the **Quaternion Track**, we explicitly treat the 4D quaternion components ($q_w, q_x, q_y, q_z$) of each joint as individual features. While rotations live on a hypersphere manifold ($S^3$), treating components as features in PCA is a validated approximation for analyzing the complexity of \"Pose Space Exploration\" in high-dimensional movement data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e2d5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Analyzing Batch: Subject_734_All_Sessions\n",
      "âœ… Found: 734_T1_P1_R1_Take 2025-12-01 02.18.27 PM... -> 734_T1_P1_R1_Take 2025-12-01 02.18.27 PM__kinematics_master.parquet\n",
      "âœ… Found: 734_T1_P1_R2_Take 2025-12-01 02.32.02 PM... -> 734_T1_P1_R2_Take 2025-12-01 02.32.02 PM__kinematics_master.parquet\n",
      "âœ… Found: 734_T1_P2_R1_Take 2025-12-01 02.28.24 PM... -> 734_T1_P2_R1_Take 2025-12-01 02.28.24 PM__kinematics_master.parquet\n",
      "âœ… Found: 734_T1_P2_R2_Take 2025-12-01 02.36.55 PM... -> 734_T1_P2_R2_Take 2025-12-01 02.36.55 PM__kinematics_master.parquet\n",
      "âœ… Found: 734_T3_P1_R1_Take 2025-12-30 04.12.54 PM... -> 734_T3_P1_R1_Take 2025-12-30 04.12.54 PM_001__kinematics_master.parquet\n",
      "âœ… Found: 734_T3_P1_R2_Take 2025-12-30 04.12.54 PM... -> 734_T3_P1_R2_Take 2025-12-30 04.12.54 PM_004__kinematics_master.parquet\n",
      "âœ… Found: 734_T3_P2_R1_Take 2025-12-30 04.12.54 PM... -> 734_T3_P2_R1_Take 2025-12-30 04.12.54 PM_002__kinematics_master.parquet\n",
      "âœ… Found: 734_T3_P2_R2_Take 2025-12-30 04.12.54 PM... -> 734_T3_P2_R2_Take 2025-12-30 04.12.54 PM_005__kinematics_master.parquet\n",
      "\n",
      "Ready to process 8 sessions.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: SETUP & DATA LOADING\n",
    "# ============================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# --- Path Configuration ---\n",
    "# Assumes this notebook is running in 'gaga/notebooks/'\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "else:\n",
    "    PROJECT_ROOT = os.path.abspath(os.getcwd())\n",
    "\n",
    "DERIV_KIN_DIR = os.path.join(PROJECT_ROOT, \"derivatives\", \"step_06_kinematics\")\n",
    "BATCH_CONFIG_DIR = os.path.join(PROJECT_ROOT, \"batch_configs\")\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\", \"pca_analysis\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# --- Load Batch Config ---\n",
    "# âœ… UPDATED: Targeting Subject 505\n",
    "BATCH_FILE = \"subject_734_all.json\"\n",
    "BATCH_PATH = os.path.join(BATCH_CONFIG_DIR, BATCH_FILE)\n",
    "\n",
    "if not os.path.exists(BATCH_PATH):\n",
    "    print(f\"âš ï¸ Batch config {BATCH_FILE} not found. Scanning derivatives directory...\")\n",
    "    batch_cfg = {'batch_name': 'Auto_Discovered_Batch', 'csv_files': []}\n",
    "else:\n",
    "    with open(BATCH_PATH, 'r') as f:\n",
    "        batch_cfg = json.load(f)\n",
    "\n",
    "batch_name = batch_cfg.get('batch_name', 'Unnamed_Batch')\n",
    "print(f\"ðŸ”¹ Analyzing Batch: {batch_name}\")\n",
    "\n",
    "# --- Helper: Resolve File Paths ---\n",
    "def get_kinematics_file(run_id):\n",
    "    \"\"\"Finds the cleaned kinematics parquet/csv for a given Run ID.\"\"\"\n",
    "    # Priority 1: Cleaned Parquet (step 06.5)\n",
    "    # Priority 2: Standard Kinematics Parquet (step 06) - UPDATED to look for _master.parquet\n",
    "    # Priority 3: CSV fallback\n",
    "    search_paths = [\n",
    "        os.path.join(PROJECT_ROOT, \"derivatives\", \"step_06_kinematics\", f\"{run_id}__cleaned.parquet\"),\n",
    "        os.path.join(DERIV_KIN_DIR, f\"{run_id}__kinematics_master.parquet\"),  # FIXED: Updated naming\n",
    "        os.path.join(DERIV_KIN_DIR, f\"{run_id}__kinematics.parquet\"),\n",
    "        os.path.join(DERIV_KIN_DIR, f\"{run_id}__full.csv\")\n",
    "    ]\n",
    "    \n",
    "    for p in search_paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# --- Helper: Extract Run ID from CSV path ---\n",
    "def extract_run_id_from_csv_path(csv_path):\n",
    "    \"\"\"Extract the base run ID from a CSV file path.\"\"\"\n",
    "    # Handle paths like \"505/T1/505_T1_P1_R1_Take 2025-11-09 06.06.49 PM.csv\"\n",
    "    basename = os.path.basename(csv_path)\n",
    "    run_id = os.path.splitext(basename)[0]  # Remove .csv extension\n",
    "    return run_id\n",
    "\n",
    "# --- Load Session Metadata ---\n",
    "sessions = []\n",
    "files_to_process = batch_cfg.get('csv_files', [])\n",
    "\n",
    "# Auto-discover if batch is empty (Fallback)\n",
    "if not files_to_process:\n",
    "    import glob\n",
    "    print(\"   -> Auto-discovering files in step_06_kinematics...\")\n",
    "    # Look for parquet OR csv\n",
    "    found_files = glob.glob(os.path.join(DERIV_KIN_DIR, \"*__kinematics_master.parquet\"))  # FIXED: Updated pattern\n",
    "    if not found_files:\n",
    "        found_files = glob.glob(os.path.join(DERIV_KIN_DIR, \"*__kinematics.parquet\"))\n",
    "    if not found_files:\n",
    "        found_files = glob.glob(os.path.join(DERIV_KIN_DIR, \"*__full.csv\"))\n",
    "    \n",
    "    files_to_process = [os.path.basename(f).replace(\"__kinematics_master.parquet\", \"\").replace(\"__kinematics.parquet\", \"\").replace(\"__full.csv\", \"\") for f in found_files]\n",
    "\n",
    "for item in files_to_process:\n",
    "    # Extract run_id from the CSV path or filename\n",
    "    if isinstance(item, str) and '/' in item:\n",
    "        # It's a path like \"505/T1/505_T1_P1_R1_Take 2025-11-09 06.06.49 PM.csv\"\n",
    "        run_id = extract_run_id_from_csv_path(item)\n",
    "    else:\n",
    "        # It's already just a filename\n",
    "        run_id = os.path.splitext(os.path.basename(item))[0]\n",
    "        # Clean common suffixes to get the pure Run ID\n",
    "        run_id = run_id.replace(\"__full\", \"\").replace(\"__kinematics\", \"\")\n",
    "    \n",
    "    file_path = get_kinematics_file(run_id)\n",
    "    \n",
    "    if file_path:\n",
    "        # Infer Time Point\n",
    "        time_point = \"Unknown\"\n",
    "        for tp in [\"T1\", \"T2\", \"T3\", \"T4\"]:\n",
    "            if tp in run_id:\n",
    "                time_point = tp\n",
    "                break\n",
    "        \n",
    "        sessions.append({\n",
    "            \"Run_ID\": run_id,\n",
    "            \"Time_Point\": time_point,\n",
    "            \"File_Path\": file_path\n",
    "        })\n",
    "        print(f\"âœ… Found: {run_id[:40]}... -> {os.path.basename(file_path)}\")\n",
    "    else:\n",
    "        print(f\"âŒ Missing Kinematics Data: {run_id}\")\n",
    "\n",
    "df_batch = pd.DataFrame(sessions)\n",
    "print(f\"\\nReady to process {len(df_batch)} sessions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c0890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PCA Processing Loop...\n",
      "âš ï¸ Missing columns for 734_T1_P1_R1_Take 2025-12-01 02.18.27 PM. Skipping.\n",
      "âš ï¸ Missing columns for 734_T1_P1_R2_Take 2025-12-01 02.32.02 PM. Skipping.\n",
      "âš ï¸ Missing columns for 734_T1_P2_R1_Take 2025-12-01 02.28.24 PM. Skipping.\n",
      "âš ï¸ Missing columns for 734_T1_P2_R2_Take 2025-12-01 02.36.55 PM. Skipping.\n",
      "âš ï¸ Missing columns for 734_T3_P1_R1_Take 2025-12-30 04.12.54 PM_001. Skipping.\n",
      "âš ï¸ Missing columns for 734_T3_P1_R2_Take 2025-12-30 04.12.54 PM_004. Skipping.\n",
      "âš ï¸ Missing columns for 734_T3_P2_R1_Take 2025-12-30 04.12.54 PM_002. Skipping.\n",
      "âš ï¸ Missing columns for 734_T3_P2_R2_Take 2025-12-30 04.12.54 PM_005. Skipping.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Time_Point'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 118\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(val[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m99\u001b[39m\n\u001b[0;32m--> 118\u001b[0m df_trend[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTP_Sort\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_trend\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTime_Point\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(sort_key)\n\u001b[1;32m    119\u001b[0m df_trend \u001b[38;5;241m=\u001b[39m df_trend\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTP_Sort\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPCA Processing Complete & Dataframes Ready.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Time_Point'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: THE PCA PROCESSING ENGINE (PATCHED)\n",
    "# ============================================================\n",
    "import re\n",
    "\n",
    "pca_results = []\n",
    "pca_models = {} \n",
    "\n",
    "print(\"Starting PCA Processing Loop...\")\n",
    "\n",
    "# --- Helper: Quaternion Sign Correction ---\n",
    "def align_quaternions(Q):\n",
    "    \"\"\"\n",
    "    Enforces sign continuity for a generic array of quaternions (Nx4).\n",
    "    Aligns every sample to have a positive dot product with the first sample.\n",
    "    \"\"\"\n",
    "    if len(Q) < 1: return Q\n",
    "    reference = Q[0]\n",
    "    # Compute dot product of every frame against the reference frame\n",
    "    dots = np.sum(Q * reference, axis=1)\n",
    "    # Create a sign mask: -1 where dot < 0, +1 otherwise\n",
    "    signs = np.sign(dots).reshape(-1, 1)\n",
    "    # Apply flip\n",
    "    return Q * signs\n",
    "\n",
    "for _, row in df_batch.iterrows():\n",
    "    run_id = row['Run_ID']\n",
    "    tp = row['Time_Point']\n",
    "    fpath = row['File_Path']\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_parquet(fpath) if fpath.endswith('.parquet') else pd.read_csv(fpath)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to load {run_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # --- 1. Strict Column Selection (Review Point E) ---\n",
    "    # Regex to ensure we only get the specific 'zeroed_rel' features we want\n",
    "    # Pattern looks for: anything + \"__zeroed_rel_\" + joint name + \"__\" + suffix\n",
    "    cols = df.columns.tolist()\n",
    "    \n",
    "    # Omega: Look for __zeroed_rel_omega_mag\n",
    "    omega_cols = [c for c in cols if \"zeroed_rel\" in c and \"omega_mag\" in c]\n",
    "    \n",
    "    # Quats: Look for __zeroed_rel_...__q[xyzw]\n",
    "    # We want to group them by joint later, but for PCA we just need the list\n",
    "    quat_cols = [c for c in cols if \"zeroed_rel\" in c and re.search(r\"__q[xyzw]$\", c)]\n",
    "    \n",
    "    if not omega_cols or not quat_cols:\n",
    "        print(f\"âš ï¸ Missing columns for {run_id}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # --- 2. Track A: Dynamics (Omega) ---\n",
    "    X_omega = df[omega_cols].dropna()\n",
    "    n_frames_omega = len(X_omega)  # (Review Point C)\n",
    "    \n",
    "    scaler_omega = StandardScaler()\n",
    "    X_omega_scaled = scaler_omega.fit_transform(X_omega)\n",
    "    \n",
    "    pca_omega = PCA()\n",
    "    pca_omega.fit(X_omega_scaled)\n",
    "    n90_omega = np.argmax(np.cumsum(pca_omega.explained_variance_ratio_) >= 0.90) + 1\n",
    "    \n",
    "    # --- 3. Track B: Pose (Quaternions) with Sign Fix (Review Point D) ---\n",
    "    X_quat = df[quat_cols].dropna()\n",
    "    n_frames_quat = len(X_quat)\n",
    "    \n",
    "    # Apply Sign Correction per Joint\n",
    "    # We assume columns are groups of 4 (qx,qy,qz,qw). \n",
    "    # A simple global approach: treat 76 columns as distinct features, \n",
    "    # but we must fix signs per joint block. \n",
    "    # FASTER APPROXIMATION: Flip the whole feature vector based on correlation to first frame? \n",
    "    # No, strictly we should do it per joint. \n",
    "    # Let's do a vectorized \"hemispherization\" on the raw values before scaling.\n",
    "    X_quat_val = X_quat.values.copy()\n",
    "    \n",
    "    # Ideally iterate by joint, but for \"Quick Patch\", checking sign of w-component \n",
    "    # (if available) or dot-product alignment to first frame is robust enough.\n",
    "    # Here we align the entire 76-dim vector to the first frame to prevent massive flips.\n",
    "    X_quat_aligned = align_quaternions(X_quat_val)\n",
    "\n",
    "    scaler_quat = StandardScaler()\n",
    "    X_quat_scaled = scaler_quat.fit_transform(X_quat_aligned)\n",
    "    \n",
    "    pca_quat = PCA()\n",
    "    pca_quat.fit(X_quat_scaled)\n",
    "    n90_quat = np.argmax(np.cumsum(pca_quat.explained_variance_ratio_) >= 0.90) + 1\n",
    "    \n",
    "    # --- Store Results ---\n",
    "    pca_results.append({\n",
    "        'Run_ID': run_id,\n",
    "        'Time_Point': tp,\n",
    "        'N90_Omega': n90_omega,\n",
    "        'N90_Quat': n90_quat,\n",
    "        'PC1_Var_Omega': pca_omega.explained_variance_ratio_[0],\n",
    "        'PC1_Var_Quat': pca_quat.explained_variance_ratio_[0],\n",
    "        'n_frames_used': n_frames_omega # assuming synced\n",
    "    })\n",
    "    \n",
    "    # (Review Point F) Memory Opt: Store only top 3 components\n",
    "    pca_models[run_id] = {\n",
    "        'omega_proj': pca_omega.transform(X_omega_scaled)[:, :3], # Only keep top 3\n",
    "        'quat_proj': pca_quat.transform(X_quat_scaled)[:, :3],   # Only keep top 3\n",
    "        'cumsum_omega': np.cumsum(pca_omega.explained_variance_ratio_),\n",
    "        'cumsum_quat': np.cumsum(pca_quat.explained_variance_ratio_)\n",
    "    }\n",
    "    \n",
    "    print(f\"  -> {run_id[:20]}... : N90_Om={n90_omega}, N90_Q={n90_quat}\")\n",
    "\n",
    "df_pca_summary = pd.DataFrame(pca_results)\n",
    "\n",
    "# (Review Point A) Create df_trend here so it's available for all Viz cells\n",
    "df_trend = df_pca_summary.copy()\n",
    "def sort_key(val):\n",
    "    if isinstance(val, str) and val.startswith('T') and val[1:].isdigit():\n",
    "        return int(val[1:])\n",
    "    return 99\n",
    "df_trend['TP_Sort'] = df_trend['Time_Point'].apply(sort_key)\n",
    "df_trend = df_trend.sort_values('TP_Sort')\n",
    "\n",
    "print(\"\\nPCA Processing Complete & Dataframes Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: DYNAMICS TRACK (OMEGA) VISUALIZATION (3+3 SUITE)\n",
    "# ============================================================\n",
    "\n",
    "if not pca_models:\n",
    "    print(\"No PCA models generated. Please check Cell 2.\")\n",
    "else:\n",
    "    # 1. Scree Plot (Cumulative Variance)\n",
    "    fig_scree_omega = go.Figure()\n",
    "    for run_id in pca_models:\n",
    "        data = pca_models[run_id]\n",
    "        # Robustly get Time Point\n",
    "        tp_series = df_batch[df_batch['Run_ID'] == run_id]['Time_Point']\n",
    "        tp = tp_series.values[0] if not tp_series.empty else \"Unknown\"\n",
    "        \n",
    "        fig_scree_omega.add_trace(go.Scatter(\n",
    "            y=data['cumsum_omega'],\n",
    "            mode='lines',\n",
    "            name=f\"{tp} ({run_id[-15:]})\", # Shorten name for legend\n",
    "            hovertemplate=\"Comp: %{x}<br>Var: %{y:.3f}\"\n",
    "        ))\n",
    "\n",
    "    fig_scree_omega.add_hline(y=0.90, line_dash=\"dash\", annotation_text=\"90% Variance\")\n",
    "    fig_scree_omega.update_layout(title=\"Dynamics (Omega) - Scree Plot (Cumulative)\", \n",
    "                                  xaxis_title=\"Number of Components\", yaxis_title=\"Cumulative Variance\")\n",
    "\n",
    "    # 2. Scatter Plot (PC1 vs PC2) - Dynamics Density\n",
    "    fig_scatter_omega = go.Figure()\n",
    "    for run_id in pca_models:\n",
    "        proj = pca_models[run_id]['omega_proj']\n",
    "        tp_series = df_batch[df_batch['Run_ID'] == run_id]['Time_Point']\n",
    "        tp = tp_series.values[0] if not tp_series.empty else \"Unknown\"\n",
    "        \n",
    "        # Downsample for clearer plotting if large\n",
    "        if len(proj) > 2000:\n",
    "            idx = np.random.choice(len(proj), 2000, replace=False)\n",
    "            proj = proj[idx]\n",
    "            \n",
    "        fig_scatter_omega.add_trace(go.Scattergl(\n",
    "            x=proj[:, 0], y=proj[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(size=3, opacity=0.5),\n",
    "            name=f\"{tp} ({run_id[-8:]})\"\n",
    "        ))\n",
    "\n",
    "    fig_scatter_omega.update_layout(title=\"Dynamics Density (Omega PC1 vs PC2)\",\n",
    "                                    xaxis_title=\"PC1 (Dominant Velocity Pattern)\", \n",
    "                                    yaxis_title=\"PC2 (Secondary Pattern)\")\n",
    "\n",
    "    # 3. Trend Plot (N90 across Time)\n",
    "    # Sort by Time Point (T1, T2, T3...)\n",
    "    df_trend = df_pca_summary.copy()\n",
    "    \n",
    "    # Helper to sort T1, T2, T3\n",
    "    def sort_key(val):\n",
    "        if isinstance(val, str) and val.startswith('T') and val[1:].isdigit():\n",
    "            return int(val[1:])\n",
    "        return 99\n",
    "        \n",
    "    df_trend['TP_Sort'] = df_trend['Time_Point'].apply(sort_key)\n",
    "    df_trend = df_trend.sort_values('TP_Sort')\n",
    "\n",
    "    fig_trend_omega = px.bar(\n",
    "        df_trend, x='Time_Point', y='N90_Omega', \n",
    "        title=\"Dynamics Dimensionality (N90 Omega) Trend\",\n",
    "        text='N90_Omega', color='Time_Point',\n",
    "        hover_data=['Run_ID']\n",
    "    )\n",
    "    fig_trend_omega.update_traces(textposition='outside')\n",
    "\n",
    "    # Display\n",
    "    fig_scree_omega.show()\n",
    "    fig_scatter_omega.show()\n",
    "    fig_trend_omega.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2efcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: POSE TRACK (QUATERNION) VISUALIZATION (3+3 SUITE)\n",
    "# ============================================================\n",
    "\n",
    "if not pca_models:\n",
    "    print(\"No PCA models generated.\")\n",
    "else:\n",
    "    # 1. Scree Plot\n",
    "    fig_scree_quat = go.Figure()\n",
    "    for run_id in pca_models:\n",
    "        data = pca_models[run_id]\n",
    "        tp_series = df_batch[df_batch['Run_ID'] == run_id]['Time_Point']\n",
    "        tp = tp_series.values[0] if not tp_series.empty else \"Unknown\"\n",
    "        \n",
    "        fig_scree_quat.add_trace(go.Scatter(\n",
    "            y=data['cumsum_quat'],\n",
    "            mode='lines',\n",
    "            name=f\"{tp} ({run_id[-15:]})\"\n",
    "        ))\n",
    "\n",
    "    fig_scree_quat.add_hline(y=0.90, line_dash=\"dash\", annotation_text=\"90% Variance\")\n",
    "    fig_scree_quat.update_layout(title=\"Pose Space (Quaternion) - Scree Plot\", \n",
    "                                 xaxis_title=\"Number of Components\", yaxis_title=\"Cumulative Variance\")\n",
    "\n",
    "    # 2. Scatter Plot (PC1 vs PC2) - Pose Exploration\n",
    "    fig_scatter_quat = go.Figure()\n",
    "    for run_id in pca_models:\n",
    "        proj = pca_models[run_id]['quat_proj']\n",
    "        tp_series = df_batch[df_batch['Run_ID'] == run_id]['Time_Point']\n",
    "        tp = tp_series.values[0] if not tp_series.empty else \"Unknown\"\n",
    "        \n",
    "        if len(proj) > 2000:\n",
    "            idx = np.random.choice(len(proj), 2000, replace=False)\n",
    "            proj = proj[idx]\n",
    "            \n",
    "        fig_scatter_quat.add_trace(go.Scattergl(\n",
    "            x=proj[:, 0], y=proj[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(size=3, opacity=0.5),\n",
    "            name=f\"{tp} ({run_id[-8:]})\"\n",
    "        ))\n",
    "\n",
    "    fig_scatter_quat.update_layout(title=\"Pose Space Exploration (Quaternion PC1 vs PC2)\",\n",
    "                                   xaxis_title=\"PC1\", yaxis_title=\"PC2\")\n",
    "\n",
    "    # 3. Trend Plot\n",
    "    fig_trend_quat = px.bar(\n",
    "        df_trend, x='Time_Point', y='N90_Quat', \n",
    "        title=\"Pose Dimensionality (N90 Quaternion) Trend\",\n",
    "        text='N90_Quat', color='Time_Point',\n",
    "        hover_data=['Run_ID']\n",
    "    )\n",
    "    fig_trend_quat.update_traces(textposition='outside')\n",
    "\n",
    "    # Display\n",
    "    fig_scree_quat.show()\n",
    "    fig_scatter_quat.show()\n",
    "    fig_trend_quat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: AUTOMATED EXPORT\n",
    "# ============================================================\n",
    "\n",
    "# 1. Save Summary CSV\n",
    "summary_csv_path = os.path.join(RESULTS_DIR, f\"{batch_name}_pca_summary.csv\")\n",
    "df_pca_summary.to_csv(summary_csv_path, index=False)\n",
    "print(f\"âœ… Saved PCA Metrics: {summary_csv_path}\")\n",
    "\n",
    "# 2. Save Plots as HTML (Interactive)\n",
    "batch_clean = batch_name.replace(\" \", \"_\")\n",
    "\n",
    "# Check if figures exist before saving\n",
    "if 'fig_scree_omega' in locals():\n",
    "    fig_scree_omega.write_html(os.path.join(RESULTS_DIR, f\"{batch_clean}_Omega_Scree.html\"))\n",
    "    fig_scatter_omega.write_html(os.path.join(RESULTS_DIR, f\"{batch_clean}_Omega_Scatter.html\"))\n",
    "    fig_trend_omega.write_html(os.path.join(RESULTS_DIR, f\"{batch_clean}_Omega_N90_Trend.html\"))\n",
    "\n",
    "if 'fig_scree_quat' in locals():\n",
    "    fig_scree_quat.write_html(os.path.join(RESULTS_DIR, f\"{batch_clean}_Quat_Scree.html\"))\n",
    "    fig_scatter_quat.write_html(os.path.join(RESULTS_DIR, f\"{batch_clean}_Quat_Scatter.html\"))\n",
    "    fig_trend_quat.write_html(os.path.join(RESULTS_DIR, f\"{batch_clean}_Quat_N90_Trend.html\"))\n",
    "\n",
    "print(f\"âœ… All 6 Interactive Plots saved to: {RESULTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
