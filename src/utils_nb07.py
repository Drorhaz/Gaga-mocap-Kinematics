"""
utils_nb07.py - Utility functions for Master Quality Report (Notebook 07)

This module centralizes:
1. JSON loading and validation
2. Parameter extraction with safe access
3. Quality scoring functions
4. Excel export utilities

Author: Gaga Pipeline
Version: 2.0 (Optimized)
"""

import os
import json
import glob
import hashlib
import subprocess
import numpy as np
import pandas as pd
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Any, Optional, Tuple, Union


# ============================================================
# SCHEMA DEFINITION - All JSON parameters used in the report
# ============================================================

PARAMETER_SCHEMA = {
    "step_01": {
        "file_suffix": "__step01_loader_report.json",
        "description": "Raw data loader report - initial parsing of OptiTrack CSV exports. Validates file integrity, extracts metadata, and computes initial quality metrics. Generated by notebooks/01_parse_csv.ipynb",
        "parameters": {
            "identity.run_id": {"type": "str", "section": "S0", "description": "Unique recording identifier extracted from filename. Format: {SubjectID}_{SessionID}_{PoseID}_{RepID}_Take {timestamp}. Used as primary key throughout pipeline. Code: notebooks/01_parse_csv.ipynb -> parse_take_name()"},
            "identity.processing_timestamp": {"type": "str", "section": "S0", "description": "ISO 8601 timestamp when this pipeline step executed. Used for audit trail and reproducibility tracking. Format: YYYY-MM-DD HH:MM:SS"},
            "identity.pipeline_version": {"type": "str", "section": "S0", "description": "Semantic version of pipeline code (e.g., v2.6_calibration_enhanced). Enables backward compatibility checks and regression tracking. Set in config/config_v1.yaml"},
            "identity.csv_source": {"type": "str", "section": "S0", "description": "Relative path to source CSV file from project root. Preserves data lineage per Cereatti et al. (2024) recommendations for motion capture provenance"},
            "raw_data_quality.total_frames": {"type": "int", "section": "S2", "description": "Total number of frames (rows) in the raw CSV after header parsing. At 120Hz: 7200 frames = 1 minute. Used to validate recording completeness and compute duration"},
            "raw_data_quality.missing_data_percent": {"type": "str", "section": "S3", "description": "Percentage of NaN/empty cells in position and rotation columns. Formula: $\\frac{\\text{NaN cells}}{\\text{total cells}} \\times 100$. Threshold: <5% acceptable, >10% requires review. Code: notebooks/01_parse_csv.ipynb -> compute_missing_stats()"},
            "raw_data_quality.sampling_rate_actual": {"type": "float", "section": "S2", "description": "Computed sampling rate from timestamp column. Formula: $f_s = \\frac{N-1}{t_{N-1} - t_0}$ Hz. Expected: 120.0 Hz for OptiTrack. Deviation >1% triggers warning. Code: notebooks/01_parse_csv.ipynb -> compute_sampling_rate()"},
            "raw_data_quality.optitrack_mean_error_mm": {"type": "float", "section": "S1", "description": "OptiTrack system calibration error from Motive export metadata. Reflects volume calibration quality per Rácz et al. (2025) Calibration Layer. Thresholds: GOLD<0.5mm (research-grade), SILVER 0.5-1mm (acceptable), BRONZE>1mm (review needed). Extracted from CSV header"},
            "raw_data_quality.optitrack_version": {"type": "str", "section": "S0", "description": "OptiTrack Motive software version string from CSV header. Important for tracking API changes and known issues. Format: 'Motive X.Y.Z'"},
            "calibration.pointer_tip_rms_error_mm": {"type": "float", "section": "S1", "description": "RMS error of pointer tip during anatomical landmark digitization. Measures precision of virtual marker placement on bony landmarks. Formula: $RMS = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$. Threshold: <1mm for accurate joint center estimation. Ref: CAST technique (Cappozzo et al., 1995)"},
            "calibration.wand_error_mm": {"type": "float", "section": "S1", "description": "Wand calibration error for capture volume. Indicates spatial accuracy of marker reconstruction across the capture space. Threshold: <0.5mm for sub-millimeter accuracy. Affects all downstream position calculations"},
            "calibration.export_date": {"type": "str", "section": "S0", "description": "Date when data was exported from Motive. Format: YYYY-MM-DD. Used to correlate with calibration logs and session notes"},
            "skeleton_info.segments_found_count": {"type": "int", "section": "S1", "description": "Number of skeleton segments successfully parsed from CSV columns. Expected: 21 for full-body (Hips, Spine, Spine1, Neck, Head, L/R Shoulder/Arm/ForeArm/Hand, L/R UpLeg/Leg/Foot/ToeBase). Missing segments indicate tracking dropout or export issues"},
            "skeleton_info.segments_missing_count": {"type": "int", "section": "S1", "description": "Number of expected skeleton segments not found in CSV. Non-zero indicates partial export or tracking failure. Critical segments: Hips (root), Head (endpoint). Code: notebooks/01_parse_csv.ipynb -> validate_skeleton()"},
            "duration_sec": {"type": "float", "section": "S2", "description": "Recording duration in seconds. Formula: $T = \\frac{N_{frames}}{f_s}$. Minimum viable: 30s for statistical analysis, 60s+ recommended for movement characterization. Used for intensity normalization"},
        }
    },
    "step_02": {
        "file_suffix": "__preprocess_summary.json",
        "description": "Preprocessing summary - gap filling, quaternion normalization, and bone length stability analysis. Implements 'No Silent Fixes' transparency per Winter (2009). Generated by notebooks/02_preprocess.ipynb",
        "parameters": {
            "run_id": {"type": "str", "section": "S0", "description": "Recording identifier matching step_01. Ensures traceability across pipeline steps"},
            "raw_missing_percent": {"type": "float", "section": "S3", "description": "Missing data percentage BEFORE gap filling. Baseline for interpolation impact assessment. High values (>5%) indicate tracking occlusions or marker dropout. Code: notebooks/02_preprocess.ipynb -> compute_gap_statistics()"},
            "post_missing_percent": {"type": "float", "section": "S3", "description": "Missing data percentage AFTER gap filling. Should approach 0% if gaps were fillable. Remaining gaps indicate unfillable regions (too long or at boundaries). Delta from raw_missing shows interpolation effectiveness"},
            "max_interpolation_gap": {"type": "int", "section": "S3", "description": "Maximum gap length (frames) that was interpolated. Configurable threshold in config_v1.yaml. Default: 12 frames (100ms at 120Hz). Longer gaps are left unfilled to avoid hallucinating movement. Ref: Robertson et al. (2013) gap filling guidelines"},
            "bone_qc_mean_cv": {"type": "float", "section": "S1", "description": "Mean coefficient of variation for all bone segment lengths across recording. Formula: $\\overline{CV} = \\frac{1}{N_b}\\sum_{b=1}^{N_b}\\frac{\\sigma_b}{\\mu_b} \\times 100\\%$ where $\\sigma_b$ = std of bone b length, $\\mu_b$ = mean length. Measures skeleton rigidity assumption validity. Thresholds per Rácz et al. (2025): GOLD<0.5% (rigid body), SILVER 0.5-1% (acceptable), BRONZE 1-2% (soft tissue artifact), FAIL>2% (tracking error). Code: notebooks/02_preprocess.ipynb -> compute_bone_cv()"},
            "bone_qc_status": {"type": "str", "section": "S1", "description": "Categorical assessment of bone stability: GOLD/SILVER/BRONZE/FAIL. Based on mean CV thresholds. FAIL triggers pipeline warning and requires manual review. Used in score_calibration() quality scoring"},
            "bone_qc_alerts": {"type": "list", "section": "S1", "description": "List of specific bone segments with CV > threshold. Format: ['LeftForeArm: 1.8%', 'RightHand: 2.1%']. Identifies problematic segments for targeted review. High CV in distal segments (hands/feet) is common due to soft tissue"},
            "worst_bone": {"type": "str", "section": "S1", "description": "Bone segment with highest CV value. Primary candidate for visual inspection. Persistent issues in same bone across recordings may indicate marker placement problem or subject-specific anatomy"},
            "interpolation_method": {"type": "str", "section": "S3", "description": "Primary interpolation method used for gap filling. Options: 'linear_quaternion_normalized' (SLERP for rotations, linear for positions), 'cubic_spline', 'pchip'. SLERP preserves unit quaternion constraint. Code: notebooks/02_preprocess.ipynb -> interpolate_gaps()"},
        }
    },
    "step_03": {
        "file_suffix": "__resample_summary.json",
        "description": "Resampling summary - temporal grid uniformization and interpolation validation. Ensures constant sampling rate for frequency-domain analysis. Generated by notebooks/03_resample.ipynb",
        "parameters": {
            "run_id": {"type": "str", "section": "S0", "description": "Recording identifier for pipeline continuity"},
            "target_fs": {"type": "float", "section": "S2", "description": "Target uniform sampling frequency in Hz. Default: 120.0 Hz (matches OptiTrack native rate). All recordings resampled to this rate for cross-recording comparability. Set in config/config_v1.yaml -> fs_target"},
            "time_grid_std_dt": {"type": "float", "section": "S2", "description": "Standard deviation of time intervals after resampling. Formula: $\\sigma_{\\Delta t} = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N-1}(\\Delta t_i - \\overline{\\Delta t})^2}$. Ideal: 0.0 (perfectly uniform grid). Non-zero indicates numerical precision limits or edge effects. Code: notebooks/03_resample.ipynb -> validate_time_grid()"},
            "temporal_status": {"type": "str", "section": "S2", "description": "Categorical temporal grid quality: PERFECT (std=0), GOOD (std<0.1ms), REVIEW (std 0.1-1ms), FAIL (std>1ms). PERFECT required for accurate frequency analysis. Affects score_temporal_quality() scoring"},
            "interpolation_methods.positions": {"type": "str", "section": "S3", "description": "Interpolation method for 3D position data during resampling. 'CubicSpline' preferred for C2 continuity (smooth velocity/acceleration). Linear fallback for edge cases. Code: notebooks/03_resample.ipynb -> resample_positions()"},
            "interpolation_methods.rotations": {"type": "str", "section": "S3", "description": "Interpolation method for rotation data (quaternions). 'SLERP' (Spherical Linear Interpolation) required to maintain unit quaternion constraint and geodesic path on SO(3). Formula: $q(t) = \\frac{\\sin((1-t)\\theta)}{\\sin\\theta}q_0 + \\frac{\\sin(t\\theta)}{\\sin\\theta}q_1$. Ref: Shoemake (1985). Code: notebooks/03_resample.ipynb -> resample_rotations()"},
        }
    },
    "step_04": {
        "file_suffix": "__filtering_summary.json",
        "description": "Filtering summary - Winter residual analysis for optimal cutoff selection and Butterworth low-pass filtering. Implements data-driven frequency selection per Winter (2009). Generated by notebooks/04_filtering.ipynb",
        "parameters": {
            "run_id": {"type": "str", "section": "S0", "description": "Recording identifier for traceability"},
            "identity.timestamp": {"type": "str", "section": "S0", "description": "Processing timestamp for this filtering run. Useful for debugging filter parameter changes over time"},
            "identity.pipeline_version": {"type": "str", "section": "S0", "description": "Pipeline version at filtering time. Filter parameters may change between versions"},
            "subject_metadata.mass_kg": {"type": "float", "section": "S0", "description": "Subject body mass in kg from data/subject_metadata.json. Used for intensity index normalization and biomechanical plausibility checks. Default: 70kg if not specified"},
            "subject_metadata.height_cm": {"type": "float", "section": "S0", "description": "Subject height in cm from metadata. Used for anthropometric validation and scaling. Cross-checked against skeleton measurements"},
            "raw_quality.total_frames": {"type": "int", "section": "S2", "description": "Frame count at filtering stage. Should match step_01 unless trimming occurred"},
            "raw_quality.sampling_rate_actual": {"type": "float", "section": "S2", "description": "Confirmed sampling rate for Nyquist frequency calculation. Filter cutoff must be < fs/2"},
            "filter_params.filter_type": {"type": "str", "section": "S4", "description": "Human-readable filter description. e.g., 'Butterworth Low-Pass, Order 2, Zero-Lag (filtfilt)'. Documents exact filter configuration"},
            "filter_params.filter_method": {"type": "str", "section": "S4", "description": "Filter selection method. Options: 'Winter Residual Analysis' (data-driven), 'Fixed Cutoff' (manual), 'Per-Region Winter Cutoff Selection' (region-specific). Per-region applies different cutoffs to proximal vs distal segments"},
            "filter_params.filter_cutoff_hz": {"type": "float", "section": "S4", "description": "Selected low-pass filter cutoff frequency in Hz. Winter method finds knee-point: $f_c = \\arg\\min_{f} \\{RMS(f) : \\frac{d(RMS)}{df} < \\epsilon\\}$. Typical dance range: 4-12 Hz. Lower cutoff = more smoothing, higher = preserves fast movements. Code: notebooks/04_filtering.ipynb -> winter_residual_analysis()"},
            "filter_params.filter_range_hz": {"type": "list", "section": "S4", "description": "Search range [min, max] for Winter analysis. Default: [1, 12] Hz. Expanded to [1, 16] Hz for Gaga dance with rapid distal movements. Set in config_v1.yaml"},
            "filter_params.filter_order": {"type": "int", "section": "S4", "description": "Butterworth filter order N. Effective order = 2N due to forward-backward (filtfilt) application. Default: 2 (4th order effective). Higher order = sharper cutoff but more ringing. Code: scipy.signal.butter()"},
            "filter_params.winter_analysis_failed": {"type": "bool", "section": "S4", "description": "True if Winter analysis could not find valid knee-point. Causes: insufficient data, no clear noise floor, or residual curve doesn't converge. Triggers fallback to fixed cutoff (6 Hz default)"},
            "filter_params.winter_failure_reason": {"type": "str", "section": "S4", "description": "Diagnostic message when Winter analysis fails. e.g., 'No knee-point found - residual monotonically decreasing', 'Insufficient frequency range'. Guides manual review"},
            "filter_params.decision_reason": {"type": "str", "section": "S4", "description": "Human-readable rationale for cutoff selection. e.g., 'Winter knee-point at 8.5 Hz', 'Fallback to 6 Hz due to analysis failure', 'Per-region: Arms=10Hz, Legs=7Hz, Spine=5Hz'"},
            "filter_params.residual_rms_mm": {"type": "float", "section": "S4", "description": "RMS of filter residual (raw - filtered) at selected cutoff. Formula: $RMS = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_{raw,i}-x_{filt,i})^2}$. Quantifies 'price of smoothing' - how much signal removed. Policy: GOLD<15mm (minimal smoothing), SILVER 15-30mm (acceptable), REVIEW>30mm (excessive). Code: notebooks/04_filtering.ipynb -> compute_residual_rms()"},
            "filter_params.residual_slope": {"type": "float", "section": "S4", "description": "Slope of residual-vs-frequency curve at knee-point. Indicates convergence quality. Near-zero slope = clear transition between signal and noise. Steep slope = ambiguous cutoff region"},
            "filter_params.biomechanical_guardrails.enabled": {"type": "bool", "section": "S4", "description": "Whether biomechanical plausibility checks were applied post-filtering. Guardrails flag physically impossible values (e.g., >2000 deg/s angular velocity). Code: notebooks/04_filtering.ipynb -> apply_guardrails()"},
        }
    },
    "step_05": {
        "file_suffix": "__reference_summary.json",
        "description": "Reference detection - automatic static pose identification and anatomical calibration per Rácz et al. (2025). Establishes neutral pose for relative angle computation. Generated by notebooks/05_reference.ipynb",
        "parameters": {
            "run_id": {"type": "str", "section": "S0", "description": "Recording identifier for continuity"},
            "subject_context.height_cm": {"type": "float", "section": "S0", "description": "Subject height computed from skeleton in reference pose. Formula: $h = \\|p_{head} - p_{foot\\_midpoint}\\|_2$ in global frame. Cross-validated against metadata. Used for anthropometric scaling and sanity checks. Code: notebooks/05_reference.ipynb -> compute_subject_height()"},
            "subject_context.scaling_factor": {"type": "float", "section": "S0", "description": "Scale factor applied to normalize skeleton. Default: 1.0 (no scaling). Non-unity indicates size normalization for cross-subject comparison"},
            "subject_context.height_status": {"type": "str", "section": "S0", "description": "Height validation result: PASS (within 10% of metadata), REVIEW (10-20% deviation), FAIL (>20% or impossible value). Catches unit errors or wrong subject association"},
            "static_offset_audit.Left.measured_angle_deg": {"type": "float", "section": "S1", "description": "Left arm abduction angle from vertical in reference pose. Measures T-pose deviation. Formula: $\\theta = \\arccos(\\hat{v}_{arm} \\cdot \\hat{v}_{vertical})$. Threshold per Rácz: <15° acceptable. Used to correct systematic arm offset in joint angles. Code: notebooks/05_reference.ipynb -> compute_arm_offset()"},
            "static_offset_audit.Right.measured_angle_deg": {"type": "float", "section": "S1", "description": "Right arm abduction angle from vertical in reference pose. Asymmetry between L/R may indicate postural habit or marker placement inconsistency"},
            "window_metadata.start_time_sec": {"type": "float", "section": "S5", "description": "Start time (seconds) of detected static reference window. Window selected for minimal motion variance across key joints (Hips, Hands). Code: notebooks/05_reference.ipynb -> detect_static_window()"},
            "window_metadata.end_time_sec": {"type": "float", "section": "S5", "description": "End time (seconds) of static window. Typical duration: 0.5-2.0 seconds. Longer windows reduce noise but may include micro-movements"},
            "window_metadata.variance_score": {"type": "float", "section": "S5", "description": "Motion variance in detected window. Formula: $V = \\sum_{j \\in joints} \\text{Var}(p_j(t))$ for t in window. Lower = more stable reference. Used to rank candidate windows"},
            "window_metadata.ref_quality_score": {"type": "float", "section": "S5", "description": "Normalized reference quality score [0-1]. Combines variance, duration, and temporal position factors. Formula: $Q = w_1(1-V_{norm}) + w_2 D_{norm} + w_3 T_{penalty}$. Threshold: >0.7 for HIGH confidence. Code: notebooks/05_reference.ipynb -> score_reference_window()"},
            "window_metadata.confidence_level": {"type": "str", "section": "S5", "description": "Categorical confidence: HIGH (Q>0.8, clear static period), MEDIUM (0.6-0.8, acceptable), LOW (<0.6, no clear static pose). LOW triggers fallback to first-frame reference"},
            "window_metadata.detection_method": {"type": "str", "section": "S5", "description": "Algorithm used: 'auto_stable_window' (sliding window variance minimization), 'first_frame_fallback' (if no stable period found), 'manual_override' (user-specified)"},
            "metadata.grade": {"type": "str", "section": "S5", "description": "Overall reference quality grade: HIGH/MEDIUM/LOW. Aggregates confidence and offset severity. Used in score_reference() quality scoring"},
            "metadata.status": {"type": "str", "section": "S5", "description": "Reference lock status: 'LOCKED' (validated, used for all angle computations), 'PROVISIONAL' (needs manual confirmation), 'REJECTED' (unusable, pipeline halted)"},
        }
    },
    "step_06": {
        "file_suffix": "__validation_report.json",
        "description": "Kinematics summary - angular velocity computation via quaternion differentiation, outlier detection, and movement characterization. Final QC gate before research use. Generated by notebooks/06_rotvec_omega.ipynb",
        "parameters": {
            "run_id": {"type": "str", "section": "S0", "description": "Recording identifier completing the pipeline chain"},
            "overall_status": {"type": "str", "section": "S8", "description": "Pipeline completion status. Values: 'COMPLETED_STEP_06' (full pipeline success), 'PARTIAL' (some joints failed), 'FAILED' (critical error). Logged to overall_gate_status for batch processing"},
            "metrics.angular_velocity.max": {"type": "float", "section": "S6", "description": "Maximum angular velocity across all joints and frames (deg/s). Formula: $\\omega_{max} = \\max_{j,t} \\|\\omega_j(t)\\|$ where $\\omega = 2 \\cdot \\frac{dq}{dt} \\cdot q^*$ (quaternion derivative method). Physiological limits: Normal movement <800, Athletic <1500, Gaga dance up to 2250 deg/s (distal segments). Exceeding 2500 suggests tracking artifact. Code: notebooks/06_rotvec_omega.ipynb -> compute_angular_velocity()"},
            "metrics.angular_velocity.limit": {"type": "float", "section": "S6", "description": "Configurable physiological limit for angular velocity (deg/s). Default: 1500 (athletic baseline). Set in config_v1.yaml. Used for outlier classification threshold"},
            "metrics.angular_accel.max": {"type": "float", "section": "S6", "description": "Maximum angular acceleration (deg/s²). Formula: $\\alpha_{max} = \\max_{j,t} \\|\\frac{d\\omega}{dt}\\|$. Computed via Savitzky-Golay differentiation of angular velocity. Thresholds: ACCEPTABLE<30k (smooth movement), HIGH 30-50k (rapid transitions), EXTREME>50k (potential artifact or impact). Code: notebooks/06_rotvec_omega.ipynb -> compute_angular_acceleration()"},
            "metrics.linear_accel.max": {"type": "float", "section": "S6", "description": "Maximum linear acceleration of any segment endpoint (mm/s²). Injury risk indicator for impacts. Formula: $a_{max} = \\max_{j,t} \\|\\ddot{p}_j(t)\\|$. Thresholds: SAFE<50k (normal), ELEVATED 50-100k (high intensity), SEVERE>100k (impact/fall). Units converted to m/s² for comparison with literature"},
            "signal_quality.avg_residual_rms": {"type": "float", "section": "S7", "description": "Mean RMS residual across all filtered signals. Formula: $\\overline{RMS} = \\frac{1}{N_j}\\sum_{j=1}^{N_j} RMS_j$. Aggregates 'price of smoothing' from step_04. Policy: GOLD<15mm (minimal distortion), SILVER 15-30mm (acceptable for research), REVIEW>30mm (filter may be removing signal). Code: notebooks/06_rotvec_omega.ipynb -> aggregate_signal_quality()"},
            "signal_quality.max_quat_norm_err": {"type": "float", "section": "S2", "description": "Maximum quaternion normalization error across all frames. Formula: $\\epsilon_{max} = \\max_{j,t} |\\|q_{j,t}\\| - 1|$. Should be <1e-6 after normalization. Values >0.01 indicate numerical instability or corrupted quaternions. ISB compliance requires unit quaternions. Code: notebooks/06_rotvec_omega.ipynb -> validate_quaternion_norms()"},
            "movement_metrics.path_length_mm": {"type": "float", "section": "S6", "description": "Total 3D path length of root segment (Hips) in mm. Formula: $L = \\sum_{t=1}^{N-1} \\|p(t+1) - p(t)\\|$. Measures total displacement during recording. Used for movement intensity characterization and cross-recording normalization"},
            "movement_metrics.intensity_index": {"type": "float", "section": "S6", "description": "Normalized movement intensity index. Formula: $I = \\frac{\\sum_j \\sum_t \\|\\omega_j(t)\\| \\cdot \\Delta t}{m \\cdot T}$ where m=body mass (kg), T=duration. Units: (deg·s)/(kg·s) = deg/kg. Enables cross-subject comparison. Higher values indicate more vigorous movement. Code: notebooks/06_rotvec_omega.ipynb -> compute_intensity_index()"},
            "outlier_analysis.counts.total_outliers": {"type": "int", "section": "S6", "description": "Total frames flagged as outliers across all joints. Outlier = exceeds any threshold (velocity, acceleration, or jerk). Used for data_usable determination. Policy: >5% triggers REVIEW"},
            "outlier_analysis.percentages.total_outliers": {"type": "float", "section": "S6", "description": "Percentage of frames with at least one outlier. Formula: $\\frac{N_{outlier}}{N_{total}} \\times 100$. Key decision metric. Thresholds: <1% ACCEPT, 1-5% REVIEW, >5% requires manual inspection. Code: notebooks/06_rotvec_omega.ipynb -> compute_outlier_statistics()"},
            "outlier_analysis.consecutive_runs.max_consecutive_any_outlier": {"type": "int", "section": "S6", "description": "Longest run of consecutive outlier frames. Classification per burst analysis: 1-3 frames = Artifact (spike), 4-7 frames = Burst (rapid movement), 8+ frames = Flow (sustained high intensity). Artifacts excluded from clean statistics; Bursts/Flows preserved as legitimate Gaga movement. Code: notebooks/06_rotvec_omega.ipynb -> classify_outlier_runs()"},
            "pipeline_params.sg_window_sec": {"type": "float", "section": "S6", "description": "Savitzky-Golay filter window for differentiation (seconds). Default: 0.175s (21 frames at 120Hz). Balances noise reduction vs. temporal resolution. Polynomial order fixed at 3. Code: scipy.signal.savgol_filter()"},
            "pipeline_params.fs_target": {"type": "float", "section": "S2", "description": "Confirmed target sampling rate used for all time-derivative calculations. Must match step_03 target_fs for consistent frequency analysis"},
        }
    }
}

# Section descriptions for documentation
SECTION_DESCRIPTIONS = {
    "S0": "Data Lineage & Provenance",
    "S1": "Rácz Calibration Layer",
    "S2": "Temporal Quality & Sampling",
    "S3": "Gap & Interpolation Transparency",
    "S4": "Winter's Residual Validation",
    "S5": "Reference Detection & Stability",
    "S6": "Biomechanics & Outlier Analysis",
    "S7": "Signal-to-Noise Quantification",
    "S8": "Decision Matrix"
}

# ============================================================
# METHODOLOGY PASSPORT - Mathematical Documentation
# ============================================================

# ============================================================
# ANATOMICAL REGION MAPPING (for human-readable reports)
# ============================================================

ANATOMICAL_REGIONS = {
    "Neck": ["Neck"],
    "Shoulders": ["LeftShoulder", "RightShoulder"],
    "Elbows": ["LeftForeArm", "RightForeArm"],  # ForeArm joint represents elbow-to-wrist segment
    "Wrists": ["LeftHand", "RightHand"],  # Hand joint represents the wrist
    "Spine": ["Spine", "Spine1"],  # Mid-back / thoracic region
    "Hips": ["Hips", "LeftUpLeg", "RightUpLeg"],  # Pelvis + hip joints
    "Knees": ["LeftLeg", "RightLeg"],  # Leg joint represents knee-to-ankle segment
    "Ankles": ["LeftFoot", "RightFoot"],  # Foot joint represents the ankle
}

# Bilateral pair naming for symmetry analysis (user-friendly labels)
BILATERAL_PAIR_LABELS = {
    "upper_arm": "Upper Arms",
    "forearm": "Elbows/Forearms",
    "hand": "Wrists/Hands",
    "thigh": "Hips/Thighs",
    "shin": "Knees/Shins",
    "foot": "Ankles/Feet",
}

METHODOLOGY_PASSPORT = {
    "interpolation": {
        "rotations": {
            "method": "SLERP (Spherical Linear Interpolation)",
            "formula": "q(t) = sin((1-t)θ)/sin(θ) · q₀ + sin(tθ)/sin(θ) · q₁",
            "constraint": "Maintains unit quaternion: ||q|| = 1",
            "geodesic": "Shortest path on SO(3) manifold",
            "reference": "Shoemake (1985)",
            "implementation": "scipy.spatial.transform.Slerp"
        },
        "positions": {
            "method": "CubicSpline",
            "continuity": "C² (smooth velocity and acceleration)",
            "formula": "p(t) = a₀ + a₁t + a₂t² + a₃t³ (piecewise)",
            "constraint": "Natural boundary conditions",
            "implementation": "scipy.interpolate.CubicSpline"
        }
    },
    "differentiation": {
        "angular_velocity": {
            "method": "Quaternion Derivative",
            "formula": "ω = 2 · (dq/dt) · q*",
            "derivation": "q̇ via finite differences, then ω = 2q̇q* (quaternion conjugate)",
            "units": "deg/s",
            "note": "Extracts instantaneous axis-angle velocity from quaternion time series"
        },
        "angular_acceleration": {
            "method": "Savitzky-Golay Filter",
            "formula": "α = d/dt(ω) via least-squares polynomial fitting",
            "window_sec": 0.175,
            "window_frames": 21,
            "polynomial_order": 3,
            "units": "deg/s²",
            "reference": "Savitzky & Golay (1964)",
            "implementation": "scipy.signal.savgol_filter with mode='interp'"
        },
        "linear_velocity": {
            "method": "CubicSpline Analytical Derivative",
            "formula": "v = dp/dt (analytical derivative of cubic spline)",
            "units": "mm/s",
            "note": "Exact derivative, not finite-difference approximation"
        },
        "linear_acceleration": {
            "method": "CubicSpline Second Derivative",
            "formula": "a = d²p/dt² (analytical second derivative)",
            "units": "mm/s²",
            "note": "Exact second derivative from spline coefficients"
        }
    },
    "filtering": {
        "pipeline_version": "v3.0_3stage_signal_cleaning",
        "philosophy": "Artifact removal with movement preservation",
        "stage1_artifact_detection": {
            "method": "Z-Score + Velocity Threshold",
            "velocity_limit_mm_s": 5000.0,
            "zscore_threshold": 5.0,
            "interpolation": "PCHIP (Piecewise Cubic Hermite Interpolating Polynomial)",
            "purpose": "Remove physically impossible spikes (tracking glitches)"
        },
        "stage2_hampel": {
            "method": "Sliding Window Median Filter",
            "window_size": 5,
            "n_sigma": 3.0,
            "purpose": "Remove isolated outliers while preserving edges",
            "note": "Median-based, robust to non-Gaussian noise"
        },
        "stage3_adaptive_winter": {
            "method": "Winter's Residual Analysis (1990)",
            "strategy": "Per-Region Adaptive Cutoff Selection",
            "fmin_hz": 1.0,
            "fmax_hz": 20.0,
            "regions": {
                "head": "Head markers",
                "upper_proximal": "Shoulders, upper arms",
                "trunk": "Spine segments",
                "lower_distal": "Feet, ankles",
                "lower_proximal": "Thighs, hips",
                "upper_distal": "Hands, forearms"
            },
            "filter_type": "Butterworth Low-Pass",
            "filter_order": 2,
            "implementation": "Zero-lag (filtfilt) - forward-backward pass",
            "rationale": "Different body regions have different frequency content (distal faster than proximal)"
        }
    },
    "resampling": {
        "target_fs_hz": 120.0,
        "purpose": "Uniform temporal grid for frequency-domain analysis",
        "method": "Interpolate to exact 1/120s intervals (8.333ms)",
        "positions_method": "CubicSpline",
        "rotations_method": "SLERP",
        "validation": "Time grid standard deviation < 0.001ms"
    },
    "reference_alignment": {
        "method": "ISB/CAST Static Pose Detection",
        "reference": "Rácz et al. (2025) - Anatomical Calibration Layer",
        "detection": "Sliding window variance minimization (1-2 second stable period)",
        "offset_correction": "V-pose to T-pose quaternion transformation",
        "bilateral_correction": "Left/Right arm abduction offset removal"
    }
}


# ============================================================
# SAFE ACCESS UTILITIES
# ============================================================

def _extract_validation_status(s04: dict) -> Dict[str, str]:
    """Extract Winter validation status per region (for fixed cutoff mode)."""
    details = safe_get_path(s04, "filter_params.region_analysis_details", default={})
    if not isinstance(details, dict):
        return {}
    return {region: info.get('validation_status', 'N/A') for region, info in details.items()}


def _extract_rms_knee_per_region(s04: dict) -> Dict[str, float]:
    """Extract Winter RMS knee (strict_knee) per region - where RMS curve flattens."""
    details = safe_get_path(s04, "filter_params.region_analysis_details", default={})
    if not isinstance(details, dict):
        return {}
    result = {}
    for region, info in details.items():
        # Try winter_strict_knee_hz first, then winter_suggested_hz as fallback
        knee = info.get('winter_strict_knee_hz') or info.get('winter_suggested_hz', 'N/A')
        result[region] = knee
    return result


def _extract_diminishing_per_region(s04: dict) -> Dict[str, float]:
    """Extract Winter diminishing returns frequency per region - where biggest RMS drop happens."""
    details = safe_get_path(s04, "filter_params.region_analysis_details", default={})
    if not isinstance(details, dict):
        return {}
    result = {}
    for region, info in details.items():
        diminishing = info.get('winter_diminishing_hz', 'N/A')
        result[region] = diminishing
    return result


def _frames_to_ranges_str(frames, max_ranges: int = 10) -> str:
    """
    Convert list of frame indices to readable range string.
    e.g., [1,2,3,7,8,10] -> "1-3, 7-8, 10"
    """
    if not frames or frames == 'N/A':
        return 'None'
    
    if isinstance(frames, str):
        return frames
    
    try:
        frames = sorted([int(f) for f in frames])
    except (TypeError, ValueError):
        return str(frames)[:100]
    
    if not frames:
        return 'None'
    
    ranges = []
    start = frames[0]
    end = frames[0]
    
    for f in frames[1:]:
        if f == end + 1:
            end = f
        else:
            if start == end:
                ranges.append(str(start))
            else:
                ranges.append(f"{start}-{end}")
            start = f
            end = f
    
    # Handle last range
    if start == end:
        ranges.append(str(start))
    else:
        ranges.append(f"{start}-{end}")
    
    # Limit output length
    if len(ranges) > max_ranges:
        return f"{', '.join(ranges[:max_ranges])}... (+{len(ranges)-max_ranges} more)"
    
    return ', '.join(ranges) if ranges else 'None'


def safe_get(d: dict, *keys, default='N/A') -> Any:
    """
    Safe nested dictionary access with default fallback.
    
    Args:
        d: Dictionary to access
        *keys: Sequence of keys for nested access
        default: Default value if key not found
        
    Returns:
        Value at nested path or default
    """
    for key in keys:
        if isinstance(d, dict):
            d = d.get(key, {})
        else:
            return default
    return d if (d != {} and d is not None) else default


def safe_get_path(d: dict, path: str, default='N/A') -> Any:
    """
    Safe nested dictionary access using dot-notation path.
    
    Args:
        d: Dictionary to access
        path: Dot-separated path (e.g., "identity.run_id")
        default: Default value if path not found
        
    Returns:
        Value at path or default
    """
    keys = path.split('.')
    return safe_get(d, *keys, default=default)


def safe_float(x, default=0.0) -> float:
    """
    Convert to float safely, handling %, None, N/A.
    
    Args:
        x: Value to convert
        default: Default if conversion fails
        
    Returns:
        Float value
    """
    if x is None or x == 'N/A':
        return default
    try:
        if isinstance(x, str):
            x = x.replace('%', '').strip()
        return float(x)
    except (ValueError, TypeError):
        return default


def safe_int(x, default=0) -> int:
    """Convert to int safely."""
    try:
        return int(safe_float(x, default))
    except (ValueError, TypeError):
        return default


# ============================================================
# FILE DISCOVERY & LOADING
# ============================================================

def discover_json_files(deriv_root: str) -> Dict[str, Dict[str, str]]:
    """
    Discover all valid JSON summary files grouped by run_id.
    
    Args:
        deriv_root: Path to derivatives folder
        
    Returns:
        Dict mapping run_id -> {step_name: file_path}
    """
    # Scan for all JSON files
    json_files = glob.glob(os.path.join(deriv_root, "**", "*.json"), recursive=True)
    
    # Filter out archive folder
    json_files = [f for f in json_files if "archive" not in f.lower()]
    
    # Group by run_id and step
    runs = defaultdict(dict)
    
    for json_path in json_files:
        filename = os.path.basename(json_path)
        
        # Match against known suffixes
        for step_name, step_info in PARAMETER_SCHEMA.items():
            suffix = step_info["file_suffix"]
            if filename.endswith(suffix):
                run_id = filename.replace(suffix, "")
                runs[run_id][step_name] = json_path
                break
    
    return dict(runs)


def load_json_safe(filepath: str) -> Optional[dict]:
    """
    Load JSON file with error handling.
    
    Args:
        filepath: Path to JSON file
        
    Returns:
        Parsed JSON dict or None on error
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"⚠️ Failed to load {filepath}: {e}")
        return None


def load_all_runs(deriv_root: str) -> Dict[str, Dict[str, dict]]:
    """
    Load all JSON data for all runs.
    
    Args:
        deriv_root: Path to derivatives folder
        
    Returns:
        Dict mapping run_id -> {step_name: json_data}
    """
    file_map = discover_json_files(deriv_root)
    
    runs_data = {}
    for run_id, files in file_map.items():
        runs_data[run_id] = {}
        for step_name, filepath in files.items():
            data = load_json_safe(filepath)
            if data:
                runs_data[run_id][step_name] = data
    
    return runs_data


def filter_complete_runs(runs_data: Dict[str, Dict[str, dict]], 
                         required_steps: List[str] = None) -> Dict[str, Dict[str, dict]]:
    """
    Filter to runs that have all required steps.
    
    Args:
        runs_data: All loaded run data
        required_steps: List of required step names (default: step_01, step_06)
        
    Returns:
        Filtered dict with only complete runs
    """
    if required_steps is None:
        required_steps = ["step_01", "step_06"]
    
    return {
        run_id: steps 
        for run_id, steps in runs_data.items()
        if all(step in steps for step in required_steps)
    }


# ============================================================
# PARAMETER EXTRACTION
# ============================================================

def extract_all_parameters(run_id: str, steps: Dict[str, dict]) -> Dict[str, Any]:
    """
    Extract all parameters from a run's JSON files.
    
    Args:
        run_id: The run identifier
        steps: Dict of step_name -> json_data
        
    Returns:
        Flat dict of all extracted parameters with metadata
    """
    extracted = {"run_id": run_id}
    
    for step_name, step_schema in PARAMETER_SCHEMA.items():
        step_data = steps.get(step_name, {})
        
        for param_path, param_info in step_schema["parameters"].items():
            # Create unique key
            key = f"{step_name}.{param_path}"
            
            # Extract value
            value = safe_get_path(step_data, param_path)
            
            extracted[key] = {
                "value": value,
                "type": param_info["type"],
                "section": param_info["section"],
                "description": param_info["description"],
                "found": value != 'N/A'
            }
    
    return extracted


def extract_parameters_flat(run_id: str, steps: Dict[str, dict]) -> Dict[str, Any]:
    """
    Extract all parameters as a flat dict (values only).
    
    Args:
        run_id: The run identifier
        steps: Dict of step_name -> json_data
        
    Returns:
        Flat dict of parameter_key -> value
    """
    flat = {"Run_ID": run_id}
    
    for step_name, step_schema in PARAMETER_SCHEMA.items():
        step_data = steps.get(step_name, {})
        
        for param_path, param_info in step_schema["parameters"].items():
            # Create readable key
            key = f"{step_name}_{param_path}".replace(".", "_")
            
            # Extract value
            value = safe_get_path(step_data, param_path)
            flat[key] = value
    
    return flat


# ============================================================
# QUALITY SCORING FUNCTIONS
# ============================================================

def score_calibration(steps: Dict[str, dict]) -> float:
    """Score calibration quality (0-100)."""
    s01 = steps.get("step_01", {})
    s02 = steps.get("step_02", {})
    s05 = steps.get("step_05", {})
    
    score = 100.0
    
    # OptiTrack error
    optitrack_err = safe_float(safe_get_path(s01, "raw_data_quality.optitrack_mean_error_mm"))
    if optitrack_err > 1.0:
        score -= 20
    
    # Bone stability
    bone_cv = safe_float(safe_get_path(s02, "bone_qc_mean_cv"))
    if bone_cv > 1.5:
        score -= 30
    elif bone_cv > 1.0:
        score -= 15
    elif bone_cv > 0.5:
        score -= 5
    
    # Static offset
    left_offset = safe_float(safe_get_path(s05, "static_offset_audit.Left.measured_angle_deg"))
    right_offset = safe_float(safe_get_path(s05, "static_offset_audit.Right.measured_angle_deg"))
    max_offset = max(abs(left_offset), abs(right_offset))
    if max_offset > 15:
        score -= 20
    elif max_offset > 10:
        score -= 10
    
    return max(0, score)


def score_temporal_quality(steps: Dict[str, dict]) -> float:
    """Score temporal/sampling quality (0-100) - includes Gate 2 metrics and step_03 validation."""
    s01 = steps.get("step_01", {})
    s02 = steps.get("step_02", {})
    s03 = steps.get("step_03", {})
    
    score = 100.0
    
    # Check sampling rate (expect ~120 Hz)
    fs = safe_float(safe_get_path(s01, "raw_data_quality.sampling_rate_actual"))
    if fs < 115 or fs > 125:
        score -= 20
    
    # Check duration (minimum viable)
    duration = safe_float(safe_get_path(s01, "duration_sec"))
    if duration < 30:
        score -= 30
    elif duration < 60:
        score -= 10
    
    # =================================================================
    # STEP 03: Resampling Temporal Grid Validation
    # =================================================================
    temporal_status = safe_get_path(s03, "temporal_status", default="N/A")
    if temporal_status == "FAIL":
        score -= 30  # Critical temporal issues
    elif temporal_status == "REVIEW":
        score -= 15  # Needs manual review
    elif temporal_status == "GOOD":
        score -= 5   # Minor issues
    # PERFECT = no penalty
    
    # Check time grid standard deviation (should be ~0 for uniform grid)
    time_grid_std = safe_float(safe_get_path(s03, "time_grid_std_dt"))
    if time_grid_std > 0.001:  # More than 1ms jitter after resampling
        score -= 10  # Resampling didn't fully normalize the grid
    
    # =================================================================
    # GATE 2: Sample Time Jitter (pre-resampling)
    # =================================================================
    jitter_ms = safe_float(safe_get_path(s02, "step_02_sample_time_jitter_ms"))
    if jitter_ms > 0:  # Only penalize if we have jitter data
        if jitter_ms > 5.0:
            score -= 25  # Severe jitter
        elif jitter_ms > 2.0:
            score -= 15  # High jitter (REVIEW threshold)
        elif jitter_ms > 1.0:
            score -= 5   # Moderate jitter
    
    # Gate 2: Interpolation fallback rate
    fallback_rate = safe_float(safe_get_path(s02, "step_02_fallback_rate_percent"))
    if fallback_rate > 15:
        score -= 20  # REJECT threshold
    elif fallback_rate > 5:
        score -= 10  # REVIEW threshold
    
    return max(0, score)


def score_interpolation(steps: Dict[str, dict]) -> float:
    """Score interpolation/gap filling quality (0-100) - validates step_02 and step_03."""
    s02 = steps.get("step_02", {})
    s03 = steps.get("step_03", {})
    
    score = 100.0
    
    # Missing data percentage
    raw_missing = safe_float(safe_get_path(s02, "raw_missing_percent"))
    if raw_missing > 5:
        score -= 40
    elif raw_missing > 2:
        score -= 20
    elif raw_missing > 0.5:
        score -= 10
    
    # Interpolation method penalty (step_02)
    method = safe_get_path(s02, "interpolation_method", default="")
    if "linear" in str(method).lower() and "quaternion" not in str(method).lower():
        score -= 10  # Linear fallback penalty
    
    # =================================================================
    # STEP 03: Validate rotation interpolation uses SLERP
    # =================================================================
    rotation_method = safe_get_path(s03, "interpolation_methods.rotations", default="")
    rotation_method_str = str(rotation_method).upper()
    
    # SLERP is the gold standard for rotation interpolation
    if rotation_method != 'N/A' and rotation_method:
        if "SLERP" not in rotation_method_str and "QUATERNION" not in rotation_method_str:
            score -= 15  # Non-SLERP rotation interpolation is problematic
    
    # Validate position interpolation (CubicSpline preferred)
    position_method = safe_get_path(s03, "interpolation_methods.positions", default="")
    position_method_str = str(position_method).lower()
    
    if position_method != 'N/A' and position_method:
        if "linear" in position_method_str and "spline" not in position_method_str:
            score -= 5  # Linear position interpolation is less smooth
    
    return max(0, score)


def score_filtering(steps: Dict[str, dict]) -> float:
    """
    Score filtering quality based on Winter analysis (0-100).
    
    GATE 3 FIX: Properly handle per-region filtering mode where successful
    region-specific cutoff selection should score 100, not penalize.
    """
    s04 = steps.get("step_04", {})
    
    score = 100.0
    
    # GATE 3: Check filtering mode
    filtering_mode = safe_get_path(s04, "filter_params.filtering_mode", default="single_global")
    
    # Check if Winter analysis succeeded
    winter_failed = safe_get_path(s04, "filter_params.winter_analysis_failed")
    
    if filtering_mode == "per_region":
        # Per-region filtering: Success = all regions found knee-points
        # If winter_failed is False, all regions succeeded -> Score 100
        if winter_failed:
            score -= 30  # One or more regions failed to find knee-point
        
        # Check if cutoff range is reasonable (weighted average should be in dance range)
        cutoff = safe_float(safe_get_path(s04, "filter_params.filter_cutoff_hz"))
        if cutoff > 0:  # Only check if we have a valid weighted average
            # For per-region, we expect higher cutoffs for distal segments
            # Acceptable range: 4-16 Hz (expanded for Gaga)
            if cutoff < 4 or cutoff > 16:
                score -= 20
        
        # Per-region mode doesn't use guardrails the same way - skip that check
        
    else:
        # Single global cutoff mode (original logic)
        if winter_failed:
            score -= 30
        
        # Check cutoff is in reasonable range (4-12 Hz for dance)
        cutoff = safe_float(safe_get_path(s04, "filter_params.filter_cutoff_hz"))
        if cutoff < 4 or cutoff > 12:
            score -= 20
        
        # Check guardrails enabled
        guardrails = safe_get_path(s04, "filter_params.biomechanical_guardrails.enabled")
        if not guardrails:
            score -= 10
    
    return max(0, score)


def score_reference(steps: Dict[str, dict]) -> float:
    """Score reference detection quality (0-100)."""
    s05 = steps.get("step_05", {})
    
    score = 100.0
    
    # Reference quality score
    ref_quality = safe_float(safe_get_path(s05, "window_metadata.ref_quality_score"))
    if ref_quality < 0.5:
        score -= 40
    elif ref_quality < 0.7:
        score -= 20
    elif ref_quality < 0.8:
        score -= 10
    
    # Confidence level
    confidence = safe_get_path(s05, "window_metadata.confidence_level")
    if confidence == "LOW":
        score -= 20
    elif confidence == "MEDIUM":
        score -= 10
    
    # Grade
    grade = safe_get_path(s05, "metadata.grade")
    if grade == "LOW":
        score -= 20
    
    return max(0, score)


def score_biomechanics(steps: Dict[str, dict]) -> Tuple[float, Dict[str, Any]]:
    """
    Score biomechanical plausibility (0-100) with detailed component breakdown.
    
    This function implements the 3-component weighted scorecard:
    - 40% Physiological Plausibility (velocity/acceleration within human limits)
    - 30% Skeleton Stability (bone length consistency)
    - 30% Movement Continuity (burst classification, data quality)
    
    Args:
        steps: Dict of step_name -> json_data
        
    Returns:
        Tuple of (overall_score, detailed_scorecard)
    """
    s02 = steps.get("step_02", {})
    s06 = steps.get("step_06", {})
    
    # =================================================================
    # COMPONENT 1: PHYSIOLOGICAL PLAUSIBILITY (40% weight)
    # =================================================================
    physiological_score = 100.0
    physiological_details = {}
    
    # Use CLEAN velocity (artifacts excluded) - Gate 5 neutralization
    clean_max_vel = safe_float(safe_get_path(s06, "clean_statistics.clean_statistics.max_deg_s"))
    raw_max_vel = safe_float(safe_get_path(s06, "metrics.angular_velocity.max"))
    
    # Fallback to raw if clean not available
    max_ang_vel = clean_max_vel if clean_max_vel > 0 else raw_max_vel
    using_clean = clean_max_vel > 0
    
    physiological_details['max_velocity_deg_s'] = round(max_ang_vel, 2)
    physiological_details['velocity_source'] = 'clean' if using_clean else 'raw'
    
    # Velocity assessment (human physiological limits)
    limit = safe_float(safe_get_path(s06, "metrics.angular_velocity.limit"), default=1500)
    if max_ang_vel > limit * 1.5:  # 2250 deg/s
        physiological_score -= 40
        physiological_details['velocity_assessment'] = 'SEVERE_EXCESS'
    elif max_ang_vel > limit:  # 1500 deg/s
        physiological_score -= 20
        physiological_details['velocity_assessment'] = 'MODERATE_EXCESS'
    else:
        physiological_details['velocity_assessment'] = 'PLAUSIBLE'
    
    # Angular acceleration check (use clean if available)
    clean_max_accel = safe_float(safe_get_path(s06, "clean_statistics.clean_statistics.max_accel_deg_s2"))
    raw_max_accel = safe_float(safe_get_path(s06, "metrics.angular_accel.max"))
    max_accel = clean_max_accel if clean_max_accel > 0 else raw_max_accel
    
    physiological_details['max_acceleration_deg_s2'] = round(max_accel, 2)
    
    if max_accel > 50000:
        physiological_score -= 30
        physiological_details['acceleration_assessment'] = 'EXTREME'
    elif max_accel > 30000:
        physiological_score -= 15
        physiological_details['acceleration_assessment'] = 'HIGH'
    else:
        physiological_details['acceleration_assessment'] = 'ACCEPTABLE'
    
    # Linear acceleration (injury risk indicator)
    max_lin_accel = safe_float(safe_get_path(s06, "metrics.linear_accel.max"))
    physiological_details['max_linear_accel_m_s2'] = round(max_lin_accel, 2)
    
    if max_lin_accel > 100:  # Extreme impact force
        physiological_score -= 20
        physiological_details['linear_accel_assessment'] = 'SEVERE'
    elif max_lin_accel > 50:
        physiological_score -= 10
        physiological_details['linear_accel_assessment'] = 'ELEVATED'
    else:
        physiological_details['linear_accel_assessment'] = 'SAFE'
    
    physiological_score = max(0, physiological_score)
    
    # =================================================================
    # COMPONENT 2: SKELETON STABILITY (30% weight)
    # =================================================================
    skeleton_score = 100.0
    skeleton_details = {}
    
    # Bone length coefficient of variation (primary stability indicator)
    bone_cv = safe_float(safe_get_path(s02, "bone_qc_mean_cv"))
    skeleton_details['bone_cv_percent'] = round(bone_cv, 3)
    
    if bone_cv > 2.0:
        skeleton_score -= 50
        skeleton_details['bone_stability'] = 'POOR'
    elif bone_cv > 1.0:
        skeleton_score -= 30
        skeleton_details['bone_stability'] = 'MARGINAL'
    elif bone_cv > 0.5:
        skeleton_score -= 10
        skeleton_details['bone_stability'] = 'GOOD'
    else:
        skeleton_details['bone_stability'] = 'EXCELLENT'
    
    # Bone QC status
    bone_status = safe_get_path(s02, "bone_qc_status")
    skeleton_details['bone_qc_status'] = bone_status
    if bone_status == "REJECT":
        skeleton_score -= 30
    elif bone_status == "REVIEW":
        skeleton_score -= 10
    
    # Worst bone identification
    worst_bone = safe_get_path(s02, "worst_bone")
    if worst_bone:
        skeleton_details['worst_bone'] = worst_bone
    
    skeleton_score = max(0, skeleton_score)
    
    # =================================================================
    # COMPONENT 3: MOVEMENT CONTINUITY (30% weight)
    # =================================================================
    continuity_score = 100.0
    continuity_details = {}
    
    # Gate 5: Burst classification decision
    burst_decision = safe_get_path(s06, "step_06_burst_decision.overall_status")
    continuity_details['burst_decision'] = burst_decision
    
    if burst_decision == "REJECT":
        continuity_score -= 50
        continuity_details['burst_assessment'] = 'DATA_QUALITY_ISSUE'
    elif burst_decision == "REVIEW":
        continuity_score -= 15
        continuity_details['burst_assessment'] = 'REQUIRES_VISUAL_AUDIT'
    elif burst_decision == "ACCEPT_HIGH_INTENSITY":
        continuity_score += 0  # No penalty - legitimate Gaga movement
        continuity_details['burst_assessment'] = 'HIGH_INTENSITY_LEGITIMATE'
    else:
        continuity_details['burst_assessment'] = 'NORMAL'
    
    # Artifact rate (Tier 1 only - short spikes)
    artifact_rate = safe_float(safe_get_path(s06, "step_06_burst_analysis.frame_statistics.artifact_rate_percent"))
    continuity_details['artifact_rate_percent'] = round(artifact_rate, 4)
    
    if artifact_rate > 1.0:
        continuity_score -= 25
        continuity_details['artifact_assessment'] = 'EXCESSIVE'
    elif artifact_rate > 0.5:
        continuity_score -= 10
        continuity_details['artifact_assessment'] = 'ELEVATED'
    else:
        continuity_details['artifact_assessment'] = 'MINIMAL'
    
    # Data retained after artifact exclusion
    data_retained = safe_float(safe_get_path(s06, "clean_statistics.comparison.data_retained_percent"))
    continuity_details['data_retained_percent'] = round(data_retained, 4)
    
    if data_retained > 0 and data_retained < 95:
        continuity_score -= 25
        continuity_details['data_retention'] = 'POOR'
    elif data_retained > 0 and data_retained < 99:
        continuity_score -= 10
        continuity_details['data_retention'] = 'FAIR'
    else:
        continuity_details['data_retention'] = 'EXCELLENT'
    
    # Overall pipeline status (processing completion indicator, NOT quality judgment)
    # Status indicates which step completed (e.g., "COMPLETED_STEP_06")
    # Quality scoring is done separately through other metrics
    status = safe_get_path(s06, "overall_status")
    continuity_details['pipeline_status'] = status
    
    # DO NOT penalize based on pipeline status - it's a processing indicator, not quality
    # Quality assessment is handled through outlier analysis, burst classification, etc.
    
    continuity_score = max(0, continuity_score)
    
    # =================================================================
    # WEIGHTED OVERALL SCORE
    # =================================================================
    weights = {
        'physiological': 0.40,
        'skeleton': 0.30,
        'continuity': 0.30
    }
    
    overall_score = (
        physiological_score * weights['physiological'] +
        skeleton_score * weights['skeleton'] +
        continuity_score * weights['continuity']
    )
    
    scorecard = {
        'overall_score': round(overall_score, 2),
        'weights': weights,
        'components': {
            'physiological_plausibility': {
                'score': round(physiological_score, 2),
                'weight': weights['physiological'],
                'weighted_contribution': round(physiological_score * weights['physiological'], 2),
                'details': physiological_details
            },
            'skeleton_stability': {
                'score': round(skeleton_score, 2),
                'weight': weights['skeleton'],
                'weighted_contribution': round(skeleton_score * weights['skeleton'], 2),
                'details': skeleton_details
            },
            'movement_continuity': {
                'score': round(continuity_score, 2),
                'weight': weights['continuity'],
                'weighted_contribution': round(continuity_score * weights['continuity'], 2),
                'details': continuity_details
            }
        },
        'neutralization_applied': {
            'tier_1_artifacts_excluded': using_clean,
            'tier_2_3_bursts_flows_preserved': True,
            'rationale': 'Biomechanical metrics calculated on Cleaned Data (Tier 1 artifacts removed, Tier 2/3 preserved for authentic Gaga movement analysis)'
        }
    }
    
    return round(overall_score, 2), scorecard


def score_signal_quality(steps: Dict[str, dict]) -> float:
    """Score signal quality (0-100) - includes Gate 4 ISB compliance."""
    s06 = steps.get("step_06", {})
    
    score = 100.0
    
    # =====================================================================
    # FIX 2026-01-23: Residual RMS - "Price of Smoothing" Policy
    # =====================================================================
    # GOLD (<15mm):   No penalty - Excellent tracking
    # SILVER (15-30mm): -10 points - Acceptable tracking
    # REVIEW (>30mm):  -30 points - High distortion (filter fighting movement)
    
    # Try new field first (avg_residual_rms_mm), fall back to old field
    rms = safe_float(safe_get_path(s06, "signal_quality.avg_residual_rms_mm"))
    if rms == 0:
        rms = safe_float(safe_get_path(s06, "signal_quality.avg_residual_rms"))
    
    if rms > 30:
        score -= 30  # REVIEW: High filtering distortion
    elif rms > 15:
        score -= 10  # SILVER: Acceptable
    # else: < 15mm = GOLD, no penalty
    
    # Quaternion norm error
    quat_err = safe_float(safe_get_path(s06, "signal_quality.max_quat_norm_err"))
    if quat_err > 0.01:
        score -= 20
    elif quat_err > 0.001:
        score -= 10
    
    # =================================================================
    # GATE 4: ISB Compliance & Mathematical Stability
    # =================================================================
    isb_compliant = safe_get_path(s06, "step_06_isb_compliant")
    if isb_compliant == False:  # Explicit False check
        score -= 15  # Not using ISB-compliant Euler sequences
    
    math_status = safe_get_path(s06, "step_06_math_status")
    if math_status == "REJECT":
        score -= 30  # Quaternion math instability
    elif math_status == "REVIEW":
        score -= 10
    
    return max(0, score)


def compute_overall_score(steps: Dict[str, dict]) -> Tuple[float, str, Dict[str, float]]:
    """
    Compute overall quality score from all components.
    
    Args:
        steps: Dict of step_name -> json_data
        
    Returns:
        Tuple of (overall_score, decision, component_scores)
    """
    # Weights (sum to 1.0)
    weights = {
        "calibration": 0.15,
        "temporal": 0.10,
        "interpolation": 0.15,
        "filtering": 0.10,
        "reference": 0.15,
        "biomechanics": 0.15,
        "signal": 0.20
    }
    
    # Compute component scores
    biomech_score, biomech_scorecard = score_biomechanics(steps)
    
    component_scores = {
        "calibration": score_calibration(steps),
        "temporal": score_temporal_quality(steps),
        "interpolation": score_interpolation(steps),
        "filtering": score_filtering(steps),
        "reference": score_reference(steps),
        "biomechanics": biomech_score,
        "signal": score_signal_quality(steps)
    }
    
    # Add detailed scorecard to response
    component_scores['biomechanics_scorecard'] = biomech_scorecard
    
    # Weighted sum
    overall = sum(
        component_scores[k] * weights[k] 
        for k in weights
    )
    
    # Decision thresholds
    if overall >= 80:
        decision = "ACCEPT"
    elif overall >= 60:
        decision = "REVIEW"
    else:
        decision = "REJECT"
    
    return round(overall, 2), decision, component_scores


# ============================================================
# AGGREGATED METRICS EXTRACTION
# ============================================================

def build_subject_profile(df_engineering, subject_id: str):
    """
    Build aggregate profile for a subject across multiple sessions (Phase 4).
    
    Args:
        df_engineering: pandas DataFrame with engineering profiles for all sessions
        subject_id: Subject identifier
        
    Returns:
        Dict with subject-level aggregated metrics
    """
    import numpy as np
    
    # Filter to this subject
    subject_data = df_engineering[df_engineering['Subject_ID'] == subject_id]
    
    if len(subject_data) == 0:
        return {}
    
    n_sessions = len(subject_data)
    
    profile = {
        "subject_id": subject_id,
        "n_sessions": n_sessions,
        "session_ids": subject_data['Session_ID'].tolist() if 'Session_ID' in subject_data.columns else [],
    }
    
    # Key metrics aggregation
    key_metrics = [
        "Duration_sec",
        "Path_Length_Total_m",
        "Intensity_Mean_m_per_s",
        "Bilateral_Symmetry_Mean",
        "Raw_Missing_Data_Percent",
        "Bone_Length_CV_Percent",
    ]
    
    for metric in key_metrics:
        if metric in subject_data.columns:
            values = subject_data[metric].values
            profile[f"{metric}_mean"] = float(values.mean())
            profile[f"{metric}_std"] = float(values.std())
            profile[f"{metric}_min"] = float(values.min())
            profile[f"{metric}_max"] = float(values.max())
            profile[f"{metric}_cv_pct"] = float((values.std() / values.mean() * 100) if values.mean() > 0 else 0)
    
    # Movement pattern signature (anatomical regions)
    region_metrics = [
        "Intensity_Neck_m_per_s",
        "Intensity_Shoulders_m_per_s",
        "Intensity_Elbows_m_per_s",
        "Intensity_Wrists_m_per_s",
        "Intensity_Knees_m_per_s",
        "Intensity_Ankles_m_per_s",
    ]
    
    movement_signature = {}
    for metric in region_metrics:
        if metric in subject_data.columns:
            region = metric.replace("Intensity_", "").replace("_m_per_s", "")
            movement_signature[region] = {
                "mean": float(subject_data[metric].mean()),
                "std": float(subject_data[metric].std()),
            }
    
    profile["movement_signature"] = movement_signature
    
    # Consistency scores (lower CV% = more consistent)
    consistency = {}
    for metric in key_metrics:
        if metric in subject_data.columns and f"{metric}_cv_pct" in profile:
            cv = profile[f"{metric}_cv_pct"]
            if cv < 10:
                consistency[metric] = "VERY_CONSISTENT"
            elif cv < 25:
                consistency[metric] = "CONSISTENT"
            elif cv < 50:
                consistency[metric] = "VARIABLE"
            else:
                consistency[metric] = "HIGHLY_VARIABLE"
    
    profile["consistency_assessment"] = consistency
    
    return profile


def _extract_max_from_per_joint(s06: dict, field_name: str) -> float:
    """
    Extract maximum value from per_joint dictionary.
    
    Args:
        s06: Step 06 JSON data
        field_name: Field to extract (e.g., 'max_omega_deg_s')
        
    Returns:
        Maximum value across all joints, or 0.0 if not found
    """
    per_joint = s06.get("per_joint", {})
    if not per_joint:
        return 0.0
    
    max_val = 0.0
    for joint, data in per_joint.items():
        if isinstance(data, dict) and field_name in data:
            val = safe_float(data[field_name])
            if val > max_val:
                max_val = val
    
    return max_val


def _extract_max_from_per_segment_linear(s06: dict, field_name: str) -> float:
    """
    Extract maximum value from per_segment_linear dictionary.
    
    Args:
        s06: Step 06 JSON data
        field_name: Field to extract (e.g., 'max_lin_acc_mm_s2')
        
    Returns:
        Maximum value across all segments, or 0.0 if not found
    """
    per_segment = s06.get("per_segment_linear", {})
    if not per_segment:
        return 0.0
    
    max_val = 0.0
    for segment, data in per_segment.items():
        if isinstance(data, dict) and field_name in data:
            val = safe_float(data[field_name])
            if val > max_val:
                max_val = val
    
    return max_val


def build_engineering_profile_row(run_id: str, steps: Dict[str, dict]) -> Dict[str, Any]:
    """
    Build pure physical/engineering profile for a run (NO SCORING, NO DECISIONS).
    
    This function extracts only raw measurements, physical values, and methodology
    documentation. It contains ZERO synthetic grades, quality scores, or decision labels.
    
    Philosophy: "Raw Data Only" - let researchers interpret the data themselves.
    
    Args:
        run_id: The run identifier
        steps: Dict of step_name -> json_data
        
    Returns:
        Dict with engineering measurements (no scores, no decisions)
    """
    s01 = steps.get("step_01", {})
    s02 = steps.get("step_02", {})
    s03 = steps.get("step_03", {})
    s04 = steps.get("step_04", {})
    s05 = steps.get("step_05", {})
    s06 = steps.get("step_06", {})
    
    # Extract Phase 2 metrics (Path Length & Bilateral Symmetry)
    phase2 = extract_phase2_metrics(steps)
    
    # Parse run_id for subject info
    parts = run_id.split('_')
    subject_id = parts[0] if len(parts) > 0 else 'N/A'
    session_id = parts[1] if len(parts) > 1 else 'N/A'
    
    # =================================================================
    # SECTION 0: DATA LINEAGE & PROVENANCE
    # =================================================================
    profile = {
        "Run_ID": run_id,
        "Subject_ID": subject_id,
        "Session_ID": session_id,
        "Processing_Timestamp": safe_get_path(s01, "identity.processing_timestamp"),
        "Pipeline_Version": safe_get_path(s01, "identity.pipeline_version"),
        "CSV_Source": safe_get_path(s01, "identity.csv_source"),
    }
    
    # =================================================================
    # SECTION 1: CAPTURE BASELINE (Raw State Before Processing)
    # =================================================================
    profile.update({
        "Total_Frames": safe_int(safe_get_path(s01, "raw_data_quality.total_frames")),
        "Duration_sec": round(safe_float(safe_get_path(s01, "duration_sec")), 2),
        "Native_Sampling_Rate_Hz": round(safe_float(safe_get_path(s01, "raw_data_quality.sampling_rate_actual")), 3),
        "Raw_Missing_Data_Percent": round(safe_float(safe_get_path(s02, "raw_missing_percent")), 3),
        "OptiTrack_System_Error_mm": round(safe_float(safe_get_path(s01, "raw_data_quality.optitrack_mean_error_mm")), 3),
        "OptiTrack_Version": safe_get_path(s01, "raw_data_quality.optitrack_version"),
    })
    
    # =================================================================
    # SECTION 2: STRUCTURAL INTEGRITY (Skeleton & Calibration)
    # =================================================================
    profile.update({
        "Skeleton_Segments_Found": safe_int(safe_get_path(s01, "skeleton_info.segments_found_count")),
        "Skeleton_Segments_Missing": safe_int(safe_get_path(s01, "skeleton_info.segments_missing_count")),
        "Bone_Length_CV_Percent": round(safe_float(safe_get_path(s02, "bone_qc_mean_cv")), 4),
        "Worst_Bone_Segment": safe_get_path(s02, "worst_bone"),
        "Subject_Height_cm": round(safe_float(safe_get_path(s05, "subject_context.height_cm")), 2),
        "Subject_Mass_kg": round(safe_float(safe_get_path(s04, "subject_metadata.mass_kg")), 1),
    })
    
    # =================================================================
    # SECTION 3: CALIBRATION OFFSETS (Static Pose Alignment)
    # =================================================================
    profile.update({
        "Left_Arm_Offset_deg": round(safe_float(safe_get_path(s05, "static_offset_audit.Left.measured_angle_deg")), 2),
        "Right_Arm_Offset_deg": round(safe_float(safe_get_path(s05, "static_offset_audit.Right.measured_angle_deg")), 2),
        "Reference_Window_Start_sec": round(safe_float(safe_get_path(s05, "window_metadata.start_time_sec")), 2),
        "Reference_Window_End_sec": round(safe_float(safe_get_path(s05, "window_metadata.end_time_sec")), 2),
        "Reference_Variance_Score": round(safe_float(safe_get_path(s05, "window_metadata.variance_score")), 3),
        "Reference_Quality_Index": round(safe_float(safe_get_path(s05, "window_metadata.ref_quality_score")), 3),
    })
    
    # =================================================================
    # SECTION 4: SIGNAL QUALITY (Pre-Processing Assessment)
    # =================================================================
    profile.update({
        "True_Raw_SNR_Mean_dB": round(safe_float(safe_get_path(s04, "snr_analysis.mean_snr_db")), 1),
        "True_Raw_SNR_Min_dB": round(safe_float(safe_get_path(s04, "snr_analysis.min_snr_db")), 1),
        "True_Raw_SNR_Max_dB": round(safe_float(safe_get_path(s04, "snr_analysis.max_snr_db")), 1),
        "SNR_Joints_Excellent_Count": safe_int(safe_get_path(s04, "snr_analysis.joints_excellent")),
        "SNR_Joints_Failed_Count": len(safe_get_path(s04, "snr_analysis.failed_joints", default=[])),
        "SNR_Failed_Joint_List": safe_get_path(s04, "snr_analysis.failed_joints"),
    })
    
    # =================================================================
    # SECTION 5: PROCESSING TRANSPARENCY
    # =================================================================
    profile.update({
        "Interpolation_Method_Positions": safe_get_path(s03, "interpolation_methods.positions"),
        "Interpolation_Method_Rotations": safe_get_path(s03, "interpolation_methods.rotations"),
        "Resampling_Target_Hz": round(safe_float(safe_get_path(s03, "target_fs")), 1),
        "Temporal_Grid_Std_ms": round(safe_float(safe_get_path(s03, "time_grid_std_dt")) * 1000, 4),
        "Filtering_Mode": safe_get_path(s04, "filter_params.filtering_mode"),
        "Filter_Cutoff_Weighted_Avg_Hz": round(safe_float(safe_get_path(s04, "filter_params.filter_cutoff_hz")), 2),
        "Filter_Residual_RMS_mm": round(safe_float(safe_get_path(s04, "filter_params.residual_rms_mm")), 2),
    })
    
    # =================================================================
    # SECTION 6: 3-STAGE CLEANING METRICS
    # =================================================================
    profile.update({
        "Stage1_Total_Artifacts_Detected": safe_int(safe_get_path(s04, "filter_params.total_artifact_frames")),
        "Stage1_Artifact_Segments": safe_int(safe_get_path(s04, "filter_params.total_artifact_segments")),
        "Stage1_Artifact_Percent": round(safe_float(safe_get_path(s04, "filter_params.artifact_frames_pct")), 3),
        "Stage2_Hampel_Outliers": safe_int(safe_get_path(s04, "filter_params.total_hampel_outliers")),
        "Stage2_Hampel_Percent": round(safe_float(safe_get_path(s04, "filter_params.hampel_frames_pct")), 3),
        "Stage3_Winter_Cutoff_Min_Hz": round(safe_float(safe_get_path(s04, "filter_params.winter_cutoff_stats.min")), 2),
        "Stage3_Winter_Cutoff_Max_Hz": round(safe_float(safe_get_path(s04, "filter_params.winter_cutoff_stats.max")), 2),
        "Stage3_Winter_Cutoff_Mean_Hz": round(safe_float(safe_get_path(s04, "filter_params.winter_cutoff_stats.mean")), 2),
    })
    
    # =================================================================
    # SECTION 7: KINEMATIC EXTREMES (Processed Output)
    # =================================================================
    profile.update({
        "Max_Angular_Velocity_deg_s": round(_extract_max_from_per_joint(s06, "max_omega_deg_s"), 2),
        "Max_Angular_Acceleration_deg_s2": round(_extract_max_from_per_joint(s06, "max_alpha_deg_s2"), 2),
        "Max_Linear_Velocity_mm_s": round(_extract_max_from_per_segment_linear(s06, "max_lin_vel_mm_s"), 2),
        "Max_Linear_Acceleration_mm_s2": round(_extract_max_from_per_segment_linear(s06, "max_lin_acc_mm_s2"), 2),
        "Path_Length_Hips_mm": round(safe_float(safe_get_path(s06, "movement_metrics.path_length_mm")), 1),
        "Intensity_Index": round(safe_float(safe_get_path(s06, "movement_metrics.intensity_index")), 3),
    })
    
    # =================================================================
    # SECTION 8: OUTLIER DISTRIBUTION (Frame-Level Analysis)
    # =================================================================
    profile.update({
        "Total_Outlier_Frames": safe_int(safe_get_path(s06, "outlier_analysis.counts.total_outliers")),
        "Outlier_Frames_Percent": round(safe_float(safe_get_path(s06, "outlier_analysis.percentages.total_outliers")), 3),
        "Max_Consecutive_Outlier_Frames": safe_int(safe_get_path(s06, "outlier_analysis.consecutive_runs.max_consecutive_any_outlier")),
        "Artifact_Events_Tier1": safe_int(safe_get_path(s06, "step_06_burst_analysis.classification.artifact_count")),
        "Burst_Events_Tier2": safe_int(safe_get_path(s06, "step_06_burst_analysis.classification.burst_count")),
        "Flow_Events_Tier3": safe_int(safe_get_path(s06, "step_06_burst_analysis.classification.flow_count")),
        "Artifact_Frame_Rate_Percent": round(safe_float(safe_get_path(s06, "step_06_burst_analysis.frame_statistics.artifact_rate_percent")), 4),
    })
    
    # =================================================================
    # SECTION 9: DATA RETENTION (After Artifact Exclusion)
    # =================================================================
    profile.update({
        "Clean_Max_Velocity_deg_s": round(safe_float(safe_get_path(s06, "clean_statistics.clean_statistics.max_deg_s")), 2),
        "Clean_Mean_Velocity_deg_s": round(safe_float(safe_get_path(s06, "clean_statistics.clean_statistics.mean_deg_s")), 2),
        "Velocity_Reduction_Percent": round(safe_float(safe_get_path(s06, "clean_statistics.comparison.velocity_reduction_percent")), 2),
        "Data_Retained_Percent": round(safe_float(safe_get_path(s06, "clean_statistics.data_retention.retention_percent")), 4),
        "Excluded_Frame_Count": safe_int(safe_get_path(s06, "clean_statistics.data_retention.frames_excluded")),
    })
    
    # =================================================================
    # SECTION 10: MATHEMATICAL VALIDATION
    # =================================================================
    profile.update({
        "Quaternion_Max_Norm_Error": round(safe_float(safe_get_path(s06, "signal_quality.max_quat_norm_err")), 8),
        "Sample_Jitter_ms": round(safe_float(safe_get_path(s02, "step_02_sample_time_jitter_ms")), 4),
        "Max_Gap_Frames": safe_int(safe_get_path(s02, "step_02_max_gap_frames")),
    })
    
    # =================================================================
    # SECTION 11: PROCESSING STATUS (Technical Only - Not Quality)
    # =================================================================
    profile.update({
        "Pipeline_Completion_Step": safe_get_path(s06, "overall_status"),
        "Overall_Gate_Status": safe_get_path(s06, "overall_gate_status"),
    })
    
    # =================================================================
    # SECTION 12: PATH LENGTH & INTENSITY METRICS (Phase 2 & 3)
    # =================================================================
    profile.update({
        # Aggregate Path Length Metrics
        "Path_Length_Max_m": safe_float(safe_get_path(phase2, "path_length.max_m")),
        "Path_Length_Mean_m": safe_float(safe_get_path(phase2, "path_length.mean_m")),
        "Path_Length_Total_m": safe_float(safe_get_path(phase2, "path_length.total_m")),
        "Most_Active_Segments": ", ".join(safe_get_path(phase2, "path_length.top_3_segments") or []),
        
        # Aggregate Intensity Metrics
        "Intensity_Max_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.max_m_per_s")), 4),
        "Intensity_Mean_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.mean_m_per_s")), 4),
        "Most_Intense_Segments": ", ".join(safe_get_path(phase2, "intensity_index.top_3_segments") or []),
        
        # Anatomical Region Path Lengths (Human-Readable)
        "Path_Neck_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Neck")), 2),
        "Path_Shoulders_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Shoulders")), 2),
        "Path_Elbows_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Elbows")), 2),
        "Path_Wrists_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Wrists")), 2),
        "Path_Spine_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Spine")), 2),
        "Path_Hips_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Hips")), 2),
        "Path_Knees_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Knees")), 2),
        "Path_Ankles_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Ankles")), 2),
        
        # Anatomical Region Intensity (Human-Readable)
        "Intensity_Neck_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Neck")), 4),
        "Intensity_Shoulders_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Shoulders")), 4),
        "Intensity_Elbows_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Elbows")), 4),
        "Intensity_Wrists_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Wrists")), 4),
        "Intensity_Spine_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Spine")), 4),
        "Intensity_Hips_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Hips")), 4),
        "Intensity_Knees_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Knees")), 4),
        "Intensity_Ankles_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Ankles")), 4),
        
        # Bilateral Symmetry
        "Bilateral_Symmetry_Mean": safe_float(safe_get_path(phase2, "bilateral_symmetry.mean_index")),
        "Bilateral_Symmetry_Min": safe_float(safe_get_path(phase2, "bilateral_symmetry.min_index")),
        "Most_Asymmetric_Pair": safe_get_path(phase2, "bilateral_symmetry.most_asymmetric"),
    })
    
    return profile


def extract_per_joint_noise_profile(steps: Dict[str, dict]) -> pd.DataFrame:
    """
    Extract per-joint noise and filtering profile.
    
    Returns DataFrame with columns:
    - Joint
    - Raw_SNR_dB (from filtering stage)
    - Winter_Cutoff_Hz (per-region filtering)
    - Artifact_Frames (WARNING/ALERT/CRITICAL counts)
    - Classification (Sporadic/Systemic/Clean)
    """
    s04 = steps.get("step_04", {})
    s06 = steps.get("step_06", {})
    
    # Load outlier validation data
    outlier_data_path = safe_get_path(s06, "_source_file_outlier_validation")
    
    # Build per-joint profile from outlier_validation.json arrays
    joint_profiles = []
    
    # Get angular velocity outlier data
    ang_vel_data = safe_get_path(s06, "outlier_validation.angular_velocity_deg_s", default=[])
    if isinstance(ang_vel_data, list):
        for joint_data in ang_vel_data:
            if isinstance(joint_data, dict):
                joint_name = joint_data.get("joint", "N/A")
                warning_frames = safe_int(joint_data.get("n_frames_WARNING", 0))
                alert_frames = safe_int(joint_data.get("n_frames_ALERT", 0))
                critical_frames = safe_int(joint_data.get("n_frames_CRITICAL", 0))
                total_frames = safe_int(joint_data.get("total_frames", 1))
                
                # Calculate outlier percentage
                outlier_pct = ((warning_frames + alert_frames + critical_frames) / total_frames * 100) if total_frames > 0 else 0
                
                # Classify noise pattern
                if critical_frames > 0:
                    classification = "Artifact_Detected"
                elif outlier_pct > 1.0:
                    classification = "Systemic_Noise"
                elif outlier_pct > 0.1:
                    classification = "Sporadic_Glitches"
                else:
                    classification = "Clean"
                
                joint_profiles.append({
                    "Joint": joint_name,
                    "WARNING_Frames": warning_frames,
                    "ALERT_Frames": alert_frames,
                    "CRITICAL_Frames": critical_frames,
                    "Outlier_Percent": round(outlier_pct, 3),
                    "Classification": classification
                })
    
    return pd.DataFrame(joint_profiles)


def extract_bone_stability_profile(steps: Dict[str, dict]) -> Dict[str, Any]:
    """
    Extract hierarchical bone stability analysis.
    
    Returns dict with:
    - mean_cv_percent: Overall skeleton rigidity
    - worst_bone: Segment with highest variance
    - bone_lengths_mm: Dict of segment -> length
    - stability_notes: Physiological context
    """
    s02 = steps.get("step_02", {})
    s05 = steps.get("step_05", {})
    
    # Get bone measurements from biomechanical_audit
    bone_lengths = safe_get_path(s05, "audit.bone_m", default={})
    
    mean_cv = safe_float(safe_get_path(s02, "bone_qc_mean_cv"))
    worst_bone = safe_get_path(s02, "worst_bone")
    
    # Add physiological context
    stability_notes = {}
    if worst_bone and "Hips" in str(worst_bone) and "Spine" in str(worst_bone):
        stability_notes[worst_bone] = "Expected variability - soft tissue (breathing artifacts)"
    
    return {
        "mean_cv_percent": round(mean_cv, 4),
        "worst_bone": worst_bone,
        "bone_lengths_mm": bone_lengths,
        "stability_notes": stability_notes,
        "interpretation": "CV < 0.5% = Excellent, 0.5-1.0% = Good, 1.0-2.0% = Review, >2.0% = Poor"
    }


def extract_selected_segments(steps: Dict[str, dict]) -> List[str]:
    """Extract the 19 kinematic segments from rotation_alignment_audit."""
    s05 = steps.get("step_05", {})
    rotation_audit = s05.get("rotation_alignment_audit", {})
    
    # Return all joints that have rotation data
    segments = [k for k in rotation_audit.keys() if isinstance(rotation_audit.get(k), dict)]
    return sorted(segments)


def compute_noise_locality_index(per_joint_profile: pd.DataFrame) -> float:
    """
    Compute a noise locality index to differentiate localized vs systemic noise.
    
    Returns:
        float: 0.0 = perfectly localized (only a few joints noisy)
               1.0 = perfectly systemic (all joints equally noisy)
    """
    if len(per_joint_profile) == 0:
        return 0.0
    
    # Count joints with significant outliers (>1% critical frames)
    critical_pct = per_joint_profile.get("critical_pct", pd.Series([0] * len(per_joint_profile)))
    noisy_joints = (critical_pct > 1.0).sum()
    total_joints = len(per_joint_profile)
    
    # Locality index: ratio of affected joints
    if total_joints == 0:
        return 0.0
    
    return noisy_joints / total_joints


def aggregate_by_anatomical_region(
    per_segment_data: Dict[str, float],
    aggregation_func: str = "max"
) -> Dict[str, float]:
    """
    Aggregate per-segment metrics by anatomical region for human-readable reporting.
    
    Args:
        per_segment_data: Dict of {segment_name: value}
        aggregation_func: "max", "mean", or "sum"
        
    Returns:
        Dict of {anatomical_region: aggregated_value}
        
    Example:
        Input: {"LeftForeArm": 234.5, "RightForeArm": 242.1, ...}
        Output: {"Elbows": 242.1} (max of left and right)
    """
    result = {}
    
    for region_name, joint_list in ANATOMICAL_REGIONS.items():
        # Get values for all joints in this region
        values = [per_segment_data.get(joint, 0.0) for joint in joint_list]
        values = [v for v in values if v > 0]  # Filter out missing/zero values
        
        if not values:
            result[region_name] = 0.0
            continue
        
        # Aggregate
        if aggregation_func == "max":
            result[region_name] = max(values)
        elif aggregation_func == "mean":
            result[region_name] = sum(values) / len(values)
        elif aggregation_func == "sum":
            result[region_name] = sum(values)
        else:
            result[region_name] = max(values)  # Default to max
    
    return result


def extract_phase2_metrics(steps: Dict[str, dict]) -> Dict[str, Any]:
    """
    Extract Phase 2 metrics: Path Length and Bilateral Symmetry.
    
    These metrics were added in Phase 2 of the engineering audit project.
    
    Args:
        steps: Dict of step_name -> json_data
        
    Returns:
        Dict with path_length and bilateral_symmetry metrics
    """
    s06 = steps.get("step_06", {})
    
    # DEBUG: Check what's in s06
    if "path_length" in s06:
        print(f"[DEBUG] extract_phase2_metrics: Found 'path_length' in s06")
        if isinstance(s06["path_length"], dict) and "by_region" in s06["path_length"]:
            print(f"[DEBUG] Found by_region structure: {s06['path_length']['by_region']}")
    else:
        print(f"[DEBUG] extract_phase2_metrics: 'path_length' NOT in s06, keys: {list(s06.keys())[:10]}")
    
    # Try new nested structure first (notebook 06 with regional grouping)
    path_length_data = s06.get("path_length", {})
    if isinstance(path_length_data, dict) and "by_segment" in path_length_data:
        # New structure: {"by_segment": {...}, "by_region": {...}}
        path_lengths = path_length_data.get("by_segment", {})
        path_by_region = path_length_data.get("by_region", {})
    else:
        # Fallback: old structure (flat dict)
        path_lengths = s06.get("path_length_m", {})
        # Compute regions from segments
        path_by_region = aggregate_by_anatomical_region(path_lengths, aggregation_func="max") if path_lengths else {}
    
    # Compute aggregate metrics
    if path_lengths:
        path_values = list(path_lengths.values())
        max_path = max(path_values) if path_values else 0.0
        mean_path = sum(path_values) / len(path_values) if path_values else 0.0
        total_path = sum(path_values)
        
        # Identify most active segments
        sorted_segments = sorted(path_lengths.items(), key=lambda x: x[1], reverse=True)
        top_3_segments = [seg for seg, _ in sorted_segments[:3]]
    else:
        max_path = 0.0
        mean_path = 0.0
        total_path = 0.0
        top_3_segments = []
    
    # Extract bilateral symmetry
    bilateral_symmetry = s06.get("bilateral_symmetry", {})
    
    # Compute mean symmetry index across all metrics
    if bilateral_symmetry:
        symmetry_indices = [v["symmetry_index"] for v in bilateral_symmetry.values() if isinstance(v, dict)]
        mean_symmetry = sum(symmetry_indices) / len(symmetry_indices) if symmetry_indices else 1.0
        min_symmetry = min(symmetry_indices) if symmetry_indices else 1.0
        
        # Find most asymmetric pair (with user-friendly label)
        sorted_by_asymmetry = sorted(
            [(k, v) for k, v in bilateral_symmetry.items() if isinstance(v, dict)],
            key=lambda x: x[1]["symmetry_index"]
        )
        if sorted_by_asymmetry:
            most_asymmetric_key = sorted_by_asymmetry[0][0]
            # Extract the pair type (e.g., "path_length_hand" -> "hand")
            pair_type = most_asymmetric_key.split('_')[-1] if '_' in most_asymmetric_key else most_asymmetric_key
            most_asymmetric = BILATERAL_PAIR_LABELS.get(pair_type, most_asymmetric_key)
        else:
            most_asymmetric = "N/A"
    else:
        mean_symmetry = 1.0
        min_symmetry = 1.0
        most_asymmetric = "N/A"
    
    # Extract intensity index (Phase 3)
    intensity_data = s06.get("intensity_index", {})
    if isinstance(intensity_data, dict) and "by_segment" in intensity_data:
        # New structure: {"by_segment": {...}, "by_region": {...}}
        intensity_index = intensity_data.get("by_segment", {})
        intensity_by_region = intensity_data.get("by_region", {})
        print(f"[DEBUG] Found new intensity structure with by_region: {intensity_by_region}")
    else:
        # Fallback: old structure (flat dict)
        intensity_index = s06.get("intensity_index_m_per_s", {})
        # Compute regions from segments
        intensity_by_region = aggregate_by_anatomical_region(intensity_index, aggregation_func="max") if intensity_index else {}
        print(f"[DEBUG] Using fallback intensity structure, computed by_region: {intensity_by_region}")
    
    if intensity_index:
        intensity_values = list(intensity_index.values())
        max_intensity = max(intensity_values) if intensity_values else 0.0
        mean_intensity = sum(intensity_values) / len(intensity_values) if intensity_values else 0.0
        
        # Most intense segments
        sorted_intensity = sorted(intensity_index.items(), key=lambda x: x[1], reverse=True)
        top_3_intense = [seg for seg, _ in sorted_intensity[:3]]
    else:
        max_intensity = 0.0
        mean_intensity = 0.0
        top_3_intense = []
    
    return {
        "path_length": {
            "max_m": round(max_path, 2),
            "mean_m": round(mean_path, 2),
            "total_m": round(total_path, 2),
            "top_3_segments": top_3_segments,
            "by_region": path_by_region,  # Anatomical region view
        },
        "intensity_index": {
            "max_m_per_s": round(max_intensity, 4),
            "mean_m_per_s": round(mean_intensity, 4),
            "top_3_segments": top_3_intense,
            "by_region": intensity_by_region,  # Anatomical region view
        },
        "bilateral_symmetry": {
            "mean_index": round(mean_symmetry, 3),
            "min_index": round(min_symmetry, 3),
            "most_asymmetric": most_asymmetric,  # Uses user-friendly labels
        }
    }


def extract_selected_segments(steps: Dict[str, dict]) -> List[str]:
    """
    Extract the 19 kinematic segments used in rotation analysis.
    
    Returns list of segment names from rotation_alignment_audit.
    """
    s05 = steps.get("step_05", {})
    
    rotation_audit = safe_get_path(s05, "rotation_alignment_audit", default={})
    
    if isinstance(rotation_audit, dict):
        return sorted(rotation_audit.keys())
    
    return []


def compute_noise_locality_index(per_joint_profile: pd.DataFrame) -> float:
    """
    Compute noise locality index to distinguish localized vs systemic issues.
    
    Formula: max(outlier_pct) / mean(outlier_pct)
    
    Interpretation:
    - High (>5): Localized tracking issue in specific joint
    - Medium (2-5): Regional problem (e.g., one limb)
    - Low (<2): Systemic noise across skeleton
    
    Returns:
        Locality index (float)
    """
    if per_joint_profile.empty:
        return 0.0
    
    outlier_pcts = per_joint_profile["Outlier_Percent"].values
    outlier_pcts = outlier_pcts[outlier_pcts > 0]  # Exclude zeros
    
    if len(outlier_pcts) == 0:
        return 0.0
    
    max_pct = outlier_pcts.max()
    mean_pct = outlier_pcts.mean()
    
    if mean_pct == 0:
        return 0.0
    
    return round(max_pct / mean_pct, 2)


def build_quality_row(run_id: str, steps: Dict[str, dict]) -> Dict[str, Any]:
    """
    Build a single row of quality metrics for a run.
    
    Args:
        run_id: The run identifier
        steps: Dict of step_name -> json_data
        
    Returns:
        Dict with all quality metrics
    """
    s01 = steps.get("step_01", {})
    s02 = steps.get("step_02", {})
    s03 = steps.get("step_03", {})
    s04 = steps.get("step_04", {})
    s05 = steps.get("step_05", {})
    s06 = steps.get("step_06", {})
    
    # Extract Phase 2 metrics (for compatibility with engineering profile)
    phase2 = extract_phase2_metrics(steps)
    
    # Compute scores
    overall_score, decision, component_scores = compute_overall_score(steps)
    
    # Parse run_id for subject info
    parts = run_id.split('_')
    subject_id = parts[0] if len(parts) > 0 else 'N/A'
    session_id = parts[1] if len(parts) > 1 else 'N/A'
    
    return {
        # Identity
        "Run_ID": run_id,
        "Subject_ID": subject_id,
        "Session_ID": session_id,
        "Processing_Date": safe_get_path(s01, "identity.processing_timestamp"),
        "Pipeline_Version": safe_get_path(s01, "identity.pipeline_version"),
        
        # Raw Data Quality
        "Total_Frames": safe_int(safe_get_path(s01, "raw_data_quality.total_frames")),
        "Duration_Sec": round(safe_float(safe_get_path(s01, "duration_sec")), 1),
        "Sampling_Rate_Hz": round(safe_float(safe_get_path(s01, "raw_data_quality.sampling_rate_actual")), 2),
        "OptiTrack_Error_mm": round(safe_float(safe_get_path(s01, "raw_data_quality.optitrack_mean_error_mm")), 3),
        
        # Preprocessing
        "Raw_Missing_%": round(safe_float(safe_get_path(s02, "raw_missing_percent")), 2),
        "Bone_CV_%": round(safe_float(safe_get_path(s02, "bone_qc_mean_cv")), 3),
        "Bone_Status": safe_get_path(s02, "bone_qc_status"),
        "Worst_Bone": safe_get_path(s02, "worst_bone"),
        "Interpolation_Method": safe_get_path(s02, "interpolation_method"),
        
        # Resampling (Step 03)
        "Target_Fs_Hz": round(safe_float(safe_get_path(s03, "target_fs")), 1),
        "Time_Grid_Std_Dt": safe_float(safe_get_path(s03, "time_grid_std_dt")),
        "Temporal_Status": safe_get_path(s03, "temporal_status"),
        "Resample_Interp_Positions": safe_get_path(s03, "interpolation_methods.positions"),
        "Resample_Interp_Rotations": safe_get_path(s03, "interpolation_methods.rotations"),
        
        # Filtering (Essential fields only)
        "Residual_RMS_mm": round(safe_float(safe_get_path(s04, "filter_params.residual_rms_mm")), 2),
        
        # Reference
        "Ref_Quality_Score": round(safe_float(safe_get_path(s05, "window_metadata.ref_quality_score")), 3),
        "Ref_Confidence": safe_get_path(s05, "window_metadata.confidence_level"),
        "Left_Offset_Deg": round(safe_float(safe_get_path(s05, "static_offset_audit.Left.measured_angle_deg")), 2),
        "Right_Offset_Deg": round(safe_float(safe_get_path(s05, "static_offset_audit.Right.measured_angle_deg")), 2),
        "Subject_Height_cm": round(safe_float(safe_get_path(s05, "subject_context.height_cm")), 2),
        "Height_Status": safe_get_path(s05, "subject_context.height_status"),
        "Subject_Mass_kg": round(safe_float(safe_get_path(s04, "subject_metadata.mass_kg")), 2),
        
        # Kinematics - Extract from per_joint data if metrics not available
        "Pipeline_Status": safe_get_path(s06, "overall_status"),
        "Max_Ang_Vel_deg_s": round(_extract_max_from_per_joint(s06, "max_omega_deg_s"), 2),
        "Max_Ang_Accel": round(_extract_max_from_per_joint(s06, "max_alpha_deg_s2"), 2),
        "Max_Lin_Accel": round(_extract_max_from_per_segment_linear(s06, "max_lin_acc_mm_s2"), 2),
        "Outlier_Frames": safe_int(safe_get_path(s06, "outlier_analysis.counts.total_outliers")),
        "Outlier_%": round(safe_float(safe_get_path(s06, "outlier_analysis.percentages.total_outliers")), 3),
        "Residual_RMS": round(safe_float(safe_get_path(s06, "signal_quality.avg_residual_rms")), 3),
        "Quat_Norm_Err": round(safe_float(safe_get_path(s06, "signal_quality.max_quat_norm_err")), 6),
        
        # =================================================================
        # PHASE 2: Path Length & Bilateral Symmetry (NEW)
        # =================================================================
        "Path_Length_Max_m": safe_float(safe_get_path(phase2, "path_length.max_m")),
        "Path_Length_Mean_m": safe_float(safe_get_path(phase2, "path_length.mean_m")),
        "Path_Length_Total_m": safe_float(safe_get_path(phase2, "path_length.total_m")),
        "Most_Active_Segments": ", ".join(safe_get_path(phase2, "path_length.top_3_segments") or []),
        
        # =================================================================
        # PHASE 3: Intensity Index (NEW)
        # =================================================================
        "Intensity_Max_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.max_m_per_s")), 4),
        "Intensity_Mean_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.mean_m_per_s")), 4),
        "Most_Intense_Segments": ", ".join(safe_get_path(phase2, "intensity_index.top_3_segments") or []),
        
        # Anatomical Region Path Lengths (Human-Readable)
        "Path_Neck_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Neck")), 2),
        "Path_Shoulders_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Shoulders")), 2),
        "Path_Elbows_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Elbows")), 2),
        "Path_Wrists_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Wrists")), 2),
        "Path_Spine_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Spine")), 2),
        "Path_Hips_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Hips")), 2),
        "Path_Knees_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Knees")), 2),
        "Path_Ankles_m": round(safe_float(safe_get_path(phase2, "path_length.by_region.Ankles")), 2),
        
        # Anatomical Region Intensity (Human-Readable)
        "Intensity_Neck_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Neck")), 4),
        "Intensity_Shoulders_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Shoulders")), 4),
        "Intensity_Elbows_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Elbows")), 4),
        "Intensity_Wrists_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Wrists")), 4),
        "Intensity_Spine_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Spine")), 4),
        "Intensity_Hips_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Hips")), 4),
        "Intensity_Knees_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Knees")), 4),
        "Intensity_Ankles_m_per_s": round(safe_float(safe_get_path(phase2, "intensity_index.by_region.Ankles")), 4),
        
        # Bilateral Symmetry
        "Bilateral_Symmetry_Mean": safe_float(safe_get_path(phase2, "bilateral_symmetry.mean_index")),
        "Bilateral_Symmetry_Min": safe_float(safe_get_path(phase2, "bilateral_symmetry.min_index")),
        "Most_Asymmetric_Pair": safe_get_path(phase2, "bilateral_symmetry.most_asymmetric"),
        
        # =================================================================
        # GATE 2: Temporal Quality (New Fields)
        # =================================================================
        "Sample_Jitter_ms": round(safe_float(safe_get_path(s02, "step_02_sample_time_jitter_ms")), 4),
        "Jitter_Status": safe_get_path(s02, "step_02_jitter_status"),
        "Fallback_Count": safe_int(safe_get_path(s02, "step_02_fallback_count")),
        "Fallback_Rate_%": round(safe_float(safe_get_path(s02, "step_02_fallback_rate_percent")), 4),
        "Max_Gap_Frames": safe_int(safe_get_path(s02, "step_02_max_gap_frames")),
        "Interpolation_Status": safe_get_path(s02, "step_02_interpolation_status"),
        
        # =================================================================
        # GATE 3: Filtering (Per-Region Analysis)
        # =================================================================
        "Filtering_Mode": safe_get_path(s04, "filter_params.filtering_mode"),  # e.g., "per_region_fixed"
        "Region_Cutoffs_Applied": safe_get_path(s04, "filter_params.region_cutoffs"),  # Applied cutoffs per region
        "RMS_Knee_Per_Region": _extract_rms_knee_per_region(s04),  # Winter strict_knee per region
        "Diminishing_Per_Region": _extract_diminishing_per_region(s04),  # Winter diminishing returns per region
        "Region_Validation_Status": _extract_validation_status(s04),  # VALID/AGGRESSIVE per region
        
        # =================================================================
        # TRUE RAW SNR (Capture Quality - from raw data frequency analysis)
        # =================================================================
        "Raw_SNR_Mean_dB": round(safe_float(safe_get_path(s04, "snr_analysis.mean_snr_db")), 1),
        "Raw_SNR_Min_dB": round(safe_float(safe_get_path(s04, "snr_analysis.min_snr_db")), 1),
        "Raw_SNR_Max_dB": round(safe_float(safe_get_path(s04, "snr_analysis.max_snr_db")), 1),
        "Raw_SNR_Status": safe_get_path(s04, "snr_analysis.overall_status"),
        "Raw_SNR_Joints_Excellent": safe_int(safe_get_path(s04, "snr_analysis.joints_excellent")),
        "Raw_SNR_Failed_Joints": safe_get_path(s04, "snr_analysis.failed_joints"),
        
        # =================================================================
        # GATE 4: ISB Compliance (New Fields)
        # =================================================================
        "ISB_Compliant": safe_get_path(s06, "step_06_isb_compliant"),
        "Math_Status": safe_get_path(s06, "step_06_math_status"),
        "Math_Decision_Reason": safe_get_path(s06, "step_06_math_decision_reason"),
        
        # =================================================================
        # GATE 5: Burst Classification (New Fields)
        # =================================================================
        "Burst_Artifact_Count": safe_int(safe_get_path(s06, "step_06_burst_analysis.classification.artifact_count")),
        "Burst_Count": safe_int(safe_get_path(s06, "step_06_burst_analysis.classification.burst_count")),
        "Burst_Flow_Count": safe_int(safe_get_path(s06, "step_06_burst_analysis.classification.flow_count")),
        "Burst_Total_Events": safe_int(safe_get_path(s06, "step_06_burst_analysis.classification.total_events")),
        "Artifact_Rate_%": round(safe_float(safe_get_path(s06, "step_06_burst_analysis.frame_statistics.artifact_rate_percent")), 4),
        "Max_Consecutive_Frames": safe_int(safe_get_path(s06, "step_06_burst_analysis.timing.max_consecutive_frames")),
        "Mean_Event_Duration_ms": round(safe_float(safe_get_path(s06, "step_06_burst_analysis.timing.mean_event_duration_ms")), 2),
        "Burst_Decision": safe_get_path(s06, "step_06_burst_decision.overall_status"),
        "Burst_Decision_Reason": safe_get_path(s06, "step_06_burst_decision.primary_reason"),
        "Data_Usable": safe_get_path(s06, "step_06_data_validity.usable"),
        "Excluded_Frames": safe_int(safe_get_path(s06, "step_06_data_validity.excluded_frame_count")),
        
        # Suspicious Frame Ranges (for quick reference)
        "Artifact_Frame_Ranges": _frames_to_ranges_str(safe_get_path(s06, "step_06_frames_to_exclude", default=[])),
        "Burst_Frame_Ranges": _frames_to_ranges_str(safe_get_path(s06, "step_06_frames_to_review", default=[])),
        
        # Clean Statistics (after artifact exclusion)
        "Clean_Max_Vel_deg_s": round(safe_float(safe_get_path(s06, "clean_statistics.clean_statistics.max_deg_s")), 2),
        "Clean_Mean_Vel_deg_s": round(safe_float(safe_get_path(s06, "clean_statistics.clean_statistics.mean_deg_s")), 2),
        "Max_Vel_Reduction_%": round(safe_float(safe_get_path(s06, "clean_statistics.comparison.max_reduction_percent")), 2),
        "Data_Retained_%": round(safe_float(safe_get_path(s06, "clean_statistics.comparison.data_retained_percent")), 4),
        
        # Overall Gate Status
        "Overall_Gate_Status": safe_get_path(s06, "overall_gate_status"),
        
        # Scores
        "Quality_Score": overall_score,
        "Research_Decision": decision,
        "Score_Calibration": component_scores["calibration"],
        "Score_Temporal": component_scores["temporal"],
        "Score_Interpolation": component_scores["interpolation"],
        "Score_Filtering": component_scores["filtering"],
        "Score_Reference": component_scores["reference"],
        "Score_Biomechanics": component_scores["biomechanics"],
        "Score_Signal": component_scores["signal"],
        
        # =================================================================
        # BIOMECHANICS SCORECARD (Detailed Transparency)
        # =================================================================
        "Biomech_Physiological_Score": component_scores.get("biomechanics_scorecard", {}).get("components", {}).get("physiological_plausibility", {}).get("score", 0),
        "Biomech_Skeleton_Score": component_scores.get("biomechanics_scorecard", {}).get("components", {}).get("skeleton_stability", {}).get("score", 0),
        "Biomech_Continuity_Score": component_scores.get("biomechanics_scorecard", {}).get("components", {}).get("movement_continuity", {}).get("score", 0),
        "Biomech_Velocity_Source": component_scores.get("biomechanics_scorecard", {}).get("components", {}).get("physiological_plausibility", {}).get("details", {}).get("velocity_source", "N/A"),
        "Biomech_Velocity_Assessment": component_scores.get("biomechanics_scorecard", {}).get("components", {}).get("physiological_plausibility", {}).get("details", {}).get("velocity_assessment", "N/A"),
        "Biomech_Accel_Assessment": component_scores.get("biomechanics_scorecard", {}).get("components", {}).get("physiological_plausibility", {}).get("details", {}).get("acceleration_assessment", "N/A"),
        "Biomech_Bone_Stability": component_scores.get("biomechanics_scorecard", {}).get("components", {}).get("skeleton_stability", {}).get("details", {}).get("bone_stability", "N/A"),
        "Biomech_Burst_Assessment": component_scores.get("biomechanics_scorecard", {}).get("components", {}).get("movement_continuity", {}).get("details", {}).get("burst_assessment", "N/A"),
        "Biomech_Artifact_Assessment": component_scores.get("biomechanics_scorecard", {}).get("components", {}).get("movement_continuity", {}).get("details", {}).get("artifact_assessment", "N/A"),
        "Biomech_Neutralization_Applied": component_scores.get("biomechanics_scorecard", {}).get("neutralization_applied", {}).get("tier_1_artifacts_excluded", False),
    }


# ============================================================
# EXCEL EXPORT UTILITIES
# ============================================================

def export_to_excel(
    runs_data: Dict[str, Dict[str, dict]],
    output_path: str,
    project_root: str
) -> str:
    """
    Export complete audit report to Excel with 4 sheets.
    
    Args:
        runs_data: Dict of run_id -> {step_name: json_data}
        output_path: Path for output Excel file
        project_root: Project root for git hash
        
    Returns:
        Path to created Excel file
    """
    # Build all data structures
    quality_rows = []
    parameter_rows = []
    
    for run_id, steps in runs_data.items():
        quality_rows.append(build_quality_row(run_id, steps))
        parameter_rows.append(extract_parameters_flat(run_id, steps))
    
    # Create DataFrames
    df_quality = pd.DataFrame(quality_rows)
    df_quality = df_quality.sort_values("Quality_Score", ascending=False).reset_index(drop=True)
    
    df_params = pd.DataFrame(parameter_rows)
    
    # Build schema DataFrame
    schema_rows = []
    for step_name, step_info in PARAMETER_SCHEMA.items():
        for param_path, param_info in step_info["parameters"].items():
            schema_rows.append({
                "Step": step_name,
                "Parameter_Path": param_path,
                "Type": param_info["type"],
                "Section": param_info["section"],
                "Section_Name": SECTION_DESCRIPTIONS.get(param_info["section"], ""),
                "Description": param_info["description"]
            })
    df_schema = pd.DataFrame(schema_rows)
    
    # Get git hash
    try:
        git_hash = subprocess.check_output(
            ['git', 'rev-parse', '--short', 'HEAD'],
            cwd=project_root
        ).decode('ascii').strip()
    except:
        git_hash = "unknown"
    
    # Write to Excel
    with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
        workbook = writer.book
        
        # Formats
        title_fmt = workbook.add_format({
            'bold': True, 'font_size': 16, 
            'bg_color': '#2E75B6', 'font_color': 'white'
        })
        header_fmt = workbook.add_format({
            'bold': True, 'bg_color': '#4472C4', 
            'font_color': 'white', 'text_wrap': True
        })
        green_fmt = workbook.add_format({
            'bg_color': '#C6EFCE', 'font_color': '#006100'
        })
        yellow_fmt = workbook.add_format({
            'bg_color': '#FFEB9C', 'font_color': '#9C6500'
        })
        red_fmt = workbook.add_format({
            'bg_color': '#FFC7CE', 'font_color': '#9C0006'
        })
        
        # ============================================================
        # SHEET 1: EXECUTIVE SUMMARY
        # ============================================================
        exec_sheet = workbook.add_worksheet('Executive_Summary')
        
        exec_sheet.merge_range('A1:E1', 'MASTER QUALITY AUDIT - EXECUTIVE SUMMARY', title_fmt)
        exec_sheet.write('A2', f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        exec_sheet.write('A3', f"Git Hash: {git_hash}")
        
        row = 5
        
        # Dataset Overview
        exec_sheet.merge_range(row, 0, row, 4, 'DATASET OVERVIEW', header_fmt)
        row += 1
        
        total_runs = len(df_quality)
        accept_count = (df_quality['Research_Decision'] == 'ACCEPT').sum()
        review_count = (df_quality['Research_Decision'] == 'REVIEW').sum()
        reject_count = (df_quality['Research_Decision'] == 'REJECT').sum()
        
        exec_sheet.write(row, 0, 'Total Recordings:')
        exec_sheet.write(row, 1, total_runs)
        row += 1
        
        exec_sheet.write(row, 0, 'Accepted:')
        exec_sheet.write(row, 1, accept_count)
        exec_sheet.write(row, 2, f"{accept_count/total_runs*100:.1f}%" if total_runs > 0 else "0%")
        exec_sheet.write(row, 3, 'ACCEPT', green_fmt)
        row += 1
        
        exec_sheet.write(row, 0, 'Need Review:')
        exec_sheet.write(row, 1, review_count)
        exec_sheet.write(row, 2, f"{review_count/total_runs*100:.1f}%" if total_runs > 0 else "0%")
        exec_sheet.write(row, 3, 'REVIEW', yellow_fmt)
        row += 1
        
        exec_sheet.write(row, 0, 'Rejected:')
        exec_sheet.write(row, 1, reject_count)
        exec_sheet.write(row, 2, f"{reject_count/total_runs*100:.1f}%" if total_runs > 0 else "0%")
        exec_sheet.write(row, 3, 'REJECT', red_fmt)
        row += 2
        
        # Quality Score Stats
        exec_sheet.merge_range(row, 0, row, 4, 'QUALITY SCORE STATISTICS', header_fmt)
        row += 1
        
        exec_sheet.write(row, 0, 'Mean Score:')
        exec_sheet.write(row, 1, f"{df_quality['Quality_Score'].mean():.2f}")
        row += 1
        exec_sheet.write(row, 0, 'Min Score:')
        exec_sheet.write(row, 1, f"{df_quality['Quality_Score'].min():.2f}")
        row += 1
        exec_sheet.write(row, 0, 'Max Score:')
        exec_sheet.write(row, 1, f"{df_quality['Quality_Score'].max():.2f}")
        row += 2
        
        # Component Scores Summary
        exec_sheet.merge_range(row, 0, row, 4, 'COMPONENT SCORES (MEAN)', header_fmt)
        row += 1
        
        for score_col in ['Score_Calibration', 'Score_Temporal', 'Score_Interpolation',
                          'Score_Filtering', 'Score_Reference', 'Score_Biomechanics', 'Score_Signal']:
            label = score_col.replace('Score_', '')
            exec_sheet.write(row, 0, f'{label}:')
            exec_sheet.write(row, 1, f"{df_quality[score_col].mean():.1f}")
            row += 1
        
        exec_sheet.set_column('A:A', 25)
        exec_sheet.set_column('B:E', 15)
        
        # ============================================================
        # SHEET 2: QUALITY REPORT
        # ============================================================
        df_quality.to_excel(writer, index=False, sheet_name='Quality_Report')
        
        ws_quality = writer.sheets['Quality_Report']
        for col_num, value in enumerate(df_quality.columns):
            ws_quality.write(0, col_num, value, header_fmt)
        
        # Conditional formatting for decision column
        decision_col = df_quality.columns.get_loc('Research_Decision')
        for row_num in range(1, len(df_quality) + 1):
            decision = df_quality.iloc[row_num-1]['Research_Decision']
            fmt = green_fmt if decision == 'ACCEPT' else (yellow_fmt if decision == 'REVIEW' else red_fmt)
            ws_quality.write(row_num, decision_col, decision, fmt)
        
        # Auto-fit columns
        for i, col in enumerate(df_quality.columns):
            max_len = max(df_quality[col].astype(str).str.len().max(), len(str(col)))
            ws_quality.set_column(i, i, min(max_len + 2, 40))
        
        # ============================================================
        # SHEET 3: PARAMETER AUDIT
        # ============================================================
        df_params.to_excel(writer, index=False, sheet_name='Parameter_Audit')
        
        ws_params = writer.sheets['Parameter_Audit']
        for col_num, value in enumerate(df_params.columns):
            ws_params.write(0, col_num, value, header_fmt)
        
        for i, col in enumerate(df_params.columns):
            max_len = max(df_params[col].astype(str).str.len().max(), len(str(col)))
            ws_params.set_column(i, i, min(max_len + 2, 50))
        
        # ============================================================
        # SHEET 4: PARAMETER SCHEMA
        # ============================================================
        df_schema.to_excel(writer, index=False, sheet_name='Parameter_Schema')
        
        ws_schema = writer.sheets['Parameter_Schema']
        for col_num, value in enumerate(df_schema.columns):
            ws_schema.write(0, col_num, value, header_fmt)
        
        for i, col in enumerate(df_schema.columns):
            max_len = max(df_schema[col].astype(str).str.len().max(), len(str(col)))
            ws_schema.set_column(i, i, min(max_len + 2, 60))
    
    return output_path


# ============================================================
# SCHEMA EXPORT UTILITIES
# ============================================================

def export_schema_json(output_path: str) -> str:
    """Export parameter schema to JSON file."""
    schema_export = {
        "version": "2.0",
        "generated": datetime.now().isoformat(),
        "sections": SECTION_DESCRIPTIONS,
        "steps": PARAMETER_SCHEMA
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(schema_export, f, indent=2)
    
    return output_path


def export_schema_markdown(output_path: str) -> str:
    """Export parameter schema to Markdown file."""
    lines = [
        "# Parameter Schema - Master Quality Report (NB07)",
        "",
        "This document describes all JSON parameters extracted by the Master Quality Report.",
        "",
        f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "---",
        "",
        "## Section Overview",
        "",
        "| Section | Name | Description |",
        "|---------|------|-------------|"
    ]
    
    for section_id, section_name in SECTION_DESCRIPTIONS.items():
        lines.append(f"| {section_id} | {section_name} | Report section |")
    
    lines.extend(["", "---", ""])
    
    for step_name, step_info in PARAMETER_SCHEMA.items():
        lines.extend([
            f"## {step_name}: {step_info['description']}",
            "",
            f"**File Suffix:** `{step_info['file_suffix']}`",
            "",
            "| Parameter Path | Type | Section | Description |",
            "|----------------|------|---------|-------------|"
        ])
        
        for param_path, param_info in step_info["parameters"].items():
            lines.append(
                f"| `{param_path}` | {param_info['type']} | {param_info['section']} | {param_info['description']} |"
            )
        
        lines.extend(["", "---", ""])
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    return output_path


# ============================================================
# UTILITY FUNCTIONS
# ============================================================

def compute_file_hash(filepath: str) -> str:
    """Compute SHA-256 hash of a file."""
    if not os.path.exists(filepath):
        return 'FILE_NOT_FOUND'
    
    sha256_hash = hashlib.sha256()
    try:
        with open(filepath, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except Exception as e:
        return f'ERROR: {str(e)}'


def get_git_hash(project_root: str) -> str:
    """Get current git commit hash."""
    try:
        return subprocess.check_output(
            ['git', 'rev-parse', '--short', 'HEAD'],
            cwd=project_root
        ).decode('ascii').strip()
    except:
        return "unknown"


def print_section_header(title: str, width: int = 80):
    """Print a formatted section header."""
    print("=" * width)
    print(title)
    print("=" * width)
