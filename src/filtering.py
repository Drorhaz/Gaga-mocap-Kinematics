"""
Winter Residual Analysis & Position Filtering Module

This module implements objective low-pass cutoff selection using Winter residual analysis
and zero-lag Butterworth filtering for position data only.

Pipeline placement: After resampling to perfect grid, before derivative computation.

References:
    Winter, D. A. (2009). Biomechanics and motor control of human movement. 4th ed.
    Wren et al. (2006). Efficacy of clinical gait analysis. Gait & Posture, 22(4), 295-305.
"""

import numpy as np
import pandas as pd
import logging
from typing import Tuple, List, Dict, Optional, Union
from scipy.signal import butter, filtfilt

logger = logging.getLogger(__name__)

# Import PSD validation (optional - only if module exists)
try:
    # Try relative import first (when used as package)
    from .filter_validation import (
        validate_winter_filter_multi_signal,
        check_filter_cutoff_validity,
        analyze_filter_psd_preservation
    )
    PSD_VALIDATION_AVAILABLE = True
except ImportError:
    try:
        # Fallback to absolute import (when run from notebook)
        from filter_validation import (
            validate_winter_filter_multi_signal,
            check_filter_cutoff_validity,
            analyze_filter_psd_preservation
        )
        PSD_VALIDATION_AVAILABLE = True
    except ImportError:
        PSD_VALIDATION_AVAILABLE = False
        logger.warning("PSD validation module not available - validation metrics will be skipped")


# =============================================================================
# GATE 3: PER-REGION BODY DEFINITIONS (Fixed Cutoffs for Research Reproducibility)
# =============================================================================
# Fixed per-region cutoffs based on biomechanical literature (Winter, 2009; Robertson, 2014)
# Rationale: Distal segments move faster than proximal; dance/athletic movements need
# higher cutoffs than walking gait. Fixed values ensure reproducibility across sessions.
#
# Literature support:
#   - Walking: 6 Hz (Winter 2009)
#   - Running: 6-12 Hz (Robertson 2014)
#   - Athletic/Dance: 8-15 Hz (sports biomechanics consensus)
#   - Hand manipulation: up to 20 Hz (upper extremity studies)

BODY_REGIONS = {
    'trunk': {
        'patterns': ['Pelvis', 'Spine', 'Torso', 'Hips', 'Abdomen', 'Chest', 'Back'],
        'fixed_cutoff': 6,            # Conservative: 6 Hz for core stability (Winter 2009 gait standard)
        'cutoff_range': (4, 10),      # Range for validation only
        'rationale': 'Core movements - 6 Hz based on gait literature, conservative for trunk stability'
    },
    'head': {
        'patterns': ['Head', 'Neck'],
        'fixed_cutoff': 8,            # Moderate: 8 Hz for head dynamics
        'cutoff_range': (6, 12),      # Range for validation only
        'rationale': 'Head dynamics - 8 Hz preserves head movements while reducing noise'
    },
    'upper_proximal': {
        'patterns': ['Shoulder', 'Clavicle', 'Scapula', 'UpperArm', 'Arm'],
        'fixed_cutoff': 8,            # Moderate: 8 Hz for shoulder/arm
        'cutoff_range': (6, 12),      # Range for validation only
        'rationale': 'Shoulder/upper arm - 8 Hz preserves arm swings and reaches'
    },
    'upper_distal': {
        'patterns': ['Elbow', 'Forearm', 'ForeArm', 'Wrist', 'Hand', 'Finger', 'Thumb', 'Index', 'Middle', 'Ring', 'Pinky'],
        'fixed_cutoff': 10,           # Higher: 10 Hz for fast hand/finger movements
        'cutoff_range': (8, 14),      # Range for validation only
        'rationale': 'Hands/fingers - 10 Hz preserves hand flicks and finger articulation'
    },
    'lower_proximal': {
        'patterns': ['Thigh', 'UpLeg', 'UpperLeg', 'Knee'],
        'fixed_cutoff': 8,            # Moderate: 8 Hz for leg dynamics
        'cutoff_range': (6, 12),      # Range for validation only
        'rationale': 'Upper leg - 8 Hz preserves leg swings and knee movements'
    },
    'lower_distal': {
        'patterns': ['Ankle', 'Leg', 'LowerLeg', 'Foot', 'Toe', 'ToeBase', 'Heel'],
        'fixed_cutoff': 10,           # Higher: 10 Hz for foot strikes and toe movements
        'cutoff_range': (8, 14),      # Range for validation only
        'rationale': 'Lower leg/foot - 10 Hz preserves foot strikes and toe articulation'
    }
}

# Global search range for Winter analysis (Gate 3)
WINTER_FMIN = 1   # Minimum cutoff frequency (Hz)
WINTER_FMAX = 16  # Maximum cutoff frequency (Hz) - expanded from 12 for Gaga


def classify_marker_region(marker_name: str) -> str:
    """
    Classify a marker into a body region based on name patterns.
    
    Args:
        marker_name: Marker column name (e.g., 'RightHand__px')
        
    Returns:
        Region name ('trunk', 'head', 'upper_proximal', 'upper_distal', 
                     'lower_proximal', 'lower_distal', 'unknown')
    """
    # Extract base marker name (remove axis suffix)
    base_name = marker_name.replace('__px', '').replace('__py', '').replace('__pz', '')
    
    # Check each region's patterns
    for region, config in BODY_REGIONS.items():
        for pattern in config['patterns']:
            if pattern.lower() in base_name.lower():
                return region
    
    # Default to upper_distal if unknown (conservative for dance)
    logger.debug(f"Marker '{marker_name}' not matched to any region, defaulting to 'upper_distal'")
    return 'upper_distal'


def winter_residual_analysis(signal: np.ndarray, 
                       fs: float, 
                       fmin: int = 1, 
                       fmax: int = 16,
                       min_cutoff: Optional[float] = None,
                       body_region: str = "general",
                       return_details: bool = False,
                       validation_mode: bool = False) -> Union[float, Dict]:
    """
    Perform Winter residual analysis to determine optimal low-pass cutoff frequency.
    
    Gate 3 Implementation: Expanded search range [1, 16] Hz for Gaga high-intensity.
    
    The method analyzes residual RMS across different cutoff frequencies to find the
    knee point where further filtering provides diminishing returns.
    
    Args:
        signal: Input signal array (1D)
        fs: Sampling frequency in Hz
        fmin: Minimum cutoff frequency to test (Hz)
        fmax: Maximum cutoff frequency to test (Hz) - 16Hz for Gaga dynamics
        min_cutoff: Biomechanically-informed minimum cutoff (Hz). If provided, 
                   Winter result will be clamped to this minimum with logging
        body_region: Body region for biomechanical context ("trunk", "distal", "general")
        return_details: If True, return dict with full analysis details instead of just cutoff
        validation_mode: If True, return raw Winter result without weighted compromise (for validation)
        
    Returns:
        If return_details=False: Optimal cutoff frequency (Hz)
        If return_details=True: Dict with {
            'cutoff_hz': final cutoff,
            'raw_cutoff_hz': pre-guardrail cutoff,
            'method_used': detection method,
            'guardrail_applied': bool,
            'guardrail_delta_hz': how much guardrail changed the cutoff,
            'knee_point_found': bool (True if real knee point was found),
            'rms_values': array of RMS at each test frequency,
            'test_frequencies': array of test frequencies,
            'residual_rms_final': RMS at final cutoff,
            'search_range_hz': [fmin, fmax]
        }
        
    Reference:
        Winter, D. A. (2009). Biomechanics and motor control of human movement.
        Note: fmax=16Hz for Gaga. If cutoff=fmax, method failed - investigate pipeline.
    """
    # Convert to float and detrend
    x = signal.astype(float)
    x = x - np.mean(x)
    
    # Check for completely flat signal (no variation) - this should cause failure
    if np.std(x) < 1e-10:  # Essentially zero variation
        logger.error(f"WINTER ANALYSIS FAILED: signal has no variation (std={np.std(x):.2e}). "
                    f"Cannot perform meaningful residual analysis.")
        if return_details:
            return {
                'cutoff_hz': float(fmax),
                'raw_cutoff_hz': float(fmax),
                'method_used': 'flat_signal_failure',
                'guardrail_applied': False,
                'guardrail_delta_hz': 0.0,
                'knee_point_found': False,
                'rms_values': [],
                'test_frequencies': [],
                'residual_rms_final': 0.0,
                'failure_reason': f"Signal has no variation (std={np.std(x):.2e}), cannot perform residual analysis"  # GATE 3
            }
        return float(fmax)
    
    # Test cutoff frequencies
    cutoffs = np.arange(fmin, fmax + 1)
    rms_values = np.zeros(len(cutoffs))
    
    for i, fc in enumerate(cutoffs):
        # Design 2nd-order Butterworth low-pass filter
        b, a = butter(N=2, Wn=fc/(0.5*fs), btype='low')
        
        # Apply zero-lag filtering (forward-backward)
        xf = filtfilt(b, a, x)
        
        # Compute residual RMS
        residual = x - xf
        rms_values[i] = np.sqrt(np.mean(residual**2))
    
    # Enhanced knee rule: find optimal cutoff using multiple criteria
    r_floor = rms_values[-1]  # RMS at fmax
    r_ceiling = rms_values[0]  # RMS at fmin
    
    # Check if RMS curve is essentially flat (no clear knee point)
    rms_range_ratio = (r_ceiling - r_floor) / (r_ceiling + 1e-10)
    curve_is_flat = rms_range_ratio < 0.15  # Less than 15% variation = flat curve
    
    # Method 1: Strict knee rule (1.05 * r_floor)
    knee_candidates_strict = np.where(rms_values <= 1.05 * r_floor)[0]
    
    # Method 2: Relaxed knee rule (1.10 * r_floor) for smooth curves
    knee_candidates_relaxed = np.where(rms_values <= 1.10 * r_floor)[0]
    
    # Method 3: Point of diminishing returns (largest relative drop)
    rms_drops = np.diff(rms_values) / (rms_values[:-1] + 1e-10)  # Relative drop between consecutive cutoffs
    best_drop_idx = np.argmax(np.abs(rms_drops)) + 1  # Index after the largest drop
    max_drop_magnitude = np.abs(rms_drops[best_drop_idx - 1]) if len(rms_drops) > 0 else 0
    
    # Collect all candidate cutoffs from different methods
    candidates = []
    knee_point_found = False
    
    # Only use strict/relaxed knee if the curve isn't flat
    if not curve_is_flat:
        # Add strict knee candidates (prefer lowest)
        if len(knee_candidates_strict) > 0:
            candidates.append((cutoffs[knee_candidates_strict[0]], "strict_knee"))
            knee_point_found = True
        
        # Add relaxed knee candidates (prefer lowest)
        if len(knee_candidates_relaxed) > 0:
            candidates.append((cutoffs[knee_candidates_relaxed[0]], "relaxed_knee"))
            knee_point_found = True
    
    # Method 3: Point of diminishing returns - only if there's a significant drop
    if max_drop_magnitude > 0.05:  # At least 5% relative drop
        diminishing_cutoff = max(4, cutoffs[best_drop_idx])
        if diminishing_cutoff <= 10:  # Only add if in reasonable dance range
            candidates.append((diminishing_cutoff, "diminishing_returns"))
            knee_point_found = True
    
    # Choose cutoff from candidates
    if candidates:
        candidates.sort(key=lambda x: x[0])  # Sort by cutoff frequency (lowest first)
        
        if validation_mode:
            # VALIDATION MODE: Return the strict_knee (where RMS flattens) - most meaningful for validation
            # This tells us "Winter says you need at least X Hz to capture useful signal"
            # Also capture diminishing_returns for reference (where biggest RMS drop happens)
            strict_knee = next((c for c in candidates if c[1] == "strict_knee"), None)
            relaxed_knee = next((c for c in candidates if c[1] == "relaxed_knee"), None)
            diminishing = next((c for c in candidates if c[1] == "diminishing_returns"), None)
            
            # Store both values for reporting
            # Note: diminishing_returns has max(4, ...) floor in the candidate generation
            # Get the raw best_drop_idx value for true diminishing_returns
            raw_diminishing_hz = float(cutoffs[best_drop_idx]) if 'best_drop_idx' in dir() else None
            
            # Prefer strict_knee for validation (where RMS actually stabilizes)
            if strict_knee:
                optimal_fc, method_used = strict_knee
                method_used = f"validation_strict_knee"
            elif relaxed_knee:
                optimal_fc, method_used = relaxed_knee
                method_used = f"validation_relaxed_knee"
            else:
                optimal_fc, method_used = candidates[0]
                method_used = f"validation_raw_{method_used}"
        else:
            # DATA-DRIVEN MODE: Use weighted compromise when needed
            # Get region configuration for biomechanical minimum
            region_config = BODY_REGIONS.get(body_region, {'cutoff_range': (8, 14)})
            min_cutoff_region = region_config['cutoff_range'][0]
            
            # Find the diminishing_returns and strict_knee candidates
            diminishing_candidate = next((c for c in candidates if c[1] == "diminishing_returns"), None)
            strict_knee_candidate = next((c for c in candidates if c[1] == "strict_knee"), None)
            relaxed_knee_candidate = next((c for c in candidates if c[1] == "relaxed_knee"), None)
            
            # Use higher knee candidate if available (prefer strict over relaxed)
            higher_knee_candidate = strict_knee_candidate or relaxed_knee_candidate
            
            # OPTION B: Weighted compromise when diminishing_returns is below minimum
            # and a higher knee-point exists. Weights (0.3, 0.7) are fixed; could be
            # made configurable or data-driven (e.g. from residual slope) for tuning.
            if (diminishing_candidate and higher_knee_candidate and
                diminishing_candidate[0] < min_cutoff_region and
                higher_knee_candidate[0] > diminishing_candidate[0]):

                # Weighted average: 30% diminishing_returns + 70% strict_knee
                # Preserves more high-frequency content for Gaga dance
                weighted_cutoff = (diminishing_candidate[0] * 0.3) + (higher_knee_candidate[0] * 0.7)
                optimal_fc = round(weighted_cutoff, 1)
                method_used = f"weighted_compromise({diminishing_candidate[0]:.1f}*0.3+{higher_knee_candidate[0]:.1f}*0.7)"
                
                logger.info(f"WEIGHTED COMPROMISE for {body_region}: diminishing_returns={diminishing_candidate[0]:.1f}Hz "
                           f"is below min={min_cutoff_region}Hz, strict_knee={higher_knee_candidate[0]:.1f}Hz. "
                           f"Using weighted average: {optimal_fc:.1f}Hz")
            else:
                # Original behavior: pick the lowest candidate
                optimal_fc, method_used = candidates[0]
    else:
        # NO KNEE POINT FOUND - Gate 3: Use region-specific fallback
        # Get region's upper cutoff limit as fallback (not fmax/2)
        region_config = BODY_REGIONS.get(body_region, {'cutoff_range': (8, 14)})
        optimal_fc = region_config['cutoff_range'][1]  # Use region's max as fallback
        method_used = f"no_knee_point_fallback_{body_region}"
        knee_point_found = False
        logger.warning(f"WINTER KNEE-POINT NOT FOUND for {body_region}: RMS curve is flat (range ratio={rms_range_ratio:.2%}). "
                      f"Using region fallback cutoff {optimal_fc} Hz. This may indicate pre-smoothed data.")
    
    raw_optimal_fc = optimal_fc  # Store pre-guardrail value
    
    # Final validation: if we still get fmax, it means the data is very smooth
    if optimal_fc >= fmax - 1:
        optimal_fc = fmax
        method_used = "fmax_fallback"
        knee_point_found = False
        logger.info(f"RMS analysis suggests fmax ({optimal_fc} Hz) - fixed literature cutoff will be used instead. "
                   f"(This is expected with clean/pre-filtered data)")
    else:
        if knee_point_found:
            logger.info(f"Winter analysis: selected cutoff {optimal_fc} Hz ({method_used})")
        else:
            logger.warning(f"Winter analysis: using fallback cutoff {optimal_fc} Hz ({method_used}) - no clear knee point")
    
    # Apply biomechanical guardrails if min_cutoff is specified
    guardrail_applied = False
    guardrail_delta = 0.0
    
    if min_cutoff is not None:
        original_fc = optimal_fc
        optimal_fc = max(optimal_fc, min_cutoff)
        guardrail_delta = optimal_fc - original_fc
        
        if optimal_fc > original_fc:
            guardrail_applied = True
            logger.warning(f"BIOMECHANICAL GUARDRAIL OVERRIDE: Winter cutoff {original_fc:.1f} Hz "
                          f"clamped to {optimal_fc:.1f} Hz (min_cutoff={min_cutoff:.1f} Hz) "
                          f"for {body_region} region. Delta = +{guardrail_delta:.1f} Hz. "
                          f"This may indicate the data is pre-smoothed or contains low dynamics.")
        else:
            logger.info(f"BIOMECHANICAL GUARDRAIL: Winter cutoff {original_fc:.1f} Hz "
                        f"within acceptable range (>= {min_cutoff:.1f} Hz) for {body_region} region.")
    
    # Get final residual RMS at the chosen cutoff
    cutoff_idx = np.argmin(np.abs(cutoffs - optimal_fc))
    residual_rms_final = float(rms_values[cutoff_idx])
    
    # Calculate residual slope at the knee-point to assess convergence quality
    # Slope indicates how quickly the residual is decreasing at the selected cutoff
    if cutoff_idx > 0 and cutoff_idx < len(rms_values) - 1:
        # Use central difference for slope estimate
        residual_slope = (rms_values[cutoff_idx + 1] - rms_values[cutoff_idx - 1]) / 2.0
    elif cutoff_idx == 0:
        # Forward difference at boundary
        residual_slope = rms_values[1] - rms_values[0]
    else:
        # Backward difference at boundary
        residual_slope = rms_values[-1] - rms_values[-2]
    
    residual_slope = float(residual_slope)
    
    if return_details:
        # GATE 3 FIX: Generate human-readable failure reason for audit transparency
        failure_reason_detail = None
        if not knee_point_found:
            if curve_is_flat:
                failure_reason_detail = f"RMS curve is flat (range={rms_range_ratio:.1%}), no clear knee-point detected in {fmin}-{fmax}Hz range"
            elif optimal_fc >= fmax - 1:
                failure_reason_detail = f"Cutoff at fmax ({optimal_fc}Hz), residual slope did not plateau in {fmin}-{fmax}Hz range"
            else:
                failure_reason_detail = f"No knee-point found, using {method_used} fallback cutoff"
        elif guardrail_applied and guardrail_delta >= 2.0:
            failure_reason_detail = f"Biomechanical guardrail override: +{guardrail_delta:.1f}Hz from {raw_optimal_fc:.1f}Hz to {optimal_fc:.1f}Hz"
        
        # Extract all candidate values for detailed reporting
        strict_knee_hz = next((c[0] for c in candidates if c[1] == "strict_knee"), None) if candidates else None
        relaxed_knee_hz = next((c[0] for c in candidates if c[1] == "relaxed_knee"), None) if candidates else None
        diminishing_returns_hz = next((c[0] for c in candidates if c[1] == "diminishing_returns"), None) if candidates else None
        # Get raw diminishing_returns (before max(4, ...) floor)
        raw_diminishing_hz = float(cutoffs[best_drop_idx]) if 'best_drop_idx' in dir() and best_drop_idx < len(cutoffs) else None
        
        return {
            'cutoff_hz': float(optimal_fc),
            'raw_cutoff_hz': float(raw_optimal_fc),
            'method_used': method_used,
            'guardrail_applied': guardrail_applied,
            'guardrail_delta_hz': float(guardrail_delta),
            'knee_point_found': knee_point_found,
            'rms_values': rms_values.tolist(),
            'test_frequencies': cutoffs.tolist(),
            'residual_rms_final': residual_rms_final,
            'residual_slope': residual_slope,  # NEW: Slope at knee-point for convergence quality
            'rms_range_ratio': float(rms_range_ratio),
            'curve_is_flat': curve_is_flat,
            'search_range_hz': [fmin, fmax],
            'body_region': body_region,
            'failure_reason': failure_reason_detail,  # GATE 3: Detailed failure reason
            # Winter analysis breakdown for audit report
            'strict_knee_hz': float(strict_knee_hz) if strict_knee_hz else None,
            'relaxed_knee_hz': float(relaxed_knee_hz) if relaxed_knee_hz else None,
            'diminishing_returns_hz': float(diminishing_returns_hz) if diminishing_returns_hz else None,
            'raw_diminishing_hz': float(raw_diminishing_hz) if raw_diminishing_hz else None
        }
    
    return float(optimal_fc)


# =============================================================================
# 3-STAGE SIGNAL CLEANING PIPELINE
# =============================================================================
# Generic "Gatekeeper" Architecture for handling artifacts and noise separately
# Stage 1: Artifact Detector (Z-Score + Velocity) - identifies tracking spikes
# Stage 2: Hampel Filter (Sliding Window Median) - removes outliers
# Stage 3: Adaptive Low-Pass (Winter's Residual Method) - per-joint optimal cutoff


def detect_artifact_gaps(signal: np.ndarray, 
                        fs: float,
                        velocity_limit: float = 1800.0,
                        zscore_threshold: float = 5.0) -> Tuple[np.ndarray, Dict]:
    """
    Stage 1: Artifact Detector using Z-Score + Velocity thresholds.

    Used by: 3-stage method (Stage 1). Detects position spikes via frame-to-frame
    velocity (diff(position)/dt) and flags frames for interpolation.

    Detects tracking artifacts (spikes) by identifying frames where:
    1. Velocity exceeds physiological limit (e.g. 5000 mm/s for position)
    2. Velocity is > zscore_threshold standard deviations from session mean

    Rationale: Configurable velocity_limit and zscore_threshold (config_v1.yaml)
    allow tuning; 5000 mm/s ‚âà 5 m/s (very fast for body markers); 5œÉ flags
    extreme outliers while sparing normal dynamics. Validate with printed
    breakdown (velocity vs zscore) and frame counts.

    Args:
        signal: 1D position signal
        fs: Sampling frequency (Hz)
        velocity_limit: Physiological velocity limit in signal units per second
                       (e.g., 5000 mm/s for position, 1800 deg/s for angles)
        zscore_threshold: Z-score threshold for outlier detection (default: 5.0)

    Returns:
        artifact_mask: Boolean mask where True = frame marked for interpolation
        stats: Dict with n_velocity_spikes, n_zscore_spikes, n_frames_marked
    """
    if len(signal) < 3:
        return np.zeros(len(signal), dtype=bool), {
            "n_velocity_spikes": 0, "n_zscore_spikes": 0, "n_frames_marked": 0
        }

    # Compute velocity (change per frame, then convert to per-second)
    dt = 1.0 / fs
    velocity = np.diff(signal) / dt  # Units: signal_units/s

    # Compute session statistics
    velocity_mean = np.nanmean(velocity)
    velocity_std = np.nanstd(velocity)

    # Avoid division by zero
    if velocity_std < 1e-10:
        velocity_std = 1e-10

    # Compute z-scores
    z_scores = np.abs((velocity - velocity_mean) / velocity_std)

    # For position data, use absolute velocity magnitude
    abs_velocity = np.abs(velocity)

    # Mark frames where velocity spike detected
    velocity_spikes = abs_velocity > velocity_limit
    zscore_spikes = z_scores > zscore_threshold

    # Combine: artifact if EITHER condition is met
    spike_mask = velocity_spikes | zscore_spikes

    # Mark the frame AFTER the spike (where the gap occurs) and the frame WITH the spike
    artifact_mask = np.zeros(len(signal), dtype=bool)
    artifact_mask[1:][spike_mask] = True
    artifact_mask[:-1][spike_mask] = True

    n_vel = int(np.sum(velocity_spikes))
    n_z = int(np.sum(zscore_spikes))
    # Frames marked can double-count where both conditions hit same transition
    n_marked = int(np.sum(artifact_mask))

    stats = {
        "n_velocity_spikes": n_vel,
        "n_zscore_spikes": n_z,
        "n_frames_marked": n_marked,
    }
    return artifact_mask, stats


def apply_hampel_filter(signal: np.ndarray, 
                       window_size: int = 5,
                       n_sigma: float = 3.0) -> Tuple[np.ndarray, np.ndarray]:
    """
    Stage 2: Hampel Filter - Universal outlier "snipper" using sliding window median.
    
    Slides a window across data and replaces any point that is an outlier compared to
    its immediate neighbors with the median of that window.
    
    Generic Rule: A window of 5-7 frames is usually enough to "snip" MoCap artifacts
    while preserving 5-10 Hz dance dynamics.
    
    Args:
        signal: 1D signal array
        window_size: Size of sliding window (default: 5, recommended: 5-7)
        n_sigma: Number of standard deviations for outlier threshold (default: 3.0)
        
    Returns:
        Tuple of (filtered_signal, outlier_mask)
        - filtered_signal: Signal with outliers replaced by window median
        - outlier_mask: Boolean mask indicating which points were replaced
    """
    if len(signal) < window_size:
        return signal.copy(), np.zeros(len(signal), dtype=bool)
    
    filtered = signal.copy().astype(float)
    outlier_mask = np.zeros(len(signal), dtype=bool)
    
    half_window = window_size // 2
    
    for i in range(len(signal)):
        # Define window boundaries
        start_idx = max(0, i - half_window)
        end_idx = min(len(signal), i + half_window + 1)
        
        window_data = signal[start_idx:end_idx]
        
        # Skip if all NaN
        if np.all(np.isnan(window_data)):
            continue
        
        # Compute median and MAD (Median Absolute Deviation)
        window_median = np.nanmedian(window_data)
        mad = np.nanmedian(np.abs(window_data - window_median))
        
        # Convert MAD to standard deviation equivalent
        # MAD ‚âà 0.6745 * std for normal distribution
        sigma = mad / 0.6745 if mad > 1e-10 else 1e-10
        
        # Check if current point is an outlier
        deviation = np.abs(signal[i] - window_median)
        if deviation > n_sigma * sigma:
            filtered[i] = window_median
            outlier_mask[i] = True
    
    return filtered, outlier_mask


def apply_quaternion_median_filter(df: pd.DataFrame,
                                   quat_cols: List[str],
                                   window_size: int = 5) -> Tuple[pd.DataFrame, Dict]:
    """
    Apply scipy.signal.medfilt to quaternion columns to remove flipping artifacts.
    
    This is a "surgical" clean that:
    1. Applies scipy.signal.medfilt to each quaternion component (qx, qy, qz, qw)
    2. Renormalizes quaternions after filtering to maintain unit length
    3. Does NOT apply low-pass filtering (which would break quaternion geometry)
    
    Use case: Fixes quaternion "flips" (e.g., 330¬∞ Hip ROM issues) without 
    distorting orientation data.
    
    Args:
        df: Input DataFrame with quaternion columns
        quat_cols: List of quaternion column names (ending in __qx, __qy, __qz, __qw)
        window_size: Median filter window size (default: 5, must be odd)
        
    Returns:
        Tuple of (filtered_dataframe, metadata_dict)
    """
    from scipy.signal import medfilt
    
    df_out = df.copy()
    
    # Ensure window_size is odd (required by medfilt)
    if window_size % 2 == 0:
        window_size += 1
        logger.info(f"Adjusted window_size to {window_size} (must be odd for medfilt)")
    
    # Group quaternion columns by joint
    joint_quats = {}
    for col in quat_cols:
        # Extract joint name (everything before __qx, __qy, __qz, __qw)
        for suffix in ['__qx', '__qy', '__qz', '__qw']:
            if col.endswith(suffix):
                joint_name = col[:-len(suffix)]
                if joint_name not in joint_quats:
                    joint_quats[joint_name] = {}
                component = suffix[2:]  # 'qx', 'qy', 'qz', 'qw'
                joint_quats[joint_name][component] = col
                break
    
    metadata = {
        'method': 'scipy.signal.medfilt',
        'window_size': window_size,
        'total_joints': len(joint_quats),
        'total_columns_filtered': 0,
        'per_joint_results': {}
    }
    
    total_cols_filtered = 0
    
    for joint_name, components in joint_quats.items():
        # Check if we have all 4 components
        if len(components) != 4:
            logger.warning(f"Joint {joint_name} missing quaternion components, skipping")
            continue
        
        # Apply medfilt to each component
        for comp_name, col in components.items():
            signal = df[col].values.astype(float)
            
            # Skip if all NaN
            if np.all(np.isnan(signal)):
                df_out[col] = signal
                continue
            
            # Handle NaNs: fill temporarily, filter, restore NaN positions
            nan_mask = np.isnan(signal)
            if np.any(nan_mask):
                # Fill NaNs with interpolated values for filtering
                signal_filled = pd.Series(signal).interpolate(limit_direction='both').values
                signal_filled = np.nan_to_num(signal_filled, nan=0.0)
            else:
                signal_filled = signal
            
            # Apply scipy.signal.medfilt
            filtered = medfilt(signal_filled, kernel_size=window_size)
            
            # Restore NaN positions
            if np.any(nan_mask):
                filtered[nan_mask] = np.nan
            
            df_out[col] = filtered
            total_cols_filtered += 1
        
        # Renormalize quaternions to unit length
        qx_col = components.get('qx')
        qy_col = components.get('qy')
        qz_col = components.get('qz')
        qw_col = components.get('qw')
        
        if all([qx_col, qy_col, qz_col, qw_col]):
            qx = df_out[qx_col].values
            qy = df_out[qy_col].values
            qz = df_out[qz_col].values
            qw = df_out[qw_col].values
            
            # Compute quaternion magnitude
            mag = np.sqrt(qx**2 + qy**2 + qz**2 + qw**2)
            
            # Avoid division by zero
            mag = np.where(mag < 1e-10, 1.0, mag)
            
            # Renormalize
            df_out[qx_col] = qx / mag
            df_out[qy_col] = qy / mag
            df_out[qz_col] = qz / mag
            df_out[qw_col] = qw / mag
        
        metadata['per_joint_results'][joint_name] = {
            'components_filtered': list(components.keys()),
            'renormalized': True
        }
    
    metadata['total_columns_filtered'] = int(total_cols_filtered)
    
    logger.info(f"Quaternion medfilt complete: {total_cols_filtered} columns filtered across {len(joint_quats)} joints")
    
    return df_out, metadata


def apply_adaptive_winter_filter(signal: np.ndarray,
                                fs: float,
                                fmin: float = 1.0,
                                fmax: float = 20.0,
                                min_cutoff: Optional[float] = None) -> Tuple[np.ndarray, Dict]:
    """
    Stage 3: Adaptive Low-Pass using Winter's Residual Method.
    
    Automatically calculates optimal cutoff for each joint/session by finding the
    "elbow" in the residual curve - where you stop removing noise and start removing signal.
    
    The script tries every cutoff from fmin to fmax Hz and looks for the knee point.
    
    Args:
        signal: 1D signal array (already artifact-cleaned)
        fs: Sampling frequency (Hz)
        fmin: Minimum cutoff to test (Hz, default: 1.0)
        fmax: Maximum cutoff to test (Hz, default: 20.0)
        min_cutoff: Biomechanical minimum cutoff (Hz, optional)
        
    Returns:
        Tuple of (filtered_signal, metadata_dict)
        - filtered_signal: Low-pass filtered signal
        - metadata_dict: Contains cutoff_hz, method_used, etc.
    """
    # Run Winter residual analysis
    winter_result = winter_residual_analysis(
        signal, fs, fmin=int(fmin), fmax=int(fmax),
        min_cutoff=min_cutoff, return_details=True
    )
    
    if isinstance(winter_result, dict):
        cutoff_hz = winter_result['cutoff_hz']
        metadata = winter_result.copy()
    else:
        cutoff_hz = float(winter_result)
        metadata = {'cutoff_hz': cutoff_hz}
    
    # Apply Butterworth filter with optimal cutoff
    if cutoff_hz >= fs * 0.5 - 1:
        # Cutoff too high, skip filtering
        logger.warning(f"Cutoff {cutoff_hz:.1f} Hz too close to Nyquist ({fs*0.5:.1f} Hz), skipping filter")
        return signal.copy(), metadata
    
    # Design 2nd-order Butterworth low-pass filter
    b, a = butter(N=2, Wn=cutoff_hz/(0.5*fs), btype='low')
    
    # Apply zero-phase filter (filtfilt)
    filtered = filtfilt(b, a, signal.astype(float))
    
    metadata['filter_applied'] = True
    metadata['filter_type'] = 'Butterworth 2nd-order (zero-phase)'
    
    return filtered, metadata


def apply_signal_cleaning_pipeline(df: pd.DataFrame,
                                  fs: float,
                                  pos_cols: List[str],
                                  velocity_limit: float = 1800.0,
                                  zscore_threshold: float = 5.0,
                                  hampel_window: int = 5,
                                  hampel_n_sigma: float = 3.0,
                                  winter_fmin: float = 1.0,
                                  winter_fmax: float = 20.0,
                                  per_joint_winter: bool = True,
                                  stage1_interpolation_method: str = 'pchip') -> Tuple[pd.DataFrame, Dict]:
    """
    Apply 3-Stage Signal Cleaning Pipeline: Artifact Detection ‚Üí Hampel ‚Üí Adaptive Winter.

    Used by: 3-stage method (config_v1.yaml filtering.method = "3_stage").
    Does: Stage 1 artifact detection (velocity + z-score) ‚Üí Stage 2 Hampel ‚Üí Stage 3
    adaptive Winter per column; returns cleaned dataframe and pipeline metadata.

    Why cleaning before low-pass: Savitzky-Golay (used in notebook 06 for derivatives)
    smooths when differentiating; it does not remove spikes. A spike in position would
    remain and be smoothed into a bump, affecting derivatives. Stage 1+2 remove or
    correct spikes and outliers first; Winter (Stage 3) then low-pass filters the
    cleaned signal. So all three stages are needed when data has artifacts.

    This generic "Gatekeeper" architecture treats artifacts (spikes) and noise (jitter)
    as two separate problems, and can be applied to any joint in any session.

    Pipeline:
    1. Stage 1 (Artifact Detector): Identifies tracking spikes using velocity + z-score
    2. Stage 2 (Hampel Filter): Removes outliers using sliding window median
    3. Stage 3 (Adaptive Winter): Finds optimal cutoff per-joint using residual analysis

    Args:
        df: Input DataFrame with position columns
        fs: Sampling frequency (Hz)
        pos_cols: List of position column names to process
        velocity_limit: Physiological velocity limit in signal units per second
                       (e.g., 5000 mm/s for position data, 1800 deg/s for angular)
        zscore_threshold: Z-score threshold for outlier detection (default: 5.0)
        hampel_window: Hampel filter window size (default: 5, recommended: 5-7)
        hampel_n_sigma: Hampel filter outlier threshold (default: 3.0)
        winter_fmin: Minimum cutoff for Winter analysis (Hz)
        winter_fmax: Maximum cutoff for Winter analysis (Hz)
        per_joint_winter: Stored in pipeline metadata only. The implementation
                         always runs Winter analysis per column (adaptive cutoff per
                         joint); there is no single-global-Winter path when False.
        stage1_interpolation_method: Method for filling artifact gaps: 'pchip' (default),
                         'linear', or 'cubic'. PCHIP is shape-preserving (no overshoot),
                         smooth; linear is most conservative; cubic can overshoot.

    Returns:
        Tuple of (cleaned_dataframe, pipeline_metadata)
    """
    df_out = df.copy()
    pipeline_metadata = {
        'pipeline_type': '3_stage_signal_cleaning',
        'stages': {
            'stage1_artifact_detector': {
                'method': 'Z-Score + Velocity Threshold',
                'velocity_limit': velocity_limit,
                'zscore_threshold': zscore_threshold,
                'interpolation_method': stage1_interpolation_method
            },
            'stage2_hampel': {
                'method': 'Sliding Window Median',
                'window_size': hampel_window,
                'n_sigma': hampel_n_sigma
            },
            'stage3_adaptive_winter': {
                'method': "Winter's Residual Method",
                'fmin': winter_fmin,
                'fmax': winter_fmax,
                'per_joint': per_joint_winter
            }
        },
        'per_joint_results': {}
    }
    
    # Validate position columns
    pos_cols_valid = [col for col in pos_cols if col in df.columns and not df[col].isna().all()]
    
    if not pos_cols_valid:
        raise ValueError(f"No valid position columns found in {len(pos_cols)} provided columns")
    
    logger.info(f"Applying 3-stage signal cleaning pipeline to {len(pos_cols_valid)} position columns")
    
    # Helper: count contiguous True segments in a boolean mask
    def _count_segments(mask: np.ndarray) -> int:
        if not np.any(mask):
            return 0
        padded = np.concatenate([[False], mask, [False]])
        runs = np.diff(padded.astype(int))
        return int(np.sum(runs == 1))

    # Process each position column
    total_artifacts = 0
    total_artifact_segments = 0
    total_velocity_spikes = 0
    total_zscore_spikes = 0
    total_hampel_outliers = 0
    n_frames_total = len(df) if len(pos_cols_valid) > 0 else 0

    for col in pos_cols_valid:
        signal = df[col].values.astype(float)

        # Skip if all NaN
        if np.all(np.isnan(signal)):
            continue

        col_metadata = {}

        # Stage 1: Artifact Detection
        artifact_mask, artifact_stats = detect_artifact_gaps(
            signal, fs,
            velocity_limit=velocity_limit,
            zscore_threshold=zscore_threshold
        )
        n_artifacts = artifact_stats["n_frames_marked"]
        n_segments = _count_segments(artifact_mask)
        total_artifacts += n_artifacts
        total_artifact_segments += n_segments
        total_velocity_spikes += artifact_stats["n_velocity_spikes"]
        total_zscore_spikes += artifact_stats["n_zscore_spikes"]

        # Interpolate artifacts (method: linear or cubic; document in summary)
        signal_stage1 = signal.copy()
        if n_artifacts > 0:
            signal_series = pd.Series(signal_stage1)
            signal_series.loc[artifact_mask] = np.nan
            signal_stage1 = signal_series.interpolate(
                method=stage1_interpolation_method,
                limit_direction='both'
            ).values
            signal_stage1 = pd.Series(signal_stage1).bfill().ffill().values

        col_metadata['stage1_artifacts_detected'] = int(n_artifacts)
        col_metadata['stage1_segments_interpolated'] = n_segments
        col_metadata['stage1_velocity_spikes'] = artifact_stats["n_velocity_spikes"]
        col_metadata['stage1_zscore_spikes'] = artifact_stats["n_zscore_spikes"]
        
        # Stage 2: Hampel Filter
        signal_stage2, hampel_outliers = apply_hampel_filter(
            signal_stage1,
            window_size=hampel_window,
            n_sigma=hampel_n_sigma
        )
        n_hampel = np.sum(hampel_outliers)
        total_hampel_outliers += n_hampel
        col_metadata['stage2_hampel_outliers'] = int(n_hampel)
        
        # Stage 3: Adaptive Winter Filter
        # Determine minimum cutoff based on body region
        marker_region = classify_marker_region(col)
        region_config = BODY_REGIONS.get(marker_region, BODY_REGIONS['upper_distal'])
        min_cutoff = region_config.get('fixed_cutoff', 6.0)  # Use fixed cutoff as minimum
        
        signal_stage3, winter_meta = apply_adaptive_winter_filter(
            signal_stage2,
            fs=fs,
            fmin=winter_fmin,
            fmax=winter_fmax,
            min_cutoff=min_cutoff
        )
        
        col_metadata['stage3_winter_cutoff'] = winter_meta.get('cutoff_hz', None)
        col_metadata['stage3_winter_method'] = winter_meta.get('method_used', 'unknown')
        col_metadata['marker_region'] = marker_region
        
        # Store result
        df_out[col] = signal_stage3
        pipeline_metadata['per_joint_results'][col] = col_metadata
    
    # Summary statistics (frames = samples; segments = contiguous interpolated runs)
    pipeline_metadata['summary'] = {
        'total_columns_processed': len(pos_cols_valid),
        'n_frames_total': n_frames_total,
        'total_artifact_frames': int(total_artifacts),
        'total_artifact_segments': int(total_artifact_segments),
        'total_velocity_spikes': int(total_velocity_spikes),
        'total_zscore_spikes': int(total_zscore_spikes),
        'total_hampel_outliers': int(total_hampel_outliers),
        'stage1_interpolation_method': stage1_interpolation_method,
        'avg_artifacts_per_column': total_artifacts / len(pos_cols_valid) if pos_cols_valid else 0,
        'avg_hampel_outliers_per_column': total_hampel_outliers / len(pos_cols_valid) if pos_cols_valid else 0,
        'artifact_frames_pct': (100.0 * total_artifacts / (len(pos_cols_valid) * n_frames_total)) if pos_cols_valid and n_frames_total else 0.0,
        'hampel_frames_pct': (100.0 * total_hampel_outliers / (len(pos_cols_valid) * n_frames_total)) if pos_cols_valid and n_frames_total else 0.0,
    }
    
    # Compute cutoff statistics
    cutoffs = [meta.get('stage3_winter_cutoff') for meta in pipeline_metadata['per_joint_results'].values()
               if meta.get('stage3_winter_cutoff') is not None]
    if cutoffs:
        pipeline_metadata['summary']['winter_cutoff_stats'] = {
            'min': float(np.min(cutoffs)),
            'max': float(np.max(cutoffs)),
            'mean': float(np.mean(cutoffs)),
            'median': float(np.median(cutoffs)),
            'std': float(np.std(cutoffs))
        }
    
    logger.info(
        f"3-stage pipeline complete: Stage1 interpolated {total_artifacts} frames in {total_artifact_segments} segments "
        f"(velocity_spikes={total_velocity_spikes}, zscore_spikes={total_zscore_spikes}), method={stage1_interpolation_method}; "
        f"Stage2 Hampel replaced {total_hampel_outliers} frames ({pipeline_metadata['summary'].get('hampel_frames_pct', 0):.2f}% of samples)"
    )
    return df_out, pipeline_metadata


def print_pipeline_debug_logs(pipeline_metadata: Dict, 
                              top_n_joints: int = 20,
                              show_all: bool = False) -> None:
    """
    Print detailed debug logs for the 3-stage signal cleaning pipeline.
    
    Args:
        pipeline_metadata: Metadata dictionary from apply_signal_cleaning_pipeline
        top_n_joints: Number of top joints to show in detail (default: 20)
        show_all: If True, show all joints regardless of top_n_joints
    """
    print(f"\n{'='*80}")
    print(f"üìä DETAILED PIPELINE DEBUG LOGS")
    print(f"{'='*80}\n")
    
    # Pipeline configuration
    stages = pipeline_metadata.get('stages', {})
    print("üîß PIPELINE CONFIGURATION:")
    print(f"   Stage 1 (Artifact Detector):")
    stage1 = stages.get('stage1_artifact_detector', {})
    print(f"     - Velocity limit: {stage1.get('velocity_limit', 'N/A')} mm/s")
    print(f"     - Z-score threshold: {stage1.get('zscore_threshold', 'N/A')}œÉ")
    print(f"     - Interpolation method: {stage1.get('interpolation_method', 'linear')} (documented)")
    print(f"   Stage 2 (Hampel Filter):")
    stage2 = stages.get('stage2_hampel', {})
    print(f"     - Window size: {stage2.get('window_size', 'N/A')} frames")
    print(f"     - Outlier threshold: {stage2.get('n_sigma', 'N/A')}œÉ")

    # Stage 1 & 2 impact: frames and segments (all numbers are frame counts)
    summary = pipeline_metadata.get('summary', {})
    n_frames = summary.get('n_frames_total', 0)
    n_cols = summary.get('total_columns_processed', 1)
    total_pos_samples = n_frames * n_cols if n_frames and n_cols else 1
    print(f"\nüìà STAGE 1 & 2 IMPACT (all numbers in frames = samples):")
    print(f"   Stage 1 (interpolation):")
    print(f"     - Frames interpolated: {summary.get('total_artifact_frames', 0)} ({summary.get('artifact_frames_pct', 0):.2f}% of position samples)")
    print(f"     - Contiguous segments interpolated: {summary.get('total_artifact_segments', 0)}")
    print(f"     - Triggered by velocity limit: {summary.get('total_velocity_spikes', 0)} frame transitions")
    print(f"     - Triggered by z-score: {summary.get('total_zscore_spikes', 0)} frame transitions")
    print(f"   Stage 2 (Hampel):")
    print(f"     - Frames replaced (outliers): {summary.get('total_hampel_outliers', 0)} ({summary.get('hampel_frames_pct', 0):.2f}% of position samples)")
    print(f"   Stage 3 (Adaptive Winter):")
    stage3 = stages.get('stage3_adaptive_winter', {})
    print(f"     - Frequency range: {stage3.get('fmin', 'N/A')} - {stage3.get('fmax', 'N/A')} Hz")
    print(f"     - Per-joint analysis: {stage3.get('per_joint', 'N/A')}")
    
    # Summary statistics
    summary = pipeline_metadata.get('summary', {})
    print(f"\nüìà SUMMARY STATISTICS:")
    print(f"   Total columns processed: {summary.get('total_columns_processed', 0)}")
    print(f"   Total artifacts detected: {summary.get('total_artifact_frames', 0)}")
    print(f"   Total Hampel outliers: {summary.get('total_hampel_outliers', 0)}")
    print(f"   Avg artifacts per column: {summary.get('avg_artifacts_per_column', 0):.2f}")
    print(f"   Avg Hampel outliers per column: {summary.get('avg_hampel_outliers_per_column', 0):.2f}")
    
    # Winter cutoff statistics
    if 'winter_cutoff_stats' in summary:
        cutoff_stats = summary['winter_cutoff_stats']
        print(f"\nüéØ ADAPTIVE WINTER CUTOFF STATISTICS:")
        print(f"   Range: {cutoff_stats['min']:.2f} - {cutoff_stats['max']:.2f} Hz")
        print(f"   Mean: {cutoff_stats['mean']:.2f} Hz")
        print(f"   Median: {cutoff_stats['median']:.2f} Hz")
        print(f"   Std Dev: {cutoff_stats['std']:.2f} Hz")
    
    # Per-joint detailed results
    per_joint = pipeline_metadata.get('per_joint_results', {})
    if per_joint:
        print(f"\n{'='*80}")
        print(f"üîç PER-JOINT DETAILED RESULTS")
        print(f"{'='*80}\n")
        
        # Sort joints by total issues (artifacts + hampel outliers)
        joint_scores = []
        for col, meta in per_joint.items():
            artifacts = meta.get('stage1_artifacts_detected', 0)
            hampel = meta.get('stage2_hampel_outliers', 0)
            total_issues = artifacts + hampel
            joint_scores.append((col, meta, total_issues))
        
        # Sort by total issues (descending)
        joint_scores.sort(key=lambda x: x[2], reverse=True)
        
        # Determine how many to show
        n_to_show = len(joint_scores) if show_all else min(top_n_joints, len(joint_scores))
        
        print(f"Showing top {n_to_show} joints (sorted by total issues detected):\n")
        print(f"{'Joint':<40} {'Region':<15} {'Artifacts':<12} {'Hampel':<12} {'Winter Cutoff':<15} {'Method':<15}")
        print(f"{'-'*40} {'-'*15} {'-'*12} {'-'*12} {'-'*15} {'-'*15}")
        
        for col, meta, total_issues in joint_scores[:n_to_show]:
            joint_name = col.replace('__px', '').replace('__py', '').replace('__pz', '')[:38]
            region = meta.get('marker_region', 'unknown')
            artifacts = meta.get('stage1_artifacts_detected', 0)
            hampel = meta.get('stage2_hampel_outliers', 0)
            cutoff = meta.get('stage3_winter_cutoff', None)
            method = meta.get('stage3_winter_method', 'unknown')
            
            cutoff_str = f"{cutoff:.2f} Hz" if cutoff is not None else "N/A"
            method_str = method[:14] if method else "N/A"
            
            print(f"{joint_name:<40} {region:<15} {artifacts:<12} {hampel:<12} {cutoff_str:<15} {method_str:<15}")
        
        if len(joint_scores) > n_to_show:
            print(f"\n   ... and {len(joint_scores) - n_to_show} more joints (set show_all=True to see all)")
        
        # Statistics by region
        print(f"\n{'='*80}")
        print(f"üìä STATISTICS BY BODY REGION")
        print(f"{'='*80}\n")
        
        region_stats = {}
        for col, meta in per_joint.items():
            region = meta.get('marker_region', 'unknown')
            if region not in region_stats:
                region_stats[region] = {
                    'count': 0,
                    'artifacts': 0,
                    'hampel': 0,
                    'cutoffs': []
                }
            
            region_stats[region]['count'] += 1
            region_stats[region]['artifacts'] += meta.get('stage1_artifacts_detected', 0)
            region_stats[region]['hampel'] += meta.get('stage2_hampel_outliers', 0)
            cutoff = meta.get('stage3_winter_cutoff')
            if cutoff is not None:
                region_stats[region]['cutoffs'].append(cutoff)
        
        print(f"{'Region':<20} {'Joints':<10} {'Artifacts':<12} {'Hampel':<12} {'Cutoff Range':<20} {'Mean Cutoff':<15}")
        print(f"{'-'*20} {'-'*10} {'-'*12} {'-'*12} {'-'*20} {'-'*15}")
        
        for region, stats in sorted(region_stats.items()):
            count = stats['count']
            artifacts = stats['artifacts']
            hampel = stats['hampel']
            cutoffs = stats['cutoffs']
            
            if cutoffs:
                cutoff_range = f"{min(cutoffs):.1f} - {max(cutoffs):.1f} Hz"
                mean_cutoff = f"{np.mean(cutoffs):.2f} Hz"
            else:
                cutoff_range = "N/A"
                mean_cutoff = "N/A"
            
            print(f"{region:<20} {count:<10} {artifacts:<12} {hampel:<12} {cutoff_range:<20} {mean_cutoff:<15}")
        
        # Joints with most issues
        print(f"\n{'='*80}")
        print(f"‚ö†Ô∏è  JOINTS WITH MOST ISSUES (Top 10)")
        print(f"{'='*80}\n")
        
        top_issues = sorted(joint_scores, key=lambda x: x[2], reverse=True)[:10]
        for i, (col, meta, total_issues) in enumerate(top_issues, 1):
            joint_name = col.replace('__px', '').replace('__py', '').replace('__pz', '')
            artifacts = meta.get('stage1_artifacts_detected', 0)
            hampel = meta.get('stage2_hampel_outliers', 0)
            cutoff = meta.get('stage3_winter_cutoff', None)
            region = meta.get('marker_region', 'unknown')
            
            print(f"{i:2d}. {joint_name}")
            print(f"     Region: {region} | Artifacts: {artifacts} | Hampel: {hampel} | Total: {total_issues}")
            if cutoff:
                print(f"     Winter Cutoff: {cutoff:.2f} Hz ({meta.get('stage3_winter_method', 'unknown')})")
            print()
        
        # Joints with Winter analysis issues
        winter_issues = []
        for col, meta in per_joint.items():
            method = meta.get('stage3_winter_method', '')
            cutoff = meta.get('stage3_winter_cutoff', None)
            if 'failure' in method.lower() or cutoff is None or (cutoff and cutoff >= 19.0):
                winter_issues.append((col, meta))
        
        if winter_issues:
            print(f"\n{'='*80}")
            print(f"‚ö†Ô∏è  WINTER ANALYSIS ISSUES ({len(winter_issues)} joints)")
            print(f"{'='*80}\n")
            for col, meta in winter_issues[:10]:
                joint_name = col.replace('__px', '').replace('__py', '').replace('__pz', '')
                method = meta.get('stage3_winter_method', 'unknown')
                cutoff = meta.get('stage3_winter_cutoff', None)
                print(f"   {joint_name}: {method}")
                if cutoff:
                    print(f"     Cutoff: {cutoff:.2f} Hz")
            if len(winter_issues) > 10:
                print(f"\n   ... and {len(winter_issues) - 10} more joints with Winter issues")
    
    print(f"\n{'='*80}")
    print(f"‚úÖ DEBUG LOGS COMPLETE")
    print(f"{'='*80}\n")


def apply_winter_filter(df: pd.DataFrame, 
                     fs: float, 
                     pos_cols: List[str], 
                     rep_col: Optional[str] = None,
                     fmax: int = 12,
                     allow_fmax: bool = False,
                     min_cutoff_trunk: Optional[float] = 6.0,
                     min_cutoff_distal: Optional[float] = 8.0,
                     use_trunk_global: bool = False,
                     per_region_filtering: bool = False,
                     fixed_cutoff_hz: Optional[float] = None) -> Tuple[pd.DataFrame, Dict]:
    """
    Apply Winter low-pass filter to position columns only.
    
    Uses Winter residual analysis to determine optimal cutoff frequency,
    then applies zero-lag 2nd-order Butterworth filtering.
    
    Args:
        df: Input DataFrame with perfect time grid
        fs: Sampling frequency in Hz
        pos_cols: List of position column names to filter
        rep_col: Representative column for cutoff analysis (optional)
        fmax: Maximum cutoff frequency for Winter analysis (Hz)
        allow_fmax: If False, raises ValueError when Winter returns fmax (method failure)
        min_cutoff_trunk: Biomechanical minimum cutoff for trunk markers (Hz)
        min_cutoff_distal: Biomechanical minimum cutoff for distal markers (Hz)
        use_trunk_global: If True, run Winter on trunk markers only and apply to all columns
        per_region_filtering: If True, apply different cutoffs per body region (recommended for dance)
        fixed_cutoff_hz: If set (e.g. 8.0), skip Winter analysis and apply this cutoff to all columns (single-global).
        
    Returns:
        Tuple of (filtered DataFrame, metadata dict)
        
    Raises:
        ValueError: If NaNs exist in position columns or Winter analysis fails
        
    Note:
        For dance/mocap: Distal segments (hands/feet) need higher cutoffs than trunk
        because they contain faster real motion. Typical: Trunk=6-8Hz, Distal=10-12Hz
        
        Per-region filtering (new feature):
        - Trunk: 6-8 Hz (slow core movements)
        - Head: 7-9 Hz (moderate dynamics)
        - Upper proximal: 8-10 Hz (shoulders)
        - Upper distal: 10-12 Hz (hands - rapid gestures)
        - Lower proximal: 8-10 Hz (thighs, knees)
        - Lower distal: 9-11 Hz (feet, ankles)
    """
    # Ticket 10.5: Build valid position columns list and log exclusions
    pos_cols_valid = []
    excluded_cols = []
    
    for col in pos_cols:
        if col not in df.columns:
            excluded_cols.append(f"{col} (missing)")
        elif df[col].isna().any():
            excluded_cols.append(f"{col} (NaNs)")
        else:
            pos_cols_valid.append(col)
    
    if excluded_cols:
        logger.warning(f"Excluding columns with issues: {excluded_cols}")
    
    if not pos_cols_valid:
        raise ValueError(f"No valid position columns found. All {len(pos_cols)} columns excluded: {excluded_cols}")
    
    # Single-global fixed cutoff from config (skip Winter analysis)
    if fixed_cutoff_hz is not None:
        fc = float(fixed_cutoff_hz)
        chosen_rep_col = "config_fixed"
        logger.info(f"Using config fixed cutoff: {fc:.1f} Hz (single-global)")
    # Smart representative column selection with multi-signal fallback
    elif rep_col is not None:
        if rep_col not in df.columns:
            raise ValueError(f"Representative column '{rep_col}' not found in DataFrame")
        if df[rep_col].isna().any():
            raise ValueError(f"Representative column '{rep_col}' contains NaNs")
        chosen_rep_col = rep_col
        cutoffs = [winter_residual_analysis(df[chosen_rep_col].values, fs, fmax=fmax)]
        fc = float(cutoffs[0])  # Bug fix: assign fc from cutoffs[0]
        logger.info(f"Using user-specified representative column: {chosen_rep_col}")
    else:
        # Determine analysis strategy based on use_trunk_global flag
        if use_trunk_global:
            logger.info("Using trunk-based global cutoff strategy...")
            
            # Identify trunk markers (pelvis, spine, torso)
            trunk_patterns = ['Pelvis', 'Spine', 'Torso', 'Hips', 'Abdomen', 'Chest', 'Neck']
            trunk_cols = [col for col in pos_cols_valid 
                        if any(pattern in col for pattern in trunk_patterns)]
            
            if not trunk_cols:
                logger.warning("No trunk markers found. Falling back to multi-signal analysis.")
                trunk_cols = pos_cols_valid  # Fallback to all columns
            
            logger.info(f"Trunk markers identified: {len(trunk_cols)} columns")
            
            # Compute dynamics score for trunk columns only
            col_scores = {}
            for col in trunk_cols:
                signal = df[col].values
                dynamics_score = np.nanstd(np.diff(signal))
                col_scores[col] = dynamics_score
            
            # Sort by dynamics score and pick top 3 trunk columns
            sorted_cols = sorted(col_scores.items(), key=lambda x: x[1], reverse=True)
            top_trunk_cols = [col for col, _ in sorted_cols[:3]]
            
            logger.info(f"Top 3 most dynamic trunk columns: {top_trunk_cols}")
            
            # Run Winter analysis on trunk columns with trunk minimum cutoff
            cutoffs = []
            for col in top_trunk_cols:
                cutoff = winter_residual_analysis(
                    df[col].values, fs, fmax=fmax, 
                    min_cutoff=min_cutoff_trunk, body_region="trunk"
                )
                cutoffs.append(cutoff)
                logger.info(f"  {col}: {cutoff:.1f} Hz (trunk)")
            
            # Use median trunk cutoff as global cutoff for all columns
            fc = np.median(cutoffs)
            chosen_rep_col = f"trunk_global_median({len(cutoffs)}_cols)"
            
            logger.info(f"Trunk-based global cutoff: median = {fc:.1f} Hz")
            logger.info(f"Individual trunk cutoffs: {[f'{c:.1f}' for c in cutoffs]}")
            logger.info(f"Applying trunk-based cutoff to all {len(pos_cols_valid)} columns")
            
        else:
            # Standard multi-signal Winter analysis: pick top 5 most dynamic columns
            logger.info("Performing multi-signal Winter analysis...")
            
            # Compute dynamics score for each valid position column
            col_scores = {}
            for col in pos_cols_valid:
                signal = df[col].values
                # Dynamics ranking: score = nanstd(diff(x))
                dynamics_score = np.nanstd(np.diff(signal))
                col_scores[col] = dynamics_score
            
            # Sort by dynamics score (descending) and pick top 5
            sorted_cols = sorted(col_scores.items(), key=lambda x: x[1], reverse=True)
            top_5_cols = [col for col, _ in sorted_cols[:5]]
            
            logger.info(f"Top 5 most dynamic columns: {top_5_cols}")
            
            # Run Winter analysis on each of the top 5 columns with appropriate guardrails
            cutoffs = []
            for col in top_5_cols:
                # Determine if this is a trunk or distal marker
                is_trunk = any(pattern in col for pattern in ['Pelvis', 'Spine', 'Torso', 'Hips', 'Abdomen', 'Chest', 'Neck'])
                min_cutoff = min_cutoff_trunk if is_trunk else min_cutoff_distal
                body_region = "trunk" if is_trunk else "distal"
                
                cutoff = winter_residual_analysis(
                    df[col].values, fs, fmax=fmax, 
                    min_cutoff=min_cutoff, body_region=body_region
                )
                cutoffs.append(cutoff)
                logger.info(f"  {col}: {cutoff:.1f} Hz ({body_region})")
            
            # Use median cutoff as global cutoff
            fc = np.median(cutoffs)
            chosen_rep_col = f"multi_signal_median({len(cutoffs)}_cols)"
            
            logger.info(f"Multi-signal Winter analysis: median cutoff = {fc:.1f} Hz")
            logger.info(f"Individual cutoffs: {[f'{c:.1f}' for c in cutoffs]}")
    
    # PER-REGION FILTERING WITH FIXED CUTOFFS (Literature-Based)
    # Reference: Winter (2009), Robertson (2014) - Fixed cutoffs for research reproducibility
    if per_region_filtering:
        logger.info("=== PER-REGION FILTERING WITH FIXED CUTOFFS ===")
        logger.info("Using literature-based fixed cutoffs per body region (Winter 2009, Robertson 2014)")
        logger.info("Winter analysis runs as VALIDATION only - not for cutoff selection")
        
        # Classify all markers by region
        marker_regions = {}
        region_columns = {region: [] for region in BODY_REGIONS.keys()}
        region_columns['unknown'] = []
        
        for col in pos_cols_valid:
            region = classify_marker_region(col)
            marker_regions[col] = region
            if region in region_columns:
                region_columns[region].append(col)
            else:
                region_columns['unknown'].append(col)
        
        # Log region classification
        for region, cols in region_columns.items():
            if cols:
                logger.info(f"  {region}: {len(cols)} markers")
        
        # Apply FIXED cutoffs per region with Winter validation
        df_out = df.copy()
        region_cutoffs = {}
        region_analysis_details = {}  # Store Winter validation results
        
        for region, cols in region_columns.items():
            if not cols or region == 'unknown':
                continue
            
            # Get region configuration with FIXED cutoff
            region_config = BODY_REGIONS.get(region, {'fixed_cutoff': 10, 'cutoff_range': (8, 12), 'rationale': 'default'})
            fc_region = region_config.get('fixed_cutoff', 10)  # Use fixed cutoff
            min_cutoff_region, max_cutoff_region = region_config['cutoff_range']
            
            # Select representative column for this region (most dynamic) - for validation
            col_scores = {col: np.nanstd(np.diff(df[col].values)) for col in cols}
            rep_col_region = max(col_scores, key=col_scores.get)
            
            # Run Winter analysis as VALIDATION (not for cutoff selection)
            # validation_mode=True skips weighted compromise to get raw knee-point
            validation_details = winter_residual_analysis(
                df[rep_col_region].values, fs, fmax=fmax,
                min_cutoff=None,  # No guardrail - we want to see raw Winter result
                body_region=region,
                return_details=True,
                validation_mode=True  # Get raw knee-point, no weighted compromise
            )
            
            # Check if fixed cutoff is appropriate
            # VALID = fixed cutoff >= Winter suggested (we preserve at least as much signal)
            # AGGRESSIVE = fixed cutoff < Winter suggested (we filter more than Winter recommends)
            winter_suggested = validation_details.get('raw_cutoff_hz', validation_details.get('cutoff_hz', 0))
            if fc_region >= winter_suggested:
                validation_status = "VALID"  # Fixed preserves more signal than Winter's minimum
            else:
                validation_status = "AGGRESSIVE"  # Fixed removes signal Winter thinks is valid
            
            region_cutoffs[region] = fc_region
            region_analysis_details[region] = {
                'cutoff_hz': fc_region,
                'cutoff_method': 'fixed_winter_validated',
                'winter_strict_knee_hz': validation_details.get('strict_knee_hz'),  # Where RMS flattens
                'winter_diminishing_hz': validation_details.get('raw_diminishing_hz'),  # Where biggest drop happens
                'winter_suggested_hz': winter_suggested,  # The chosen validation value (strict_knee)
                'validation_status': validation_status,
                'knee_point_found': validation_details.get('knee_point_found'),
                'method_used': validation_details.get('method_used'),
                'rms_range_ratio': validation_details.get('rms_range_ratio'),
                'curve_is_flat': validation_details.get('curve_is_flat'),
                'rep_col': rep_col_region,
                'rationale': region_config.get('rationale', 'N/A'),
                'residual_rms_mm': validation_details.get('residual_rms_final', 0)  # RMS at the applied cutoff
            }
            
            strict_knee = validation_details.get('strict_knee_hz', 'N/A')
            diminishing = validation_details.get('raw_diminishing_hz', 'N/A')
            logger.info(f"  {region}: FIXED={fc_region:.0f} Hz | Winter RMS knee: {strict_knee} Hz, diminishing: {diminishing} Hz | {validation_status}")
            
            # Design and apply filter for this region
            b_region, a_region = butter(N=2, Wn=fc_region/(0.5*fs), btype='low')
            
            for col in cols:
                df_out[col] = filtfilt(b_region, a_region, df[col].values.astype(float))
        
        # Handle unknown markers with median cutoff
        if region_columns['unknown']:
            median_cutoff = np.median(list(region_cutoffs.values()))
            logger.warning(f"  unknown: {len(region_columns['unknown'])} markers, using median cutoff={median_cutoff:.1f} Hz")
            b_unknown, a_unknown = butter(N=2, Wn=median_cutoff/(0.5*fs), btype='low')
            for col in region_columns['unknown']:
                df_out[col] = filtfilt(b_unknown, a_unknown, df[col].values.astype(float))
            region_cutoffs['unknown'] = median_cutoff
        
        # Compute statistics for audit reports
        valid_cutoffs = [v for k, v in region_cutoffs.items() if k != 'unknown']
        weighted_avg_cutoff = float(np.mean(valid_cutoffs)) if valid_cutoffs else 0.0
        
        # Check Winter validation status for all regions
        aggressive_regions = []
        for region, details in region_analysis_details.items():
            if details.get('validation_status') == 'AGGRESSIVE':
                winter_hz = details.get('winter_suggested_hz', 0)
                fixed_hz = details.get('cutoff_hz', 0)
                aggressive_regions.append(f"{region} (fixed={fixed_hz}Hz < winter={winter_hz:.1f}Hz)")
        
        # Winter validation summary (informational only - not a failure)
        if aggressive_regions:
            winter_validation_note = f"Fixed cutoffs are AGGRESSIVE for {len(aggressive_regions)} region(s): {', '.join(aggressive_regions)}. This filters more than Winter suggests - some signal may be removed."
        else:
            winter_validation_note = "All fixed cutoffs validated by Winter analysis (fixed >= Winter suggested)"
        
        # Compute aggregate residual RMS (mean across all regions)
        all_region_rms = [details.get('residual_rms_mm', 0) for details in region_analysis_details.values()]
        mean_residual_rms = float(np.mean(all_region_rms)) if all_region_rms else 0.0
        
        # Update metadata for per-region filtering with FIXED cutoffs
        metadata = {
            "filtering_mode": "per_region_fixed",
            "cutoff_method": "fixed_literature_based",
            "literature_reference": "Winter (2009), Robertson (2014)",
            "region_cutoffs": region_cutoffs,
            "marker_regions": marker_regions,
            "region_analysis_details": region_analysis_details,
            "n_regions": len([r for r in region_cutoffs.keys() if r != 'unknown']),
            "cutoff_range": (min(region_cutoffs.values()), max(region_cutoffs.values())),
            "fmax": fmax,
            "fmin": 1,
            "pos_cols_valid": pos_cols_valid,
            "pos_cols_excluded": excluded_cols,
            "total_pos_cols": len(pos_cols),
            # Audit report compatibility
            "filter_cutoff_hz": weighted_avg_cutoff,
            "filter_range_hz": [1, fmax],
            "residual_rms_mm": mean_residual_rms,  # Aggregate residual RMS across all regions
            # Fixed cutoff approach - Winter is validation only, not failure indicator
            "winter_analysis_failed": False,  # Fixed cutoffs don't "fail" - they're intentional
            "winter_failure_reason": None,
            "winter_validation_note": winter_validation_note,
            "decision_reason": f"Fixed per-region cutoffs based on biomechanical literature. Trunk: 6Hz, Head/Proximal: 8Hz, Distal: 10Hz (Winter 2009, Robertson 2014)"
        }
        
        logger.info(f"Per-region FIXED filtering complete: {len(region_cutoffs)} regions")
        logger.info(f"  Cutoffs applied: {region_cutoffs}")
        logger.info(f"  Validation: {winter_validation_note}")
    
    else:
        # SINGLE GLOBAL CUTOFF (ORIGINAL BEHAVIOR)
        # Check for Winter analysis failure
        if not allow_fmax and fc >= fmax - 1:
            raise ValueError(f"WINTER ANALYSIS FAILED: cutoff = {fc:.1f} Hz (at fmax). "
                            f"This indicates data is already oversmoothed or method applied too late. "
                            f"Expected dance cutoff: 4-10 Hz. "
                            f"To override, set allow_fmax=True.")
        
        # Design filter with optimal cutoff
        b, a = butter(N=2, Wn=fc/(0.5*fs), btype='low')
        
        # Apply filter to valid position columns only
        df_out = df.copy()
        for col in pos_cols_valid:
            df_out[col] = filtfilt(b, a, df[col].values.astype(float))
        
        # Run detailed analysis on a representative column for metadata
        # (Pick the most dynamic column from top_5)
        detailed_analysis = None
        if rep_col is None and not use_trunk_global:
            # Use multi-signal approach - get details from most dynamic column
            col_scores = {col: np.nanstd(np.diff(df[col].values)) for col in pos_cols_valid}
            most_dynamic_col = max(col_scores, key=col_scores.get)
            is_trunk = any(pattern in most_dynamic_col for pattern in ['Pelvis', 'Spine', 'Torso', 'Hips', 'Abdomen', 'Chest', 'Neck'])
            min_cutoff_for_rep = min_cutoff_trunk if is_trunk else min_cutoff_distal
            detailed_analysis = winter_residual_analysis(
                df[most_dynamic_col].values, fs, fmax=fmax,
                min_cutoff=min_cutoff_for_rep, body_region="trunk" if is_trunk else "distal",
                return_details=True
            )
        
        # Determine if Winter analysis actually failed
        # Failure conditions:
        # 1. No knee point found (curve is flat)
        # 2. Guardrail significantly changed the cutoff (>2Hz delta)
        # 3. Cutoff at fmax
        winter_analysis_failed = False
        failure_reason = None
        
        if detailed_analysis:
            # GATE 3 FIX: Use the detailed failure_reason from analysis if available
            if detailed_analysis.get('failure_reason'):
                winter_analysis_failed = True
                failure_reason = detailed_analysis['failure_reason']
            elif not detailed_analysis['knee_point_found']:
                winter_analysis_failed = True
                failure_reason = f"No knee-point found (RMS curve flat, range={detailed_analysis['rms_range_ratio']:.1%})"
            elif detailed_analysis['guardrail_applied'] and detailed_analysis['guardrail_delta_hz'] >= 2.0:
                winter_analysis_failed = True
                failure_reason = f"Guardrail override (+{detailed_analysis['guardrail_delta_hz']:.1f}Hz from {detailed_analysis['raw_cutoff_hz']:.1f}Hz)"
        
        if fc >= fmax - 1:
            winter_analysis_failed = True
            if not failure_reason:  # Only override if not already set
                failure_reason = f"Cutoff at fmax ({fc:.1f}Hz) - data may be pre-smoothed"
        
        # Prepare metadata
        metadata = {
            "filtering_mode": "single_global",
            "cutoff_hz": fc,
            "rep_col": chosen_rep_col,
            "fmin": 1,
            "fmax": fmax,
            "multi_signal_analysis": rep_col is None,
            "individual_cutoffs": cutoffs if rep_col is None else [fc],
            "allow_fmax": allow_fmax,
            "pos_cols_valid": pos_cols_valid,
            "pos_cols_excluded": excluded_cols,
            "total_pos_cols": len(pos_cols),
            # Winter analysis success/failure tracking (Cereatti et al., 2024 - No Silent Fixes)
            "winter_analysis_failed": winter_analysis_failed,
            "winter_failure_reason": failure_reason,
            "winter_details": detailed_analysis,
            # Biomechanical guardrails metadata
            "biomechanical_guardrails": {
                "enabled": True,
                "min_cutoff_trunk": min_cutoff_trunk,
                "min_cutoff_distal": min_cutoff_distal,
                "use_trunk_global": use_trunk_global,
                "strategy": "trunk_global" if use_trunk_global else "multi_signal_with_guardrails"
            }
        }
        
        # Log warning if Winter failed
        if winter_analysis_failed:
            logger.warning(f"WINTER ANALYSIS FAILURE DETECTED: {failure_reason}. "
                          f"Using cutoff={fc:.1f}Hz but flagging as failed for traceability.")
        
        # TASK 3: Filter Ceiling + Residual RMS Synergy Check
        # If filter is at ceiling (16Hz) AND RMS is high (>20mm), flag it
        if detailed_analysis:
            residual_rms = detailed_analysis.get('residual_rms_final', 0)
            residual_slope = detailed_analysis.get('residual_slope', 0)
            
            if fc >= 16.0 and residual_rms > 20.0:
                synergy_warning = (f"High-frequency intensity exceeding filter bounds: "
                                 f"Cutoff at ceiling ({fc:.1f}Hz) with RMS={residual_rms:.2f}mm > 20mm threshold. "
                                 f"This suggests movement contains genuine high-frequency content beyond filter capacity.")
                logger.warning(synergy_warning)
                
                # Add to metadata for audit logging
                metadata['filter_ceiling_warning'] = {
                    'triggered': True,
                    'cutoff_hz': fc,
                    'residual_rms_mm': residual_rms,
                    'residual_slope': residual_slope,
                    'decision_reason': synergy_warning
                }
            else:
                metadata['filter_ceiling_warning'] = {
                    'triggered': False,
                    'cutoff_hz': fc,
                    'residual_rms_mm': residual_rms,
                    'residual_slope': residual_slope
                }
    
    # Ensure quaternion columns are unchanged (both modes)
    quat_cols = [col for col in df.columns if col.endswith(('__qx', '__qy', '__qz', '__qw'))]
    for col in quat_cols:
        if col in df.columns:
            df_out[col] = df[col].values
    
    # PSD Validation (Research Validation Phase 1 - Item 1)
    if PSD_VALIDATION_AVAILABLE:
        try:
            logger.info("Running PSD validation to verify filter quality...")
            
            if per_region_filtering:
                # Per-region PSD validation: validate sample markers from each region
                all_metrics = []
                regions_validated = set()
                
                # Sample up to 2 markers per region for validation
                for region_name, region_cutoff in region_cutoffs.items():
                    region_markers = [m for m in pos_cols_valid 
                                     if marker_regions.get(m) == region_name][:2]
                    
                    for marker in region_markers:
                        if marker in df.columns and marker in df_out.columns:
                            signal_raw = df[marker].values
                            signal_filt = df_out[marker].values
                            
                            if not np.any(np.isnan(signal_raw)) and not np.any(np.isnan(signal_filt)):
                                metrics = analyze_filter_psd_preservation(
                                    signal_raw, signal_filt, fs, region_cutoff
                                )
                                metrics['column'] = marker
                                metrics['region'] = region_name
                                metrics['cutoff_hz'] = region_cutoff
                                all_metrics.append(metrics)
                                regions_validated.add(region_name)
                
                if all_metrics:
                    # Aggregate metrics
                    dance_preservations = [m.get('dance_preservation_pct', 0) for m in all_metrics]
                    noise_attenuations = [m.get('noise_attenuation_pct', 0) for m in all_metrics]
                    
                    psd_validation = {
                        'status': 'VALIDATED',
                        'mode': 'per_region',
                        'regions_validated': list(regions_validated),
                        'n_markers_validated': len(all_metrics),
                        'dance_preservation_mean': float(np.mean(dance_preservations)),
                        'dance_preservation_min': float(np.min(dance_preservations)),
                        'noise_attenuation_mean': float(np.mean(noise_attenuations)),
                        'per_marker_metrics': all_metrics,
                        'overall_filter_quality': 'GOOD' if np.mean(dance_preservations) >= 80 else 'MARGINAL'
                    }
                else:
                    psd_validation = {'status': 'NO_VALID_MARKERS', 'mode': 'per_region'}
                    
                metadata['psd_validation'] = psd_validation
                logger.info(f"Per-region PSD Validation: Dance preservation={psd_validation.get('dance_preservation_mean', 0):.1f}%, "
                           f"Quality={psd_validation.get('overall_filter_quality', 'UNKNOWN')}, "
                           f"Regions={list(regions_validated)}")
            else:
                # Single-cutoff validation
                cutoff_validity = check_filter_cutoff_validity(fc, fs, fmax)
                metadata['cutoff_validity'] = cutoff_validity
                
                psd_validation = validate_winter_filter_multi_signal(
                    df, df_out, pos_cols_valid, fs, fc, n_samples=5
                )
                metadata['psd_validation'] = psd_validation
                
                logger.info(f"PSD Validation Complete: Dance preservation={psd_validation.get('dance_preservation_mean', 0):.1f}%, "
                           f"Filter quality={psd_validation.get('overall_filter_quality', 'UNKNOWN')}")
            
        except Exception as e:
            logger.warning(f"PSD validation failed: {e}")
            metadata['psd_validation'] = {'status': 'ERROR', 'error': str(e)}
    else:
        logger.info("PSD validation skipped (module not available)")
        metadata['psd_validation'] = {'status': 'SKIPPED', 'reason': 'module_not_available'}
    
    # Log completion
    if per_region_filtering:
        logger.info(f"Per-region Winter filtering complete: {len(pos_cols_valid)}/{len(pos_cols)} position columns filtered")
    else:
        logger.info(f"Winter filtering applied: cutoff={fc} Hz, {len(pos_cols_valid)}/{len(pos_cols)} position columns filtered")
    
    return df_out, metadata


def get_position_columns(df: pd.DataFrame) -> List[str]:
    """
    Extract position column names from DataFrame.
    
    Args:
        df: Input DataFrame
        
    Returns:
        List of position column names
    """
    return [col for col in df.columns if col.endswith(('__px', '__py', '__pz'))]


def get_quaternion_columns(df: pd.DataFrame) -> List[str]:
    """
    Extract quaternion column names from DataFrame.
    
    Args:
        df: Input DataFrame
        
    Returns:
        List of quaternion column names
    """
    return [col for col in df.columns if col.endswith(('__qx', '__qy', '__qz', '__qw'))]


def validate_filtering_input(df: pd.DataFrame, 
                         fs: float,
                         pos_cols: Optional[List[str]] = None) -> None:
    """
    Validate input data for filtering operations.
    
    Args:
        df: Input DataFrame
        fs: Sampling frequency
        pos_cols: Position columns to validate
        
    Raises:
        ValueError: If validation fails
    """
    if fs <= 0:
        raise ValueError(f"Sampling frequency must be positive, got {fs}")
    
    if 'time_s' not in df.columns:
        raise ValueError("DataFrame must contain 'time_s' column")
    
    if pos_cols is None:
        pos_cols = get_position_columns(df)
    
    if not pos_cols:
        raise ValueError("No position columns found for filtering")
    
    missing_cols = [col for col in pos_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"Position columns not found: {missing_cols}")
    
    # Check time grid regularity (should be constant dt)
    time_diffs = np.diff(df['time_s'].values)
    if not np.allclose(time_diffs, time_diffs[0], rtol=1e-6):
        logger.warning("Time grid is not perfectly regular - filtering may be suboptimal")


def compute_filter_characteristics(fc: float, fs: float) -> Dict[str, float]:
    """
    Compute filter characteristics for documentation.
    
    Args:
        fc: Cutoff frequency in Hz
        fs: Sampling frequency in Hz
        
    Returns:
        Dictionary with filter characteristics
    """
    # Normalized cutoff frequency
    wn = fc / (0.5 * fs)
    
    # Phase delay at different frequencies (approximate for 2nd-order Butterworth)
    freqs = np.array([0.1, 0.5, 1.0, 5.0, 10.0]) * fc
    phase_delays = []
    
    for freq in freqs:
        if freq < fc:
            # Below cutoff: minimal phase delay
            phase_delay = 0.0
        else:
            # Above cutoff: increasing phase delay
            phase_delay = np.degrees(np.arctan(freq/fc))
        phase_delays.append(phase_delay)
    
    return {
        "cutoff_hz": fc,
        "normalized_wn": wn,
        "filter_order": 2,
        "phase_delay_10pct_fc": phase_delays[0] if len(phase_delays) > 0 else 0,
        "phase_delay_50pct_fc": phase_delays[1] if len(phase_delays) > 1 else 0,
        "phase_delay_fc": phase_delays[2] if len(phase_delays) > 2 else 0,
        "phase_delay_5x_fc": phase_delays[3] if len(phase_delays) > 3 else 0,
        "phase_delay_10x_fc": phase_delays[4] if len(phase_delays) > 4 else 0
    }
